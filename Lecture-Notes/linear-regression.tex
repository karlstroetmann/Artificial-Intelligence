\chapter{Linear Regression}
A great deal of the current success of artificial intelligence is due to recent advances in
\href{https://en.wikipedia.org/wiki/Machine_learning}{machine learning}.  
In order to get a first taste of what machine learning is about, we introduce 
\href{https://en.wikipedia.org/wiki/Linear_regression}{linear regression} \index{linear regression}
in this chapter, since linear regression
is one of the most basic algorithms in machine learning.  It is also the foundation for more advanced
forms of machine learning like \href{https://en.wikipedia.org/wiki/Logistic_regression}{logistic regression} and 
\href{https://en.wikipedia.org/wiki/Artificial_neural_network}{neural networks}.
Furthermore, linear regression is surprisingly versatile and powerful.  Finally, many of the fundamental problems of machine
learning can already be illustrated with linear regression.  Therefore it is only natural that we begin our
study of machine learning with the study of linear regression.

\section{Simple Linear Regression}
\index{simple linear regression} 
Assume we want to know how the \href{https://en.wikipedia.org/wiki/Engine_displacement}{engine displacement} of
a car engine relates to its \blue{fuel consumption}.  One approach to understand this relation would be to derive a
\blue{theoretical model} that is able to predict the fuel consumption from the engine displacement by using the
appropriate laws of physics and chemistry.  However, due to our lack of understanding of the underlying theory,
this is not an option for us.  Instead, we follow a \blue{statistical approach} and collect data from a large number
of cars.  For these cars, we compare their engine displacement with the corresponding fuel consumption.  This
way, we will collect a set of $m$ \blue{observations} of the form
\\[0.2cm]
\hspace*{1.3cm}
$\langle x_1, y_1\rangle, \cdots, \langle x_m, y_m\rangle$ 
\\[0.2cm]
where $x_i$ is the engine displacement of the engine in the $i$-th car, while $y_i$ is the fuel consumption of the
$i$-th car.  We call $x$ the \blue{independent variable}, \index{independent variable} while $y$ is the 
\blue{dependent variable}.  \index{dependent variable} We define the vectors $\mathbf{x}$ and $\mathbf{y}$ as follows:
\\[0.2cm]
\hspace*{1.3cm}
$\mathbf{x} := \langle x_1, \cdots, x_m \rangle^\top$ \quad and \quad
$\mathbf{y} := \langle y_1, \cdots, y_m \rangle^\top$.
\\[0.2cm]
Here, the postfix operator $^\top$ is interpreted as the
\href{https://en.wikipedia.org/wiki/Transpose}{transposition operator}\index{transposition operator}.
The reason is that $\mathbf{x}$ and $\mathbf{y}$ are column vectors, but writing them as column vectors
would take to much space.  By using the transposition operator we are able to write these vectors in a single 
line.  In these lecture notes, I will write vectors using bold face.  On the blackboard I will write $\vec{x}$
instead of $\mathbf{x}$.

In linear regression, we use a \blue{linear model} \index{linear model}
and assume that the dependent variable $y_i$ is related to the independent variable $x_i$ via an
equation of the form
\\[0.2cm]
\hspace*{1.3cm}
$y_i = \vartheta_1 \cdot x_i + \vartheta_0$.
\\[0.2cm]
We do not assume that this equation will hold precisely. This is because, in addition to
engine displacement, numerous other factors affect fuel consumption. For instance, the \blue{weight} of a car and its
\href{https://en.wikipedia.org/wiki/Automotive_aerodynamics}{aerodynamics} undoubtedly play significant roles
in determining fuel consumption. 
We want to calculate those values $\vartheta_0$ and $\vartheta_1$ such that the 
\blue{\underline{m}ean \underline{s}quared \underline{e}rror}, \index{mean squared error} which is defined as 
\begin{equation}
  \label{eq:mse}
 \mathtt{MSE}(\vartheta_0, \vartheta_1) := \frac{1}{m-1} \cdot \sum\limits_{i=1}^m \bigl(\vartheta_1 \cdot x_i + \vartheta_0 - y_i\bigr)^2,
\end{equation}
is minimized.  It can be shown (and you will do so in an exercise) that the solution to this minimization
problem is given as follows: 
\begin{equation}
  \label{eq:theta0}
  \ds \vartheta_1 = r_{x,y} \cdot \frac{\,s_y\,}{s_x} \quad \mbox{and} \quad
      \vartheta_0 = \bar{\mathbf{y}} - \vartheta_1 \cdot \bar{\mathbf{x}}.
\end{equation}
This solution utilizes the values $r_{x,y}$, $s_x$, and $s_y$. To define these values, we initially establish
the \blue{sample mean values} \index{sample mean value} $\bar{\mathbf{x}}$ and $\bar{\mathbf{y}}$ for the
vectors $\mathbf{x}$ and $\mathbf{y}$ respectively. Specifically, we calculate 
\\[0.2cm]
\hspace*{1.3cm}
$\ds \bar{\mathbf{x}} = \frac{1}{m} \cdot \sum_{i=1}^m x_i$ \quad and \quad
$\ds \bar{\mathbf{y}} = \frac{1}{m} \cdot \sum_{i=1}^m y_i$.
\\[0.2cm]
Furthermore, $s_x$ and $s_y$ are the \href{https://en.wikipedia.org/wiki/Standard_deviation#Sample_standard_deviation}{corrected sample standard deviations} \index{corrected sample standard deviation}
of $\mathbf{x}$ and $\mathbf{y}$, i.e.~we have
\\[0.2cm]
\hspace*{1.3cm}
$\ds s_x = \sqrt{\frac{1}{m-1} \cdot \sum\limits_{i=1}^m \bigl(x_i - \bar{\mathbf{x}}\bigr)^2\;}$ \quad and \quad
$\ds s_y = \sqrt{\frac{1}{m-1} \cdot \sum\limits_{i=1}^m \bigl(y_i - \bar{\mathbf{y}}\bigr)^2\;}$.
\\[0.2cm]
In the rest of these lecture notes, $s_x$ and $s_y$ will be referred to as \blue{sample standard deviation},
\index{sample standard deviation} i.e.~we drop the attribute \emph{corrected}.
Next, $\mathrm{Cov}[\mathbf{x}, \mathbf{y}]$ is the \href{https://en.wikipedia.org/wiki/Sample_mean_and_covariance}{sample covariance} \index{sample covariance} and is defined as 
\\[0.2cm]
\hspace*{1.3cm}
$\ds \mathrm{Cov}[\mathbf{x}, \mathbf{y}] = \frac{1}{(m-1)} \cdot \sum\limits_{i=1}^m \bigl(x_i - \bar{\mathbf{x}}\bigr) \cdot \bigl(y_i - \bar{\mathbf{y}}\bigr)$.
\\[0.2cm]
Finally, $r_{x,y}$ is the \blue{sample correlation coefficient} \index{sample correlation coefficient} that is
defined as 
\\[0.2cm]
\hspace*{1.3cm}
$\ds r_{x,y} = \frac{1}{(m-1) \cdot s_x \cdot s_y} \cdot \sum\limits_{i=1}^m \bigl(x_i - \bar{\mathbf{x}}\bigr) \cdot \bigl(y_i - \bar{\mathbf{y}}\bigr)
            = \frac{\mathrm{Cov}[\mathbf{x}, \mathbf{y}]}{s_x \cdot s_y}
$.
\\[0.2cm]
The number $r_{x,y}$ is also known as the
\href{https://en.wikipedia.org/wiki/Pearson_correlation_coefficient}{Pearson correlation coefficient} 
\index{Pearson correlation coefficient} or \index{Pearson's r}
\blue{Pearson's \textrm{r}}.  It is named after \href{https://en.wikipedia.org/wiki/Karl_Pearson}{Karl Pearson}
(1857 -- 1936).
Note that the formula for the parameter $\vartheta_1$ can be simplified to  
\begin{equation}
  \label{eq:theta1}
\ds\vartheta_1 = \frac{\sum\limits_{i=1}^m \bigl(x_i - \bar{\mathbf{x}}\bigr) \cdot \bigl(y_i - \bar{\mathbf{y}}\bigr)}{
                        \sum\limits_{i=1}^m \bigl(x_i - \bar{\mathbf{x}}\bigr)^2}  
\end{equation}
This latter formula should be used to calculate $\vartheta_1$.  However, the previous formula is also useful
because it shows that the correlation coefficient is identical to the coefficient $\vartheta_1$, provided the
variables $\mathbf{x}$ and $\mathbf{y}$ have been \blue{normalized} so that their standard deviation is $1$.

\exercise
Prove Equation \ref{eq:theta0} and Equation \ref{eq:theta1}.

\hint
The expression $\mathtt{MSE}(\vartheta_0, \vartheta_1)$ is a quadratic function with respect to the parameters
$\vartheta_0$ and $\vartheta_1$.  Therefore, it has exactly one global minimum.  Take the partial derivatives
of $\mathtt{MSE}(\vartheta_0, \vartheta_1)$ with respect to $\vartheta_0$ and $\vartheta_1$.  If the expression
$\mathtt{MSE}(\vartheta_0, \vartheta_1)$ is minimal, then these partial derivatives have to be equal to $0$.
\eox

\subsection{Assessing the Quality of Linear Regression}
Assuming we are provided with a set of $m$ observations in the form $\langle x_1, y_1\rangle, \ldots, \langle
x_m, y_m\rangle$, and that we have calculated the parameters $\vartheta_0$ and $\vartheta_1$ as per Equation
\ref{eq:theta0} and Equation \ref{eq:theta1}, these formulas will yield values for $\vartheta_0$ and
$\vartheta_1$, assuming the $x_i$ values are not all the same. These values define a linear model for $\mathbf{y}$
as a function of $\mathbf{x}$. Yet, to assess the effectiveness of this linear model, we require a metric that
quantifies its accuracy. To evaluate the quality of the linear model represented by 
\\[0.2cm]
\hspace*{1.3cm}
$y = \vartheta_0 + \vartheta_1 \cdot x$,
\\[0.2cm]
we can calculate the mean squared error using Equation \ref{eq:mse}. However, the mean squared error alone may
not provide a complete picture due to its absolute nature and the potential inherent noise within
$\mathbf{y}$. To contextualize this noise relative to the mean squared error, we measure the noise within
$\mathbf{y}$ through the \blue{sample variance} \index{sample variance}, calculated as follows: 
\begin{equation}
  \label{eq:var}
  \mathtt{Var}(y) := \frac{1}{m-1} \cdot \sum_{i=1}^m (y_i - \bar{y})^2.
\end{equation}
If we compare this formula to the formula for the mean squared error
\\[0.2cm]
\hspace*{1.3cm}
$\ds\mathtt{MSE}(\vartheta_0, \vartheta_1) := 
    \frac{1}{m-1} \cdot\sum\limits_{i=1}^m \bigl(\vartheta_1 \cdot x_i + \vartheta_0 - y_i\bigr)^2
=   \frac{1}{m-1} \cdot\sum\limits_{i=1}^m \bigl(y_i - \vartheta_1 \cdot x_i - \vartheta_0\bigr)^2
$,
\\[0.2cm]
we see that the sample variance of $\mathbf{y}$ is an upper bound for the mean squared error since we have
\\[0.2cm]
\hspace*{1.3cm}
$\mathtt{Var}(\mathbf{y}) = \mathtt{MSE}(\bar{\mathbf{y}}, 0)$,
\\[0.2cm]
i.e.~the sample variance is the value that we would get for the mean squared error if we set $\vartheta_0$ to
the average value of $\mathbf{y}$ and $\vartheta_1$ to zero.  Since $\vartheta_0$ and $\vartheta_1$ are chosen to
minimize the mean squared error, we have
\\[0.2cm]
\hspace*{1.3cm}
$\mathtt{MSE}(\vartheta_0, \vartheta_1) \leq \mathtt{MSE}(\bar{\mathbf{y}}, 0) = \mathtt{Var}(\mathbf{y})$.
\\[0.2cm]
The mean squared error is an absolute value and, therefore, difficult to interpret.  The fraction
\\[0.2cm]
\hspace*{1.3cm}
$\ds \frac{\mathtt{MSE}(\vartheta_0, \vartheta_1)}{\mathtt{Var}(y)}$
\\[0.2cm]
is called the \blue{proportion of the unexplained variance} \index{proportion of unexplained variance}
because it is the variance that is still
left if we use our linear model to predict the values of $\mathbf{y}$ given the values of $\mathbf{x}$.  The
\blue{proportion of the explained variance} \index{proportion of explained variance}
which is also known as the \blue{$\mathtt{R}^2$ statistic} \index{$R^2$} is defined as 
\begin{equation}
  \label{eq:Rsquare}
  \mathtt{R}^2 := \frac{\mathtt{Var}(\mathbf{y}) - \mathtt{MSE}(\vartheta_0, \vartheta_1)}{\mathtt{Var}(\mathbf{y})} 
                = 1 - \frac{\mathtt{MSE}(\vartheta_0, \vartheta_1)}{\mathtt{Var}(\mathbf{y})}.
\end{equation}
The statistic $\mathtt{R}^2$ measures the quality of our
model: If it is small, then our model does not explain the variation of the value of $\mathbf{y}$ when the value of $\mathbf{x}$
changes.  On the other hand, if it is near to $100\%$, then our model does a good job in explaining the 
variation of $\mathbf{y}$ when $\mathbf{x}$ changes.

Since the formulas for $\mathtt{Var}(\mathbf{y})$ and $\mathtt{MSE}(\vartheta_0, \vartheta_1)$ have the same
denominator $m-1$, this denominator can be cancelled when $\mathtt{R}^2$ is computed.  To this
end we define the \blue{total sum of squares} $\mathtt{TSS}$ \index{total sum of squares} \index{TSS} as
\\[0.2cm]
\hspace*{1.3cm}
$\ds\mathtt{TSS} := \sum\limits_{i=1}^m \bigl(y_i - \bar{\mathbf{y}}\bigr)^2 = (m-1) \cdot \mathtt{Var}(\mathbf{y})$
\\[0.2cm]
and the \blue{residual sum of squares} \index{residual sum of squares} $\mathtt{RSS}$ \index{RSS} as
\\[0.2cm]
\hspace*{1.3cm}
$\ds\mathtt{RSS} := \sum\limits_{i=1}^m \bigl(\vartheta_1 \cdot x_i + \vartheta_0 - y_i\bigr)^2
                  = (m-1) \cdot \mathtt{MSE}(\vartheta_0, \vartheta_1)
$.
\\[0.2cm]
Then the formula for the $\mathtt{R}^2$ statistic can be written as
\\[0.2cm]
\hspace*{1.3cm}
$\ds \mathtt{R}^2 = 1 - \frac{\mathtt{RSS}}{\mathtt{TSS}}$.
\\[0.2cm]
This is the formula that we will use when we implement simple linear regression.

It should be noted that $\mathrm{R}^2$ is the square of Pearson's \textrm{r}.  The notation is a bit
inconsistent since Pearson's $r$ is written in lower case, while $\mathrm{R}^2$ is written in upper
case.  However, since this is the notation used in most books on statistics, we will use it too.
The number $\mathrm{R}^2$ is also known as the
\href{https://en.wikipedia.org/wiki/Coefficient_of_determination}{coefficient of determination}.  It tells us 
to what extent the value of the variable $y$ is \blue{determined} by the value of $x$.


\subsection{Putting the Theory to the Test}
In order to get a better feeling for linear regression, we want to test it to investigate the factors that
determine the fuel consumption of cars.  \myFig{cars.csv} shows the head of the data file 
``\href{https://www.statlearning.com/s/Auto.csv}{\texttt{Auto.csv}}''
which I have adapted from the file
\\[0.2cm]
\hspace*{1.3cm}
\href{https://www.statlearning.com/s/Auto.csv}{\texttt{https://www.statlearning.com/s/Auto.csv}}.
\\[0.2cm]
\myFig{cars.csv} shows the column headers and the first ten data entries contained in this file.  
Altogether, this file contains data of 392 different car models.

\begin{figure}[!ht]
\centering
\begin{Verbatim}[ frame         = lines, 
                  framesep      = 0.3cm, 
                  firstnumber   = 1,
                  labelposition = bottomline,
                  numbers       = left,
                  numbersep     = -0.2cm,
                  xleftmargin   = 0.3cm,
                  xrightmargin  = 0.3cm,
                ]
     mpg, cyl, displacement,    hp, weight,  acc, year, name
    18.0,   8,        307.0, 130.0, 3504.0, 12.0,   70, chevrolet chevelle malibu
    15.0,   8,        350.0, 165.0, 3693.0, 11.5,   70, buick skylark 320
    18.0,   8,        318.0, 150.0, 3436.0, 11.0,   70, plymouth satellite
    16.0,   8,        304.0, 150.0, 3433.0, 12.0,   70, amc rebel sst
    17.0,   8,        302.0, 140.0, 3449.0, 10.5,   70, ford torino
    15.0,   8,        429.0, 198.0, 4341.0, 10.0,   70, ford galaxie 500
    14.0,   8,        454.0, 220.0, 4354.0,  9.0,   70, chevrolet impala
    14.0,   8,        440.0, 215.0, 4312.0,  8.5,   70, plymouth fury iii
    14.0,   8,        455.0, 225.0, 4425.0, 10.0,   70, pontiac catalina
    15.0,   8,        390.0, 190.0, 3850.0,  8.5,   70, amc ambassador dpl
\end{Verbatim}
\vspace*{-0.3cm}
\caption{The head of the file \texttt{cars.csv}.}
\label{fig:cars.csv}
\end{figure}

The file ``\texttt{cars.csv}'' is part of the data set accompanying the excellent book 
\href{https://www.statlearning.com}{Introduction to Statistical Learning} by Gareth James,
Daniela Witten, Trevor Hastie, and Robert Tibshirani \cite{james:2014}.  The file
\href{https://www.statlearning.com/s/Auto.csv}{\texttt{cars.csv}} contains the fuel consumption of a number of
different cars that were in widespread use during 
the seventies and early eighties of the last century.  The first column of this data set gives the 
\blue{miles per gallon} of a car, i.e.~the number of miles a car can drive with one gallon of gas.  Note that
this number is in \blue{reciprocal} relation to the fuel consumption:  If a car $\mathrm{A}$ can drive \textbf{twice} as many miles per gallon
than another car $\mathrm{B}$, then the fuel consumption of $\mathrm{A}$ is \textbf{half} of the fuel consumption of
$\mathrm{B}$. Furthermore, besides the miles per gallon, for every car the following other parameters are listed:
\begin{enumerate}
\item $\mathtt{cyl}$ is the number of cylinders,
\item $\mathtt{displacement}$ is the engine displacement in
      \href{https://en.wikipedia.org/wiki/Cubic_inch}{cubic inches},  
      (100 cubic inch is $1.638\,706\,4$ litres)
\item $\mathtt{hp}$ is the engine power given in units of \href{https://en.wikipedia.org/wiki/Horsepower}{horsepower},
\item $\mathtt{weight}$ is the weight in \href{https://en.wikipedia.org/wiki/Pound_(mass)}{pounds} (1 pound is
      the same as $0.453\,592\,37$ kg), 
\item $\mathtt{acc}$ is the acceleration given as the time in seconds needed to accelerate from 0 miles per
      hour to 60 miles per hour,
\item $\mathtt{year}$ is the year in which the model was introduced, and
\item $\mathtt{name}$ is the name of the model.
\end{enumerate}
Our aim is to determine what part of the fuel consumption of a car is explained by its engine displacement.
To this end, I have written the function \texttt{simple\_linear\_regression} shown in \myFig{simple_linear_regression.py}.


\begin{figure}[!ht]
\centering
\begin{minted}[ frame         = lines, 
                framesep      = 0.3cm, 
                firstnumber   = 1,
                bgcolor       = sepia,
                numbers       = left,
                numbersep     = -0.2cm,
                xleftmargin   = 0.0cm,
                xrightmargin  = 0.0cm,
              ]{python3}
    def simple_linear_regression(X, Y):
        """
        This function implements linear regression.
        
        * X:     explaining variable, numpy array
        * Y:     dependent  variable, numpy array   
    
        Output: The R2 value of the linear regression.
        """
        m      = len(X)
        xMean  = np.mean(X);
        yMean  = np.mean(Y);
        theta1 = np.sum( (X - xMean) * (Y - yMean) ) / np.sum((X - xMean) ** 2)
        theta0 = yMean - theta1 * xMean;
        TSS    = np.sum((Y - yMean) ** 2)
        RSS    = np.sum((theta1 * X + theta0 - Y) ** 2)
        R2     = 1 - RSS / TSS;
        return R2
\end{minted}
\vspace*{-0.3cm}
\caption{Simple Linear Regression}
\label{fig:simple_linear_regression.py}
\end{figure}


\noindent
The function $\mathtt{simple\_linear\_regression}$ takes two arguments:
\begin{enumerate}[(a)]
\item $\mathtt{X}$ is a \texttt{NumPy} array containing the independent variable.
\item $\mathtt{Y}$ is a \texttt{NumPy} array containing the dependent variable.
\end{enumerate}
The implementation of the function $\mathtt{simple\_linear\_regression}$ works as follows:
\begin{enumerate}
\item $\mathtt{m}$ is the number of data that are present in the array $\texttt{X}.$
\item $\mathtt{xMean}$ is the mean value $\bar{x}$ of the independent variable $\mathtt{x}$.
\item $\mathtt{yMean}$ is the mean value $\bar{\mathbf{y}}$ of the dependent variable $\mathbf{y}$. 
\item The coefficient $\mathtt{theta1}$ is computed according to Equation \ref{eq:theta1}, which is repeated
      here for convenience:
      \\[0.2cm]
      \hspace*{1.3cm}
      $\ds\vartheta_1 = \frac{\sum\limits_{i=1}^m \bigl(x_i - \bar{\mathbf{x}}\bigr) \cdot \bigl(y_i - \bar{\mathbf{y}}\bigr)}{
                        \sum\limits_{i=1}^m \bigl(x_i - \bar{\mathbf{x}}\bigr)^2}  
      $.
      \\[0.2cm]
      Note that the expression \texttt{(X - xMean)} computes an array of the same shape as \texttt{X}
      by subtracting \texttt{xMean} from every entries of \texttt{X}.
      Next, the expression \texttt{(X - xMean) * (Y - yMean)} computes the elementwise product
      of the arrays \texttt{X - xMean} and \texttt{Y - yMean}.
      The expression \texttt{(X-xMean)**2} computes the elementwise squares of the array \texttt{X - xMean}.
      Finally, the function \texttt{sum} computes the sum of all the elements of an array.
\item The coefficient $\mathtt{theta0}$ is computed according to Equation \ref{eq:theta0}, which reads
      \\[0.2cm]
      \hspace*{1.3cm}
      $\ds\vartheta_0 = \bar{\mathbf{y}} - \vartheta_1 \cdot \bar{\mathbf{x}}$.
\item $\mathtt{TSS}$ is the \blue{total sum of squares} and is computed using the formula
      \\[0.2cm]
      \hspace*{1.3cm}
      $\ds\mathtt{TSS} = \sum\limits_{i=1}^m \bigl(y_i - \bar{\mathbf{y}}\bigr)^2$.
\item $\mathtt{RSS}$ is the \blue{residual sum of squares} and is computed as
      \\[0.2cm]
      \hspace*{1.3cm}
      $\ds\mathtt{RSS} := \sum\limits_{i=1}^m \bigl(\vartheta_1 \cdot x_i + \vartheta_0 - y_i\bigr)^2$.
\item $\mathtt{R2}$ is the $\mathtt{R}^2$ statistic and measures the \blue{proportion of the explained variance}.
      It is computed using the formula
      \\[0.2cm]
      \hspace*{1.3cm}
      $\ds \mathtt{R}^2 = \frac{\mathtt{TSS} - \mathtt{RSS}}{\mathtt{TSS}}$.
\end{enumerate}


\begin{figure}[!ht]
\centering
\begin{minted}[ frame         = lines, 
                framesep      = 0.3cm, 
                firstnumber   = 1,
                bgcolor       = sepia,
                numbers       = left,
                numbersep     = -0.2cm,
                xleftmargin   = 0.8cm,
                xrightmargin  = 0.8cm,
              ]{python3}
    import csv
    import numpy as np
    
    with open('cars.csv') as input_file:
        reader       = csv.reader(input_file, delimiter=',')
        line_count   = 0
        kpl          = []
        displacement = []
        for row in reader:
            if line_count != 0:
                kpl         .append(float(row[0]) * 0.00425144) 
                displacement.append(float(row[2]) * 0.0163871)
                line_count += 1
    m  = len(displacement)
    X  = np.array(displacement)
    Y  = np.array([1 / kpl[i] for i in range(m)])
    R2 = simple_linear_regression(X, Y)
    print(f'The explained variance is {R2}%')
\end{minted}
\vspace*{-0.3cm}
\caption{Calling the function $\mathtt{simple\_linear\_regression}$.}
\label{fig:simple_linear_regression.py:test}
\end{figure}
In order to use the function we can use the code that is shown in 
\myFig{simple_linear_regression.py:test}.
\begin{enumerate}
\item We import the module \texttt{csv} in order to be able to read the \textsc{Csv} file ``\texttt{cars.csv}''
      conveniently.
\item We import the module \texttt{numpy} in order to use \texttt{NumPy} arrays.
\item We open the file ``\texttt{cars.csv}''.
\item This file is processed as a \textsc{Csv} file where different columns are separated by the character 
      ``\texttt{,}''.
\item \texttt{kpl} is a list of the numbers that appear in the first column of the \textsc{Csv} file.
      The numbers in the \textsc{Csv} file are interpreted as the \blue{miles per gallon} of a car.
      These numbers are converted into metric units, i.e. how many kilometers a car can run on a litre.
\item \texttt{displacement} is a list the numbers appearing in the third column of the \textsc{CSV} file.
      These numbers are interpreted as the \emph{engine displacement} in cubic inches.
      These numbers are converted to litres.
\item The first line of the \textsc{Csv} file contains a header.  This header is skipped.
      In order to do so we use the variable \texttt{line\_count}.
\item \texttt{m} is the number of data pairs that have been read.
\item The independent variable \texttt{X} is given by the engine displacement. 
      In order to be able to use \texttt{NumPy} features later we convert this list into a \texttt{NumPy} array.
\item The dependent variable \texttt{Y} is given by the inverse of the variable \texttt{kph}.
\item Finally, the coefficient of determination $R^2$ is computed. Here we use the function \hspace*{\fill} \linebreak
      \texttt{simple\_linear\_regression} that is shown in \myFig{simple_linear_regression.py}.
\end{enumerate}
In the same way as we have computed the coefficient of determination that measures how the fuel consumption is
influenced by the engine displacement we can also compute the coefficient of determination for other variables
like the number of cylinders or the weight of the car.  
The resulting values are shown in Table \ref{tab:explained-variance}.  It
seems that, given the data in the file ``\texttt{cars.csv}'', the best indicator for the fuel consumption is
the $\mathtt{weight}$ of a car.  The $\mathtt{displacement}$, the power $\mathtt{hp}$ of an engine, and the
number of cylinders $\mathtt{cyl}$ are also good predictors.  But notice that the $\mathtt{weight}$ is the real
cause of fuel consumption:  If a car is heavy, it will also need a more powerful engine. Hence the
variable $\mathtt{hp}$ is correlated with the variable $\mathtt{weight}$ and will therefore also provide a
reasonable explanation of the fuel consumption, although the high engine power is not the most important cause
of the fuel consumption. 


\begin{table}
  \centering
  \begin{tabular}{|l|r|}
  \hline
  dependent variable  & explained variance   \\
  \hline
  \hline
  displacement        & 0.75                 \\
  \hline
  number of cylinders & 0.70                 \\
  \hline
  horsepower          & 0.73                 \\
  \hline
  weight              & 0.78                 \\
  \hline
  acceleration        & 0.21                 \\
  \hline
  year of build       & 0.31                 \\
  \hline
  \end{tabular}
  \caption[explained variance]{Explained variance for various dependent variables.}
  \label{tab:explained-variance}
\end{table}

\exercise
In this exercise we are going to investigate the concept of a
\href{https://en.wikipedia.org/wiki/Random_walk}{random walk}.
To understand the idea of a \blue{random walk}, imagine a person standing at the origin $(0,0)$ of a two
dimensional plane.  Every second the robot chooses a random direction from the set
\\[0.2cm]
\hspace*{1.3cm}
$\{ \texttt{north},  \texttt{east},  \texttt{south},  \texttt{west} \}$
\\[0.2cm]
and moves one unit into this direction.  For example, if the robot chooses the direction \texttt{north} in the
first step, it will move from $(0,0)$ to the position $(0,1)$.  Then, it could choose the direction
\texttt{west} to arrive at the position $(-1,1)$.  The link
\\[0.2cm]
\hspace*{1.3cm}
\href{https://upload.wikimedia.org/wikipedia/commons/c/cb/Random_walk_25000.svg}{https://upload.wikimedia.org/wikipedia/commons/c/cb/Random\_walk\_25000.svg}
\\[0.2cm]
shows a random walk with $25,000$ steps.  Our goal is to find a formula for the average distance of the robot
after its $i^\texttt{th}$ step.  Our assumption is that the average distance after $i$ steps satisfies the
following formula:
\\[0.2cm]
\hspace*{1.3cm}
$\mathtt{Distance}[i] = \alpha \cdot i^\beta$.
\\[0.2cm]
In order to find $\alpha$ and $\beta$ we will simulate a large number of random walks and compute
the average distance of the robot in all of these walks.  Then we use an appropriate variation of linear
regression to find the coefficients $\alpha$ and $\beta$ formula.  Complete the notebook
\\[0.2cm]
\hspace*{0.8cm}
\href{https://github.com/karlstroetmann/Artificial-Intelligence/blob/master/Python/5 Linear Regression/Random-Walk-Frame.ipynb}{
   Artificial-Intelligence/blob/master/Python/5 Linear Regression/Random-Walk-Frame.ipynb}.
\\[0.2cm]
to solve this exercise.
\eox

\section{General Linear Regression}
\index{general linear regression}
In practice, it is uncommon for an observed variable $y$ to depend solely on a single variable $x$. Taking the
example of a car's fuel consumption further, it is generally anticipated that the fuel consumption depends not
only on the car's engine displacement but also on various other parameters. For instance, it is plausible to 
assume that the car's mass has a  significant influence on its fuel consumption. To model such complex
relationships, we introduce the theory of \href{https://en.wikipedia.org/wiki/Linear_regression}{general linear regression}. 

In the context of a \blue{general regression problem}, we are provided with a list of $m$ pairs in the form $\langle\mathbf{x}^{(i)}, y^{(i)} \rangle$, where $\mathbf{x}^{(i)} \in \mathbb{R}^p$ and $y^{(i)} \in \mathbb{R}$ for all $i \in \{1, \ldots, m\}$. The term $p$ denotes the number of \blue{features}, and the pairs are referred to as the \blue{training examples}. Our objective is to compute a linear function
\\[0.2cm]
\hspace*{1.3cm}
$\ds F:\mathbb{R}^p \rightarrow \mathbb{R}$
\\[0.2cm]
such that $F\bigl(\mathbf{x}^{(i)}\bigr)$ approximates $y^{(i)}$ as accurately as possible for all $i \in \{1, \ldots, m\}$; that is, we aim to achieve
\\[0.2cm]
\hspace*{1.3cm}
$F\bigl(\mathbf{x}^{(i)}\bigr) \approx y^{(i)}$ \quad for all $i \in \{1, \ldots, m\}$.
\\[0.2cm]
In order to make the notation $F\bigl(\mathbf{x}^{(i)}\bigr) \approx y^{(i)}$ more precise, we
define the \blue{mean squared error} \index{mean squared error} \index{MSE}
\begin{equation}
  \label{eq:squared-error-1}
  \mathtt{MSE} := \frac{1}{m-1} \cdot \sum\limits_{i=1}^{m} \Bigl(F\bigl(\mathbf{x}^{(i)}\bigr) - y^{(i)}\Bigr)^2. 
\end{equation}
Then, given the list of training examples
$[\langle \mathbf{x}^{(1)}, y^{(1)} \rangle, \cdots, \langle \mathbf{x}^{(m)}, y^{(m)} \rangle]$, our goal is to
minimize the $\mathtt{MSE}$.   
In order to proceed, we assume that $F$ is given as 
\\[0.2cm]
\hspace*{1.3cm}
$\ds F(\mathbf{x}) = \sum\limits_{j=1}^p w_j \cdot x_j + b = \mathbf{x}^\top \cdot \mathbf{w} + b$ 
\quad where  $\mathbf{w} \in \mathbb{R}^p$ and $b\in\mathbb{R}$.
\\[0.2cm]
Here, the expression $\mathbf{x}^\top \cdot \mathbf{w}$ denotes the matrix product of the vector
$\mathbf{x}^\top$, which is viewed as a $1$-by-$p$ matrix, and the vector $\mathbf{w}$, where $\mathbf{w}$ is
viewed as a $p$-by-1 matrix.  Alternatively, this
expression could be interpreted as the dot product of the vector $\mathbf{x}$ and the vector $\mathbf{w}$.
At this point you might wonder why it is useful to introduce matrix notation here.  The reason is
that this notation shortens the formula and, furthermore, is more efficient to implement since most
programming languages used in machine learning have special library support for matrix operations.  
Provided the computer is equipped with a graphics card,  some
programming languages are even able to delegate matrix operations to the graphics card.  This results in a
considerable speed-up.

The definition of $F$ given above is the model used in
\href{https://en.wikipedia.org/wiki/Linear_regression}{linear regression}. 
Here, $\mathbf{w}$ is called the \blue{\color{blue}weight vector} and $b$ is called the \blue{\color{blue}bias}.  It turns
out that the notation can be simplified if we extend the $p$-dimensional feature vector $\mathbf{x}$ to a
$(p+1)$-dimensional vector $\mathbf{x}'$ such that
\\[0.2cm]
\hspace*{1.3cm}
$x_j' := x_j$ \quad for all $j\in\{1,\cdots,p\}$ \quad and \quad $x_{p+1}' := 1$.
\\[0.2cm]
To put it in words, the feature vector $\mathbf{x}'$ results from the feature vector $\mathbf{x}$ by appending
the number $1$ as a constant feature to every data point:
\\[0.2cm]
\hspace*{1.3cm}
$\mathbf{x}' = \langle x_1, \cdots, x_p, 1 \rangle^\top$ \quad where $\langle x_1, \cdots, x_p \rangle = \mathbf{x}^\top$.
\\[0.2cm]
Furthermore, we define 
\\[0.2cm]
\hspace*{1.3cm}
$\mathbf{w}' := \langle w_1, \cdots, w_p, b \rangle^\top$ \quad where $\langle w_1, \cdots, w_p \rangle = \mathbf{w}^\top$.
\\[0.2cm]
Then we have
\\[0.2cm]
\hspace*{1.3cm}
$F(\mathbf{x}) = \mathbf{x}^\top \cdot \mathbf{w} + b = \mathbf{x}'^\top \cdot \mathbf{w}'$.
\\[0.2cm]
Hence, the bias has been incorporated into the weight vector at the cost of appending the number $1$ at the end of
input vector $\mathbf{x}$.  This is also known as the \blue{bias trick}\index{bias trick} or
\blue{homogenization}\index{homogenization}.  As we want to use this simplification, from now on we assume that
the input vectors $\mathbf{x}^{(i)}$ have all been extended so that their last component is always $1$.  Then we can just write
$\mathbf{x}$ and $\mathbf{w}$ instead of $\mathbf{x}'$ and $\mathbf{w}'$.  Using this simplification,  we define the
function $F$ as
\\[0.2cm]
\hspace*{1.3cm}
$F(\mathbf{x}) := \mathbf{x}^\top \cdot \mathbf{w}$.
\\[0.2cm]
Now equation (\ref{eq:squared-error-1}) can be rewritten as follows:
\begin{equation}
  \label{eq:squared-error-2}
  \mathtt{MSE}(\mathbf{w}) = \frac{1}{m-1} \cdot \sum\limits_{i=1}^m \Bigl(\bigl(\mathbf{x}^{(i)})^\top \cdot \mathbf{w}  - y^{(i)}\Bigr)^2.
\end{equation}
Our aim is to rewrite the sum appearing in this equation as a scalar product of a vector with
itself.  To this end, we first define the vector $\mathbf{y}$ as follows:
\\[0.2cm]
\hspace*{1.3cm}
$\mathbf{y} := \langle y^{(1)}, \cdots, y^{(m)} \rangle^\top$.
\\[0.2cm]
Note that $\mathbf{y} \in \mathbb{R}^m$ since it has a component for all of the $m$ training
examples.  Next, we define the \blue{design matrix} \index{design matrix} $X$ as follows:
\\[0.2cm]
\hspace*{1.3cm}
$X := \left(
  \begin{array}{c}
    \bigl(\mathbf{x}^{(1)}\bigr)^\top  \\
    \vdots                         \\
    \bigl(\mathbf{x}^{(m)}\bigr)^\top
  \end{array}
  \right)   
$
\\[0.2cm]
In the literature, $X$ is also called the \blue{feature matrix}. \index{feature matrix}
If $X$ is defined in this way, the row vectors of the matrix $X$ are the transpositions of the vectors $\mathbf{x}^{(i)}$.
Then we have the following:
\\[0.2cm]
\hspace*{1.3cm}
$X \cdot \mathbf{w} - \mathbf{y} = \left(
  \begin{array}{c}
    \bigl(\mathbf{x}^{(1)}\bigr)^\top  \\
    \vdots                         \\
    \bigl(\mathbf{x}^{(m)}\bigr)^\top
  \end{array}
  \right) \cdot \mathbf{w} - \mathbf{y} = \left(
  \begin{array}{c}
    \bigl(\mathbf{x}^{(1)}\bigr)^\top \cdot \mathbf{w} - y^{(1)} \\
    \vdots                         \\
    \bigl(\mathbf{x}^{(m)}\bigr)^\top \cdot \mathbf{w} - y^{(m)}
  \end{array}
  \right)
$
\\[0.2cm]
Taking the square of the vector $X \cdot \mathbf{w} - \mathbf{y}$ we discover that
we can rewrite equation (\ref{eq:squared-error-2}) as follows:
\begin{equation}
  \label{eq:squared-error-3}
  \mathtt{MSE}(\mathbf{w}) = \frac{1}{m-1} \cdot \bigl(X \cdot \mathbf{w} - \textbf{y}\bigr)^\top \cdot 
                                            \bigl(X \cdot \mathbf{w} - \textbf{y}\bigr).
\end{equation}

\subsection{Some Useful Gradients}
In the last section, we have computed the mean squared error $\mathtt{MSE}(\mathbf{w})$ using equation
(\ref{eq:squared-error-3}).  Our goal is to minimize the $\mathtt{MSE}(\mathbf{w})$ by choosing the weight
vector $\mathbf{w}$ appropriately.  A necessary condition for $\mathtt{MSE}(\mathbf{w})$ to be minimal is 
\\[0.2cm]
\hspace*{1.3cm}
$\nabla \mathtt{MSE}(\mathbf{w}) = \mathbf{0}$,
\\[0.2cm]
i.e.~the \href{https://en.wikipedia.org/wiki/Gradient}{gradient} \index{gradient} of $\mathtt{MSE}(\mathbf{w})$ with respect to
$\mathbf{w}$ needs to be zero.  In order to prepare for the computation of 
$\nabla \mathtt{MSE}(\mathbf{w})$, we first compute the gradient of two simpler functions.

\subsubsection{Computing the Gradient of $f(\mathbf{x}) = \mathbf{x}^\top \cdot C \cdot \mathbf{x}$}
Suppose the function $f:\mathbb{R}^n \rightarrow \mathbb{R}$ is defined as
\\[0.2cm]
\hspace*{1.3cm}
$f(\mathbf{x}) := \mathbf{x}^\top \cdot C \cdot \mathbf{x}$ \quad where $C \in \mathbb{R}^{n \times n}$.
\\[0.2cm]
If we write the matrix $C$ as $C = (c_{i,j})_{i=1,\cdots,n \atop j=1,\cdots,n}$ and the vector
$\mathbf{x}$ as $\mathbf{x} = \langle x_1, \cdots, x_n \rangle^\top$,  then $f(\mathbf{x})$ can be
computed as follows:
\\[0.2cm]
\hspace*{1.3cm}
$\ds f(\mathbf{x}) = \sum\limits_{i=1}^n x_i \cdot \sum\limits_{j=1}^n c_{i,j} \cdot x_j 
                   = \sum\limits_{i=1}^n \sum\limits_{j=1}^n x_i \cdot c_{i,j} \cdot x_j
$.
\\[0.2cm]
We compute the partial derivative of $f$ with respect to $x_k$ and use the
\href{https://en.wikipedia.org/wiki/Product_rule}{product rule} together with the
definition of the \href{https://en.wikipedia.org/wiki/Kronecker_delta}{Kronecker delta} $\delta_{i,j}$, which
is defined as $1$ if $i = j$ and as $0$ otherwise:
\\[0.2cm]
\hspace*{1.3cm}
$ \delta_{i,j} := 
\left\{
\begin{array}[c]{ll}
  1 & \mbox{if $i = j$;} \\
  0 & \mbox{otherwise.}
\end{array}
\right.
$
\\[0.2cm]
Then the partial derivative of $f$ with respect to $x_k$, which is written
as $\ds\frac{\partial f}{\partial x_k}$, is computed as follows:
\\[0.2cm]
\hspace*{1.3cm}
$
\begin{array}[t]{lcl}
\ds \frac{\partial f}{\partial x_k} & = &
\ds \sum\limits_{i=1}^n \sum\limits_{j=1}^n \Bigl(
    \frac{\partial x_i}{\partial x_k} \cdot c_{i,j} \cdot x_j + x_i \cdot c_{i,j} \cdot \frac{\partial x_j}{\partial x_k}
    \Bigr) \\[0.5cm]
& = &
\ds \sum\limits_{i=1}^n \sum\limits_{j=1}^n \Bigl(
    \delta_{i,k} \cdot c_{i,j} \cdot x_j + x_i \cdot c_{i,j} \cdot \delta_{j,k} \Bigr) \\[0.5cm]
& = &
\ds \sum\limits_{j=1}^n c_{k,j} \cdot x_j + \sum\limits_{i=1}^n x_i \cdot c_{i,k} \\[0.5cm]
& = &
  \bigl(C \cdot \mathbf{x}\bigr)_k + \bigl(C^\top \cdot \mathbf{x}\bigr)_k
\end{array}
$
\\[0.2cm]
Hence we have shown that 
\\[0.2cm]
\hspace*{1.3cm}
$\ds \nabla f(\mathbf{x}) = (C + C^\top) \cdot \mathbf{x}$.
\\[0.2cm]
If the matrix $C$ is \blue{symmetric}, i.e.~if $C = C^\top$, this simplifies to
\\[0.2cm]
\hspace*{1.3cm}
$\ds \nabla f(\mathbf{x}) = 2 \cdot C \cdot \mathbf{x}$.
\\[0.2cm]
Next, if the function $g: \mathbb{R}^n \rightarrow \mathbb{R}$ is defined as 
\\[0.2cm]
\hspace*{1.3cm}
$g(\mathbf{x}) := \mathbf{b}^\top \cdot A \cdot \mathbf{x}$, \quad where $\mathbf{b} \in \mathbb{R}^m$ and $A \in \mathbb{R}^{m \times n}$,
\\[0.2cm]
then a similar, but slightly easier, calculation shows that
\\[0.2cm]
\hspace*{1.3cm}
$\ds \nabla g(\mathbf{x}) = A^\top \cdot \mathbf{b}$.

\exercise
Prove this equation.

\subsection{Deriving the Normal Equation}
Next, we will derive the so called \blue{normal equation} \index{normal equation}
for linear regression.  To this end, we first
expand the product in equation (\ref{eq:squared-error-3}):
\\[0.2cm]
\hspace*{0.3cm}
$
\begin{array}[t]{lcll}
 \mathtt{MSE}(\mathbf{w}) & = & 
 \ds \frac{1}{m-1} \cdot \bigl(X \cdot \mathbf{w} - \textbf{y}\bigr)^\top \cdot \bigl(X \cdot \mathbf{w} - \textbf{y}\bigr) 
 \\[0.5cm]
 & = & 
 \ds \frac{1}{m-1} \cdot \bigl(\mathbf{w}^\top \cdot X^\top - \textbf{y}^\top\bigr) \cdot \bigl(X \cdot \mathbf{w} - \textbf{y}\bigr) 
 & \hspace*{-1.5cm}\mbox{since $(A \cdot B)^\top = B^\top \cdot A^\top$}
 \\[0.5cm]
 & = & 
 \ds \frac{1}{m-1} \cdot \bigl(\mathbf{w}^\top \cdot X^\top \cdot X \cdot \mathbf{w} 
                             - \textbf{y}^\top \cdot X \cdot \mathbf{w} 
                             - \mathbf{w}^\top \cdot X^\top \cdot \mathbf{y}
                             + \mathbf{y}^\top \cdot \mathbf{y}
                       \bigr)
 \\[0.5cm]
 & = & 
 \ds \frac{1}{m-1} \cdot \bigl(\mathbf{w}^\top \cdot X^\top \cdot X \cdot \mathbf{w} 
                             - 2 \cdot \textbf{y}^\top \cdot X \cdot \mathbf{w} 
                             + \mathbf{y}^\top \cdot \mathbf{y}
                       \bigr)
 & \hspace*{-1.5cm}\mbox{since $\mathbf{w}^\top \cdot X^\top \cdot \mathbf{y} = \textbf{y}^\top \cdot X \cdot \mathbf{w}$}
\end{array}
$
\\[0.2cm]
The fact that 
\\[0.2cm]
\hspace*{1.3cm}
$\mathbf{w}^\top \cdot X^\top \cdot \mathbf{y} = \textbf{y}^\top \cdot X \cdot \mathbf{w}$
\\[0.2cm]
might not be immediately obvious.  It follows from two facts:
\begin{enumerate}
\item For two matrices $A$ and $B$ such that the matrix product $A \cdot B$ is defined we have 
      \\[0.2cm]
      \hspace*{1.3cm}
      $(A \cdot B)^\top = B^\top \cdot A^\top$.
\item The matrix product $\mathbf{w}^\top \cdot X^\top \cdot \mathbf{y}$ is a real number.  The transpose $r^\top$ of a real number $r$ is the number
      itself, i.e.~$r^\top = r$ for all $r \in \mathbb{R}$.  Therefore, we have
      \\[0.2cm]
      \hspace*{1.3cm}
      $\mathbf{w}^\top \cdot X^\top \cdot \mathbf{y} = 
\bigl(\mathbf{w}^\top \cdot X^\top \cdot \mathbf{y}\bigr)^\top =
\mathbf{y}^\top \cdot X \cdot \mathbf{w}
$.
\end{enumerate}
Hence we have shown that
\begin{equation}
  \label{eq:squared-error-4}
  \mathtt{MSE}(\mathbf{w}) = \ds \frac{1}{m-1} \cdot \Bigl(\mathbf{w}^\top \cdot \bigl(X^\top \cdot X\bigr) \cdot \mathbf{w} 
                                             - 2 \cdot \textbf{y}^\top \cdot X \cdot \mathbf{w} 
                                             + \mathbf{y}^\top \cdot \mathbf{y}
                                        \Bigr)
\end{equation}
holds.  The matrix $X^\top \cdot X$ used in the first term is symmetric because
\\[0.2cm]
\hspace*{1.3cm}
$\bigl(X^\top \cdot X\bigr)^\top = X^\top \cdot \bigl(X^\top\bigr)^\top = X^\top \cdot X$.
\\[0.2cm]
Using the results from the previous section, i.e.~
\\[0.2cm]
\hspace*{1.3cm}
$\nabla \bigl(\mathbf{x} \mapsto \mathbf{x}^\top \cdot C \cdot \mathbf{x}\bigr) = 2 \cdot C \cdot \mathbf{x}$
\quad provided that $C^\top = C$ \quad and
\\[0.2cm]
\hspace*{1.3cm}
$\nabla \bigl(\mathbf{x} \mapsto \mathbf{b}^\top \cdot A \cdot \mathbf{x}\bigr) = A^\top \cdot \mathbf{b}$,
\\[0.2cm]
we can now compute the gradient of $\mathtt{MSE}(\mathbf{w})$ with respect to
$\mathbf{w}$.  The gradient of the term
\\[0.2cm]
\hspace*{1.3cm}
$\mathbf{w}^\top \cdot \bigl(X^\top \cdot X\bigr) \cdot \mathbf{w}$
\\[0.2cm]
with respect to $\mathbf{w}$ is $2 \cdot \bigl(X^\top \cdot X\bigr) \cdot \mathbf{w}$, while the gradient of
the term 
\\[0.2cm]
\hspace*{1.3cm}
$\textbf{y}^\top \cdot X \cdot \mathbf{w}$
\\[0.2cm]
is $X^\top \cdot \mathbf{y}$.  Since the gradient of $\mathbf{y}^\top \cdot \mathbf{y}$ with respect to
$\mathbf{w}$ vanishes, the gradient of $\mathtt{MSE}(\mathbf{w})$ is given as
\\[0.2cm]
\hspace*{1.3cm}
$\ds \nabla \mathtt{MSE}(\mathbf{w}) = \frac{2}{m-1} \cdot \Bigl(X^\top \cdot X \cdot \mathbf{w} - X^\top \cdot \mathbf{y}\Bigr)$.
\\[0.2cm]
If the squared error $\mathtt{MSE}(\mathbf{w})$ has a minimum for the weights $\mathbf{w}$, then we must have
\\[0.2cm]
\hspace*{1.3cm}
$\nabla \mathtt{MSE}(\mathbf{w}) = \mathbf{0}$.
\\[0.2cm]
This leads to the equation
\\[0.2cm]
\hspace*{1.3cm}
$\ds\frac{2}{m-1} \cdot \Bigl(X^\top \cdot X \cdot \mathbf{w} - X^\top \cdot \mathbf{y}\Bigr) = \mathbf{0}$.
\\[0.2cm]
This equation can be rewritten as
\begin{equation}
  \label{eq:normal-equation}
 \colorbox{red}{\framebox{\colorbox{yellow}{\framebox{
 $\ds\bigl(X^\top \cdot X\bigr) \cdot \mathbf{w} = X^\top \cdot \mathbf{y}$}}}} 
\end{equation}
and is known as the \blue{normal equation}.  \index{normal equation}

\remark
Although the matrix $X^\top \cdot X$ will often be invertible, for numerical reasons it is not
advisable to rewrite the normal equation as
\\[0.2cm]
\hspace*{1.3cm}
$\mathbf{w} = \bigl(X^\top \cdot X)^{-1} \cdot X^\top \cdot \mathbf{y}$.
\\[0.2cm]
Instead, when solving the normal equation we will use the \textsl{Python} function $\texttt{numpy.linalg.solve}(A,b)$, which
takes a matrix $A \in \mathbb{R}^{n \times n}$ and a vector $\mathbf{b} \in \mathbb{R}^n$ and solves the equation
\\[0.2cm]
\hspace*{1.3cm}
$A \cdot \mathbf{x} = \mathbf{b}$.  \eox

\subsection{Implementation}
\myFig{linear-regression.stlx} shows an implementation of general linear regression.
The function
\\[0.2cm]
\hspace*{1.3cm}
$\mathtt{linear\_regression}(\mathtt{fileName},\; \mathtt{target},\; \mathtt{explaining},\; \mathtt{f})$
\\[0.2cm]
takes four arguments:
\begin{enumerate}
\item \texttt{fileName} is a string that is interpreted as the name of a \textsc{Csv} file containing the data.
\item \texttt{target} is an integer that specifies the column that contains the dependent variable. 
\item \texttt{explaining} is a list of integers.  These integers specify the columns of the \textsc{Csv} file
      that contain the independent variables.  These are also called the \blue{explaining variables}.
\item \texttt{f} is a function that takes one floating point argument and outputs one floating point function.
      This function is used to modify the dependent variable.

      Later, when we call the function \texttt{linear\_regression} to investigate the fuel consumption, we will
      use the function
      \\[0.2cm]
      \hspace*{1.3cm}
      $\ds x \mapsto \frac{1}{x}$
      \\[0.2cm]
      to transform the variable \textsl{miles per gallon} into a variable expressing the fuel consumption.  The
      reason is that there is reciprocal relation between the number of miles that a car drives on one gallon
      of gasoline and the fuel consumption:  If you drive only a few miles with one gallon of gas, then your
      fuel consumption is high.
\end{enumerate}

\begin{figure}[!ht]
\centering
\begin{minted}[ frame         = lines, 
                framesep      = 0.3cm, 
                firstnumber   = 1,
                bgcolor       = sepia,
                numbers       = left,
                numbersep     = -0.2cm,
                xleftmargin   = 0.8cm,
                xrightmargin  = 0.8cm,
              ]{python3}
    import csv
    import numpy as np
    
    def linear_regression(fileName, target, explaining, f):
        with open(fileName) as input_file:
            reader     = csv.reader(input_file, delimiter=',')
            line_count = 0
            goal       = []
            Causes     = []
            for row in reader:
                if line_count != 0:  
                    goal  .append(f(float(row[target])))  
                    Causes.append([float(row[i]) for i in explaining] + [1.0]) 
                line_count += 1
        m = len(goal)
        X = np.array(Causes)
        y = np.array(goal)
        w = np.linalg.solve(X.T @ X, X.T @ y)
        RSS   = np.sum((X @ w - y) ** 2)
        yMean = np.sum(y) / m
        TSS   = sum((y - yMean) ** 2)
        R2    = 1 - RSS / TSS
        return R2
    
    def main():
        explaining = [1, 2, 3, 4, 5, 6]
        R2 = linear_regression("cars.csv", 0, explaining, lambda x: 1/x)
        print(f'portion of explained variance : {R2}')
\end{minted}
\vspace*{-0.3cm}
\caption{General linear regression.}
\label{fig:linear-regression.stlx}
\end{figure}

\noindent
The function \texttt{linear\_regression} works as follows:
\begin{enumerate}
\item It reads the specified \textsc{Csv} file line by line and stores the data in the variables \texttt{goal} and
      \texttt{Causes}.  This is done by creating a \texttt{csv} reader in line 6.  This reader returns the
      entries in the specified input file line by line in the for loop in line 10. 
      \begin{enumerate}
      \item \texttt{goal} is a list containing the data of the dependent variable that was specified by
            \texttt{target}.  This list is initialized in line 8.  It is filled with data in line 12.
            Note that the values stored in \texttt{goal} are transformed by the function \texttt{f}.
      \item \texttt{Causes} is a list of lists containing the data of the explaining variables.
            Every row in the \textsc{Csv} file corresponds to one list in the list Causes.
            Note also that we append the number $1.0$ to each of these lists.  This corresponds to adding a
            constant feature to our data and it enables us to use the normal equations as we have derived them.
      \end{enumerate}
\item \texttt{m} is the number of data pairs and is computed in line 15.
\item \texttt{Causes} is transformed into the \texttt{NumPy} matrix \texttt{X} in line 16. 
\item \texttt{goal} is transformed into the \texttt{NumPy} array \texttt{y} in line 17.
\item The normal equation $(X^\top \cdot X) \cdot \mathbf{w} = X^\top \cdot \mathbf{y}$ is formulated and
      solved using the function $\mathtt{np.linalg.solve}$ in line 18.
     
      Note that \texttt{X.T} is the transpose of the matrix \texttt{X}.  The operator \texttt{@} computes the
      matrix product.  Hence the expression \texttt{X.T @ X} is interpreted as 
      $X^\top \cdot X$.  Similarly, the expression \texttt{X.T @ y} is interpreted as $X^\top \cdot \mathbf{y}$.
\item The expression $\texttt{(X @ w - y)}$ is the difference between the predictions of the linear model and the observed
      values $\mathtt{y}$.  By squaring it and the summing over all entries of the resulting vector we compute
      is the residual sum of squares $\texttt{RSS}$ in line 19.
\item $\mathtt{yMean}$ is the mean value of the variable $\mathbf{y}$.
\item $\mathtt{TSS}$ is the total sum of squares.
\item $\mathtt{R2}$ is the proportion of the explained variance.
\end{enumerate}
When we run the program shown in \myFig{linear-regression.stlx} with the data stored in \texttt{cars.csv},
which had been discussed previously, then the proportion of explained variance is $88 \mathtt{\symbol{37}}$.  Considering that our data does
not take the aerodynamics of the cars into take account, this seems like a reasonable result.  A Jupyter notebook
containing a similar program is available at
\\[0.2cm]
\hspace*{1.3cm}
\href{https://github.com/karlstroetmann/Artificial-Intelligence/blob/master/Python/5 Linear Regression/6-Linear-Regression.ipynb}{\texttt{https://github.com/karlstroetmann/Artificial-Intelligence/\\
\hspace*{2.2cm}
 blob/master/Python/5 Linear Regression/6-Linear-Regression.ipynb}}.

\section{Polynomial Regression}
\label{sec:polynomial_regression}

Sometimes the model of linear regression is not flexible enough to capture the underlying patterns in data. One
effective way to extend its capabilities without changing the fundamental algorithm is to add
\blue{higher order features}\index{higher order features}.
In general, these are products or powers of the given features. For example, assume we have a single feature
$x$ and a dependent variable $y$. If the relationship between $x$ and $y$ is non-linear, we can extend the
feature matrix by adding powers of $x$, such as $x^2, x^3$, etc. 
A model of the form:
\[
y = w_0 + w_1 \cdot x + w_2 \cdot x^2
\]
is still considered a \blue{linear model} in the context of machine learning because it is linear in terms of the parameters $w_0, w_1, w_2$, even though it describes a non-linear curve (a parabola) in terms of the feature $x$. Therefore, we can still use the \blue{normal equation} to solve for the optimal weights.

\subsection{Case Study: German Civil Servant Salaries}
To illustrate this concept, we will examine a real-world dataset: the salaries of German Federal Civil
Servants. In Germany, state employees are paid according to a table known as the
\href{https://oeffentlicher-dienst.info/c/t/rechner/beamte/bund?id=beamte-bund-2025&matrix=1}{Bundes-Besoldungs-Ordnung}.
The pay grades range from A3 to A16.  Typically, the salary gaps widen as one moves up the hierarchythe jump
from A15 to A16 is significantly larger than the jump from A3 to A4. This suggests that a simple straight line
(linear model) might be insufficient to predict the salary based on the grade. 

\subsubsection{Data Preparation}
First, we define our dataset manually using the official values valid from April 2025. The variable \texttt{grades\_list} contains the pay grades (3 to 16), and \texttt{salary\_list} contains the corresponding maximum monthly salaries in Euro.

\begin{figure}[!ht]
\centering
\begin{minted}[
    frame=lines,
    framesep=2mm,
    baselinestretch=1.2,
    bgcolor=sepia,
    fontsize=\footnotesize,
    linenos
]{python}
# Grade (A3 to A16)
grades_list = list(range(3, 16+1))

# Maximum Monthly Salary in Euro (Bund 2025)
salary_list = [
    3156.42, 3267.76, 3369.46, 3526.11, 3794.62, 
    4101.79, 4411.80, 4917.77, 5458.71, 5989.42, 
    6620.73, 7182.11, 8081.71, 8978.48
]
\end{minted}
\caption{Data entry for German Civil Servant Salaries.}
\label{fig:salary_data}
\end{figure}

\subsubsection{The Linear Model (Degree 1)}
We first attempt to model this data using simple linear regression. We construct a feature matrix $X_{linear}$ where each row consists of the grade $x$ and the constant $1$ (for the bias term).
\[
\mathbf{x}_{\text{linear}} = [x, 1]
\]
We solve for the weights using the standard linear algebra library in NumPy.

\begin{figure}[!ht]
\centering
\begin{minted}[
    frame=lines,
    framesep=2mm,
    baselinestretch=1.2,
    bgcolor=sepia,
    fontsize=\footnotesize,
    linenos
]{python}
import numpy as np

# Construct X as a list of lists: [[x, 1], ...]
X_linear = [[x, 1.0] for x in grades_list]

# Convert to NumPy arrays
X_linear = np.array(X_linear)
Y        = np.array(salary_list)

# Solve: (X^T * X) * w = X^T * y
w_linear = np.linalg.solve(X_linear.T @ X_linear, X_linear.T @ Y)

print(f"Linear Weights: Slope={w_linear[0]:.2f}, Intercept={w_linear[1]:.2f}")
\end{minted}
\caption{Implementing Simple Linear Regression.}
\label{fig:linear_implementation}
\end{figure}

The resulting $R^2$ score for this model is approximately $0.93$. While this appears high, a visual inspection
of Figure \ref{fig:salaries.png} on page \pageref{fig:salaries.png} reveals that the model significantly
underestimates the salaries of top-level civil servants. The linear model cannot capture the accelerating
growth (curvature) of the salary structure. 

\subsubsection{The Polynomial Model (Degree 2)}
To improve the model, we introduce a \textbf{quadratic term}. We hypothesize that the salary relates to the pay
grade via a polynomial of degree 2:
\[
y = w_2 \cdot x^2 + w_1 \cdot x + w_0
\]
To implement this, we do not need a new algorithm. We simply modify how we construct our feature matrix by providing \textbf{higher order features}. We construct a new matrix $X_{poly}$ where each row contains the square of the grade, the grade itself, and the bias constant:
\[
\mathbf{x}_{\text{poly}} = [x^2, x, 1]
\]
This time, the resulting $R^2$ score is better than $0.9999\%$.

\begin{figure}[!ht]
\centering
\begin{minted}[
    frame=lines,
    framesep=2mm,
    baselinestretch=1.2,
    bgcolor=sepia,
    fontsize=\footnotesize,
    linenos
]{python}
# Construct X as a list of lists: [[x^2, x, 1], ...]
X_poly = [[x**2, x, 1.0] for x in grades_list]

# Convert to NumPy array
X_poly = np.array(X_poly)

# Solve using Normal Equation
w_poly = np.linalg.solve(X_poly.T @ X_poly, X_poly.T @ Y)

print(f"Poly Weights: Quad={w_poly[0]:.2f}, Lin={w_poly[1]:.2f}, Bias={w_poly[2]:.2f}")
\end{minted}
\caption{Implementing Polynomial Regression (Degree 2).}
\label{fig:poly_implementation}
\end{figure}


\begin{figure}[!th]
\epsfig{file=Figures/salaries.png, scale=0.37}
\caption{Attempting to match salaries with a linear and a quadratic model.}
\label{fig:salaries.png}
\end{figure}



\exercise
The file ``\texttt{trees.csv}'', which is available at
\\[0.2cm]
\hspace*{1.3cm}
\href{https://github.com/karlstroetmann/Artificial-Intelligence/blob/master/Python/5 Linear Regression/trees.csv}{\texttt{Artificial-Intelligence/blob/master/Python/5 Linear Regression/trees.csv}},
\\[0.2cm]
contains data about 31 lovely cherry trees from the Allegheny National Forest in Pennsylvania that have fallen
prey to a \href{https://en.wikipedia.org/wiki/The_Texas_Chain_Saw_Massacre}{chainsaw massacre}.  I have taken this data from
\\[0.2cm]
\hspace*{1.3cm}
\href{http://www.statsci.org/data/general/cherry.txt}{\texttt{http://www.statsci.org/data/general/cherry.txt}}.
\begin{enumerate}
\item The first column of this \textsc{Csv} file contains the diameter of these trees at a height of 54 inches above
      the ground.
\item The second column lists the heights of these trees in foot.
\item The third column list the volume of wood that has been harvested from these trees.  This volume is given
      in cubic inches.
\end{enumerate}
Try to derive a model that estimates the volume of the trees from the diameter and the height.
\eox


\exercise
The file ``\texttt{nba.csv}'', which is available at
\\[0.2cm]
\hspace*{0.3cm}
\href{https://github.com/karlstroetmann/Artificial-Intelligence/blob/master/Python/nba.csv}{\texttt{https://github.com/karlstroetmann/Artificial-Intelligence/blob/master/Python/nba.csv}},
\\[0.2cm]
contains various data about professional basket ball players.
\begin{enumerate}
\item The first column gives the name of the player.
\item The second column specifies the position of the player.
\item The third column lists the height of the player.
\item The fourth column contains the weight of the player.
\item The fifth column shows the age of each player.
\end{enumerate}
To what extend can you predict the weight of a player given his height and his age?
\eox

\section{Overfitting and Underfitting in Linear Regression}

In machine learning, our primary goal is not merely to do well on the data we have seen (the training data),
but to generalize to new data we have not seen (the test data). When building regression models, we often
encounter two primary failure modes: 
\begin{enumerate}
\item \textbf{Underfitting (High Bias):} This occurs when the model is too simple to capture the underlying
      structure of the data. For example, trying to fit a straight line to a parabolic curve. The model
      performs poorly on both the training set and the test set. 
\item \textbf{Overfitting (High Variance):} This occurs when the model is too complex relative to the
      amount of training data. The model begins to "memorize" the random noise and fluctuations in the training
      set rather than learning the true signal. As a result, it performs exceptionally well on the training
      data but fails to generalize, leading to poor performance on the test set. 
\end{enumerate}
To illustrate these concepts, we analyze a real-world dataset using the Python library
\texttt{scikit-learn}. The corresponding Jupyter Notebook is \texttt{9-Overfitting-SK.ipynb}. 

\section{Case Study: Baseball Player Salaries}
We utilize the \textbf{Hitters} dataset, which contains statistics for Major League Baseball players (e.g.,
hits, runs, years in the league) and their salaries. Our objective is to predict a player's salary based on
these statistics.  To demonstrate overfitting, we will simulate a \emph{low data} scenario where the number of
features is high relative to the number of training examples. We will incrementally add features to our model
and observe the divergence between training performance and test performance. 

\subsection{Data Preprocessing}
First, we load the data using \texttt{pandas}. We perform necessary cleaning steps, such as removing columns that are not useful (like row names), dropping rows with missing salary information, and converting categorical variables (like League or Division) into numerical values using One-Hot Encoding.

\begin{figure}[!ht]
\centering
\begin{minted}[
    frame=lines,
    framesep=2mm,
    baselinestretch=1.2,
    bgcolor=sepia,
    fontsize=\footnotesize,
    linenos
]{python}
import pandas as pd

# Load DataFrame and drop unrelated index column
df = pd.read_csv("Hitters.csv")
df = df.drop(columns=["rownames"])

# Drop rows where the target 'Salary' is missing
df = df.dropna(subset=['Salary'])

# Convert categorical variables to numeric (0/1)
df = pd.get_dummies(df, drop_first=True)
\end{minted}
\caption{Data loading and preprocessing.}
\label{fig:preprocessing}
\end{figure}

\subsection{Feature Selection by Importance}
To see the effect of model complexity clearly, we do not add features randomly. Instead, we first determine
which features are most predictive. We calculate the correlation of every feature with the target variable
(\texttt{Salary}) and sort them by their absolute value. 

\begin{figure}[!ht]
\centering
\begin{minted}[
    frame=lines,
    framesep=2mm,
    baselinestretch=1.2,
    bgcolor=sepia,
    fontsize=\footnotesize,
    linenos
]{python}
# Calculate correlation of all columns with 'Salary'
correlations = df.corr()['Salary'].abs().sort_values(ascending=False)

# Drop 'Salary' itself from the list to get feature list
sorted_features = correlations.drop('Salary').index.tolist()
\end{minted}
\caption{Sorting features by correlation importance.}
\label{fig:sorting}
\end{figure}

\subsubsection{Splitting the Data}
We split our data into a training set and a test set using the function \texttt{train\_test\_split} from
\texttt{scikit-learn}. 
\begin{itemize}
\item \textbf{Training Set:} Used to calculate the model weights. We intentionally restrict this to only
      \textbf{50 samples} to make overfitting more likely. 
\item \textbf{Test Set:} Used strictly for evaluation.
\end{itemize}

\begin{figure}[!ht]
\centering
\begin{minted}[
    frame=lines,
    framesep=2mm,
    baselinestretch=1.2,
    bgcolor=sepia,
    fontsize=\footnotesize,
    linenos
]{python}
from sklearn.model_selection import train_test_split

X = df[sorted_features]
y = df['Salary']

# Split: 50 samples for training, rest for testing
X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=50, random_state=42)
\end{minted}
\caption{Creating a small training set to induce overfitting.}
\label{fig:splitting}
\end{figure}

\subsection{The Experiment}
We now perform an iterative experiment. We start by training a model with only the single most important
feature. Then, we train a model with the top 2 features, then top 3, and so on, until we use all available
features.  For each iteration, we use the \texttt{LinearRegression} class from \texttt{scikit-learn}. We use
the \texttt{.fit()} method to train the model and the \texttt{.score()} method to calculate the $R^2$
(coefficient of determination). 

\begin{figure}[!ht]
\centering
\begin{minted}[
    frame=lines,
    framesep=2mm,
    baselinestretch=1.2,
    bgcolor=sepia,
    fontsize=\footnotesize,
    linenos
]{python}
from sklearn.linear_model import LinearRegression

train_scores = []
test_scores = []
num_features = []

# Loop from k=1 to all features
for k in range(1, len(sorted_features) + 1):
    top_k_features = sorted_features[:k]
    
    # Subset the data
    X_train_k = X_train[top_k_features]
    X_test_k  = X_test[top_k_features]
    
    # Initialize and Train
    model = LinearRegression()
    model.fit(X_train_k, y_train)
    
    # Record Scores
    train_scores.append(model.score(X_train_k, y_train))
    test_scores.append(model.score(X_test_k, y_test))
    num_features.append(k)
\end{minted}
\caption{Iteratively training models with increasing complexity.}
\label{fig:experiment_loop}
\end{figure}

\subsection{Results and Interpretation}
When we plot the training and test scores against the number of features, we observe a classic overfitting pattern:
\begin{enumerate}
\item \textbf{Training Score (Blue Line):} This score generally increases as we add more features. With enough
      features, the model has enough degrees of freedom to fit the specific 50 data points in the training set
      almost perfectly. 
\item \textbf{Test Score (Red Line):} Initially, the score increases as we add relevant information (features
      strongly correlated with salary). However, after a certain point (the "Sweet Spot"), adding more features
      causes the test score to drop. 
\end{enumerate}
Figure \ref{fig:overfitting.png} on page \pageref{fig:overfitting.png} shows the results.
The drop in the test score indicates that the model has started to overfit. The "less important" features added
later in the loop (which have low correlation with salary) are essentially noise. The model uses these features
to "explain" the random quirks of the small training set, which harms its ability to predict salaries for
players in the test set. 

This experiment demonstrates that \textbf{more features are not always better}. A simpler model (with fewer,
higher-quality features) often generalizes better than a complex model. 

\begin{figure}[!th]
\epsfig{file=Figures/overfitting.png, scale=0.37}
\caption{The effect of overfitting.}
\label{fig:overfitting.png}
\end{figure}



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "artificial-intelligence"
%%% End:
