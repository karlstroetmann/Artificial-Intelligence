\chapter{Introduction}
Artificial Intelligence has evolved through two primary approaches. The first, known as \blue{symbolic AI}, focuses on \blue{symbolic logic}. \index{symbolic AI} This approach led to the creation of automatic theorem provers, \blue{symbolic integration} systems, and chess-playing programs like \href{https://en.wikipedia.org/wiki/Deep_Blue_(chess_computer)}{Deep Blue}. Initially, symbolic AI was the predominant paradigm in the field.

The second approach, \blue{machine learning}, was defined by Arthur Samuel as ``the field of study that enables
computers to learn without explicit programming'' \cite{samuel:1959}. \index{machine learning, definition} This
approach has primarily fueled the recent hype in AI. 

\section{Overview}
This lecture discusses only symbolic AI, as machine learning is part of the module \blue{data mining}.  
It emphasizes \textcolor{blue}{declarative programming}. The core principle of declarative programming
involves starting with a \textcolor{blue}{formal problem 
specification}, a succinct description of the issue at hand. This specification is then processed by a
\textcolor{blue}{problem solver} to produce a solution. Originally,
\href{https://en.wikipedia.org/wiki/Declarative_programming}{declarative programming} adopted a broad
approach to problem-solving, where problems were framed as logical formulas and tackled using
\href{https://en.wikipedia.org/wiki/Automated_theorem_proving}{automated theorem provers}. The
programming language \href{https://en.wikipedia.org/wiki/Prolog}{Prolog} is based on this
paradigm. However, this approach has proven to be less effective as a universal problem-solving framework
for two reasons: 
\begin{enumerate}
\item It is often challenging to fully articulate practical problems within a logical framework.
\item In cases where it is possible to completely define a problem using logical formulas, automatic
       theorem proving generally lacks the capability to autonomously find solutions. 
\end{enumerate}
Despite these limitations, declarative programming has proven valuable in several domains, which we will
explore, demonstrating its application in solving various types of problems: 
\begin{enumerate}
\item \textcolor{blue}{Search problems}, where the objective is to find a path within a graph. A classic
      instance is the \href{https://en.wikipedia.org/wiki/15_puzzle}{fifteen puzzle}. We will examine several
      advanced algorithms designed to resolve such search problems. 
\item \href{https://en.wikipedia.org/wiki/Constraint_satisfaction_problem}{Constraint satisfaction
      problems} hold significant practical relevance. Currently, highly efficient constraint solvers exist,
      capable of addressing various practical constraint satisfaction problems. We will delve into different
      strategies for solving these problems and discuss \href{https://github.com/Z3Prover/z3}{Z3}, a
      leading-edge automatic theorem prover and constraint solver developed by
      \href{https://www.microsoft.com/en-us/research/project/z3-3/}{Microsoft}. 
\item \textcolor{blue}{Games}, such as \href{https://en.wikipedia.org/wiki/Chess}{chess} or
      \href{https://en.wikipedia.org/wiki/checkers}{checkers}, can be defined using a declarative
      approach. We will cover several techniques enabling computers to devise optimal strategies for these
      adversarial games. 
\item We discuss \textcolor{blue}{automatic theorem proving}. Having previously covered
      \textcolor{blue}{resolution theorem proving} in our lecture on
      \href{https://github.com/karlstroetmann/Logic}{logic}, we will now turn our attention to
      \textcolor{blue}{equational theorem proving} in the final chapter of this first part. 
\item Next, we turn to the topic of \href{https://en.wikipedia.org/wiki/Machine_learning}{machine learning}.  
      We start by discussing \textcolor{blue}{linear regression}, since this is one of the most machine
      learning algorithms and is also the foundation for more advanced
      forms of machine learning like \href{https://en.wikipedia.org/wiki/Logistic_regression}{logistic regression} and 
      \href{https://en.wikipedia.org/wiki/Artificial_neural_network}{neural networks}.

      The goal of linear regression is to predict the values of unknown variables from the values of known
      variables.  For example, given the weight and the engine displacement of a car, we want to predict its
      fuel consumption.      
\item The following chapter discusses
      \href{https://en.wikipedia.org/wiki/Statistical_classification}{classification}.  A good example of
      classification is \href{https://en.wikipedia.org/wiki/Anti-spam_techniques#Detecting_spam}{spam detection}. 

      In particular, we will discuss \blue{logistic regression}, since this is needed later when discussing
      neural networks.
\item The we discuss discuss \href{https://en.wikipedia.org/wiki/Artificial_neural_network}{artificial neural
      networks}.
\item Finally, we discuss \blue{automatic differentiation}, which is a technique for computing the gradient of
      a function that does neither rely on numeric approximation nor does it force us to compute symbolic
      derivatives manually.   This technique is the basis for all modern libraries for neural networks,
      i.e.~\href{https://www.tensorflow.org}{TensorFlow} and \href{https://pytorch.org}{PyTorch} rely heavily
      on automatic differentiation.
\end{enumerate}

\section{Literature}
My main sources for these lecture notes were the following:
\begin{enumerate}
    \item A specialized course on artificial intelligence available through the \textsc{edX} platform. All
          relevant course materials can be accessed at
          \href{http://ai.berkeley.edu/home.html}{http://ai.berkeley.edu/home.html}. 
    \item The book titled
          \href{https://www.amazon.de/Artificial-Intelligence-Modern-Approach-Global/dp/1292401133/}{\textit{Introduction to Artificial Intelligence}},
          authored by Stuart Russell and Peter Norvig \cite{russell:2020}. 
    \item The \href{https://www.udacity.com/course/intro-to-artificial-intelligence--cs271}{\textit{Intro to
          Artificial Intelligence}} course provided by the \href{https://www.udacity.com}{Udacity} platform. 
\end{enumerate}
For exam preparation, a thorough understanding of the material covered in these lecture notes should
suffice. Therefore, purchasing additional books or enrolling in other courses is certainly not necessary.

\remark
The programs presented in these lecture notes are expected to run with the \textsl{Python} version 3.13.
Unfortunately, there is a bug in Python 3.14 that prevents us from using this most recent version of Python.
I have created the Python environment that I am using for these lecture notes via the shell commands shown in
Figure \ref{fig:ai.sh} on page \pageref{fig:ai.sh}.

\begin{figure}[!ht]
\centering
\begin{minted}[ frame         = lines, 
                 framesep      = 0.3cm, 
                 firstnumber   = 1,
                 bgcolor       = sepia,
                 numbers       = left,
                 numbersep     = -0.2cm,
                 xleftmargin   = 0.3cm,
                 xrightmargin  = 0.3cm,
                 ]{bash}
    conda create -n ai
    conda activate ai
    conda install -y python=3.13 jupyter nbclassic
    conda install -y graphviz
    conda install -y python-graphviz
    conda install -y numpy matplotlib seaborn
    conda install -y scikit-learn 
    conda install -y -c conda-forge ipycanvas 
    pip install ply
    pip install mypy
    pip install nb-mypy
    pip install z3-solver
    pip install git+https://github.com/reclinarka/problem_visuals
    pip install git+https://github.com/reclinarka/chess-problem-visuals
\end{minted}
\vspace*{-0.3cm}
\caption{Bash commands to set up an Anaconda environment for Python.}
\label{fig:ai.sh}
\end{figure}

When starting \textsl{Jupyter notebooks} you should take care to use the  command
\\[0.2cm]
\hspace*{1.3cm}
\texttt{jupyter nbclassic}
\\[0.2cm]
instead of the command \texttt{jupyter notebook}.  The command \texttt{jupyter nbclassic} uses the classic
version of \textsl{Jupyter notebooks}.  There are lots of incompatibilites with 
the new \textsl{Jupyter notebooks} of version $7.x$ and I have found that they do not work for me.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "artificial-intelligence"
%%% End:
