\chapter{Classification \label{chapter:classification}}
One of the earliest application of artificial intelligence is 
\href{https://en.wikipedia.org/wiki/Statistical_classification}{classification}.  A good
example of classification is
\href{https://en.wikipedia.org/wiki/Anti-spam_techniques#Detecting_spam}{spam detection}.
A system for spam detection classifies an email as either spam or not spam, where ``not spam'' is often
abbreviated as ``ham''.  To do so, it first
computes various \blue{features} \index{feature} of the email and then uses these features to determine whether
the email is likely to be spam.  For example, a possible feature would be the number of occurrences of the word
``\texttt{pharmacy}'' in the text of the email.

Another famous example of classification is \blue{character recognition}.  In character recognition, the
input is an image depicting a character.  This image is usually coded as a vector of gray values.  The task is
then to recognize the letter shown.  While spam detection is a \blue{binary} classification problem, character
recognition is a \blue{multi-class} classification problem, since there are 26 different characters.

\section{Introduction}
Formally, the \blue{classification problem} \index{classification problem} 
in machine learning can be stated as follows:  We are given a
set of objects $S := \{ o_1, \cdots, o_n \}$ and a set of classes $C := \{ c_1, \cdots, c_k \}$.  Furthermore, there exists a function 
\\[0.2cm]
\hspace*{1.3cm}
$\mathtt{classify}: S \rightarrow C$
\\[0.2cm]
that assigns a class $\texttt{classify}(o)$ to every object $o \in S$.  The set $S$ is called the \blue{sample space}.
In the example of spam detection, the sample space $S$ is the set of all emails that we might receive, i.e.~$S$
is the set of all strings, while the set of classes $C$ is given as
\\[0.2cm]
\hspace*{1.3cm}
$C = \{ \mathtt{spam}, \mathtt{ham} \}$.
\\[0.2cm]
Our goal is to compute the function \texttt{classify}.  In order to do this, we use an approach known as
\href{https://en.wikipedia.org/wiki/Supervised_learning}{supervised learning}:  We take a subset $S_\textsl{train} \subseteq S$ of
emails where we already know whether the emails are spam or not.  This set $S_\textsl{train}$ is called the
\blue{training set}. \index{training set}
Next, we define a set of $D$ \blue{features} for every $o \in S$.  These features have to be \blue{computable},
i.e.~we must have a function
\\[0.2cm]
\hspace*{1.3cm}
$\mathtt{feature}: S \times \{ 1, \cdots, D \} \rightarrow \mathbb{R}$ 
\\[0.2cm]
such that $\mathtt{feature}(o, j)$ computes the $j$-th feature and we have to be able to implement
this function with reasonable efficiency.  In general, the values of the features are real values.
However, there are cases where these values are just 
Boolean values.  If
\\[0.2cm]
\hspace*{1.3cm}
$\texttt{feature}(o, j) \in \mathbb{B}$ \quad for all $o \in S$,
\\[0.2cm]
then the $j$-th feature is called a \blue{binary feature}.  \index{binary feature}
If we encode $\mathtt{False}$ as $-1$ and $\mathtt{True}$ as
$+1$, then the set of Boolean values $\mathbb{B}$ can be considered a subset of $\mathbb{R}$ and hence Boolean features can be considered
as real numbers.  For example, in the case of spam detection, the first feature
could be the occurrence of the string ``\texttt{pharmacy}''.  In this case, we would have
\\[0.2cm]
\hspace*{1.3cm}
$\texttt{feature}(o, 1) := \left\{
\begin{array}{ll}
  +1 & \mbox{if \ $\texttt{pharmacy}     \in o$,}      \\
  -1 & \mbox{if \ $\texttt{pharmacy} \not\in o$,}  
\end{array}\right.
$
\\[0.2cm]
i.e.~the first feature would be to check whether the email $o$ contains the string ``\texttt{pharmacy}''.  
If we want to be more precise, we can instead define the first feature as
\\[0.2cm]
\hspace*{1.3cm}
$\texttt{feature}(o, 1) := \mathtt{count}(\texttt{"pharmacy"}, o)$,
\\[0.2cm]
i.e.~we would count the number of occurrences of the string ``\texttt{pharmacy}'' in our email $o$.   
As the value of
\\[0.2cm]
\hspace*{1.3cm}
 $\mathtt{count}(\texttt{"pharmacy"}, o)$ 
\\[0.2cm]
is always a natural number, in this case the first feature would be a
 \blue{discrete} feature.  However, we can be even more precise: Instead of just counting the number of occurrences of
 ``\texttt{pharmacy}'' we can compute its \blue{frequency}.  \index{frequency}
 After all, there is a difference whether the
 string ``\texttt{pharmacy}'' occurs once in an email  containing but a hundred characters or whether is occurs
 once in an email with a length of several thousand  characters.  To this end, we would then define the first feature as
\\[0.2cm]
\hspace*{1.3cm}
$\ds \texttt{feature}(o, 1) := \frac{\mathtt{count}(\texttt{"pharmacy"}, o)}{\mathtt{len}(o)}$, 
\\[0.2cm]
where $\mathtt{len}(o)$ defines the number of characters in the string $o$.  In this case, the first feature would be a
\blue{continuous} feature and as this is the most general case, unless stated otherwise, we deal with the continuous
case. 

Having defined the features, we next need a \blue{model} of the function \texttt{classify} that tries to approximate the
function \texttt{classify} via the features.  This model is given by a function
\\[0.2cm]
\hspace*{1.3cm}
$\mathtt{model}: \mathbb{R}^d \rightarrow C$
\\[0.2cm]
such that
\\[0.2cm]
\hspace*{1.3cm}
$\mathtt{model}\bigl(\mathtt{feature}(o,1), \cdots, \mathtt{feature}(o,D)\bigr) \approx \mathtt{classify}(o)$.
\\[0.2cm]
Using the function \texttt{model}, we can then approximate the function classify using a function \texttt{guess} that is
defined as
\\[0.2cm]
\hspace*{1.3cm}
$\mathtt{guess}(o) := \mathtt{model}\bigl(\mathtt{feature}(o,1), \cdots, \mathtt{feature}(o,D)\bigr)$
\\[0.2cm]
Most of the time, the function \texttt{guess} will only \blue{approximate} the function \texttt{classify}, i.e.~we will have
\\[0.2cm]
\hspace*{1.3cm}
$\mathtt{guess}(o) = \mathtt{classify}(o)$
\\[0.2cm]
for most objects of $o \in S$ but not for all of them.  The \blue{accuracy} \index{accuracy} of our model is
defined as the fraction of those objects that are classified correctly, i.e.~
\\[0.2cm]
\hspace*{1.3cm}
$\ds \mathtt{accuracy} := \frac{\;\mathtt{card}\bigl( \{ o \in S \mid \mathtt{guess}(o) = \mathtt{classify}(o)\}\bigr)\;}{\mathtt{card}(S)}$.
\\[0.2cm]
To keep matters simple, we will assume that the sample space is finite.

The function $\mathtt{model}$ is usually determined by a set of \blue{parameters} or \blue{weights} $\mathbf{w}$. In
this case, we have
\\[0.2cm]
\hspace*{1.3cm}
$\mathtt{model}(\mathbf{x}) = \mathtt{model}(\mathbf{x},\,\mathbf{w})$
\\[0.2cm]
where $\mathbf{x}$ is the feature vector, while $\mathbf{w}$ is the weight vector.  Later, when we
introduce \blue{logistic regression}, we will assume that the number of weights is one more than the number of
features.  Then, the weights specify the relative importance of the different features. Furthermore, there will
be a weight that is interpreted as a \blue{bias term}. \index{bias term}

When it comes to the choice of model, it is important to understand that, at least in practical applications,
\underline{all} models are wrong.  Nevertheless, \underline{some} models are useful.  There are two reasons for this:
\begin{enumerate}
\item We do not fully understand the function \texttt{classify} that we want to approximate by the function $\mathtt{model}$.
\item In the most general setting, the function \texttt{classify} is so complex, that even if we could compute
      it exactly, the resulting model would be much too complicated.
\end{enumerate}
The situation is similar in physics: Let us assume that we intend to model the fall of an object.  A model that is a
hundred percent accurate would have to include the following forces:
\begin{enumerate}[(a)]
\item gravitational acceleration,
\item air friction, 
\item tidal forces, i.e.~the effects that the rotation of the earth has on moving objects,
\item celestial forces, i.e.~the gravitational acceleration caused by celestial objects like the moon or the
      sun.
\item In the case of an iron object we have to be aware of the magnetic forces
      caused by the \href{https://www.britannica.com/science/geomagnetic-field}{geomagnetic field}.
\item To be fully accurate, we might have to include corrections from relativistic physics and even quantum
      physics.   
\item As physics is not a closed subject, there might be other forces at work which we still do not know of. 
\end{enumerate}
Hence, a correct model would be so complicated that it would be unmanageable and therefore useless. 

Let us summarize our introductory discussion of machine learning in general and classification in particular.
A set $S$ of objects and a set $C$ of classes are given.  Our goal is to approximate a function
\\[0.2cm]
\hspace*{1.3cm}
$\mathtt{classify}: S \rightarrow C$
\\[0.2cm]
using certain \blue{features} \index{feature} of our objects.  The function $\mathtt{classify}$ is then approximated using a function
\texttt{model} as follows:
\\[0.2cm]
\hspace*{1.3cm}
$\mathtt{model}\bigl(\mathtt{feature}(o,1), \cdots, \mathtt{feature}(o,D), \mathbf{w}\bigr) \approx \mathtt{classify}(o)$.
\\[0.2cm]
The model depends on a vector of parameters $\mathbf{w}$.  In order to \blue{learn} these parameters, we are given a 
\blue{training set} $S_\textsl{train}$ that is a subset of $S$.  As we are dealing with \blue{supervised learning},
\index{supervised learning} the function 
$\mathtt{classify}$ is known for all objects $o \in S_\textsl{train}$.   Our goal is to determine the parameters $\mathbf{w}$ such that the
number of mistakes we make on the training set is minimized.  

\subsection{Notation}
We conclude this introductory section by fixing some notation.  Let us assume that the objects $o \in S_\textsl{train}$
are numbered 
from $1$ to $n$, while the features are numbered from $1$ to $d$.  Then we define
\begin{enumerate}
\item $\textbf{x}_i := \langle\mathtt{feature}(o_i, 1), \cdots, \mathtt{feature}(o_i, d)\rangle^\top$ \quad for all $i \in \{1, \cdots, n\}$.

      i.e.~$\mathbf{x}_i$ is a $D$-dimensional column vector that collects the features of the $i$-th training object.
\item $x_{i,j} := \mathtt{feature}(o_i, j)$ \quad for all $i \in \{1, \cdots, n\}$ and $j \in \{1, \cdots, d\}$.

      i.e.~$x_{i,j}$ is the $j$-th feature of the $i$-th object.  The numbers $x_{i,j}$ are combined into the
      \blue{feature matrix} $X$, \index{feature matrix} i.e.~we have
      \\[0.2cm]
      \hspace*{1.3cm}
      $X=(x_{i,j})_{i\in\{1,\cdots,n\} \atop j \in \{1,\cdots,d\}.}$
      \\[0.2cm]
      The matrix $X$ is similar to the \blue{feature matrix} that was introduced in the previous chapter.
\item $y_i := \mathtt{classify}(o_i)$ \quad for all $i \in \{1, \cdots, n\}$

      i.e.~$y_i$ is the class of the $i$-th object.  These number are collected into the $n$-dimensional column vector $\mathbf{y}$.
\end{enumerate}

\subsection{Applications of Classification}
Besides spam detection, there are many other classification problems that can be solved using machine learning.  To give
just one more example, imagine a doctor that receives a patient and examines her symptoms.  In this case,
the symptoms can be seen as the features of the patient.  For example, these features could be
\begin{enumerate}[(a)]
\item body temperature,
\item blood pressure,
\item heart rate,
\item body weight,
\item breathing difficulties,
\item age,
\end{enumerate}
to name but a few of the possible features.  Based on these symptoms, the doctor would then decide on an
illness, i.e.~the set of classes for the classification problem would be
\\[0.2cm]
\hspace*{1.3cm}
$\{ \mathtt{commonCold}, \mathtt{pneumonia}, \mathtt{asthma}, \mathtt{flu}, \texttt{Covid-19}, \cdots, \mathtt{unknown} \}$.
\\[0.2cm]
Hence, the task of disease diagnosis is a classification problem.  This was one of the earliest problems that
was tackled by artificial intelligence.  As of today, 
\href{https://en.wikipedia.org/wiki/Computer-aided_diagnosis}{computer-aided diagnosis} and 
\href{https://en.wikipedia.org/wiki/Clinical_decision_support_system}{clinical decision support systems}
have been used for more than 40 years in many hospitals.  Today, there are a number of diseases that can be
diagnosed more accurately by a computer than by a specialist.  One such example is the
\href{http://www.ultromics.com}{diagnosis of heart disease}.  Other applications of classification are the following:
\begin{enumerate}[(a)]
\item image recognition,
\item speech recognition,
\item credit card fraud detection,
\item credit approval.
\end{enumerate}


\section{Digression: The Method of Gradient Ascent \label{section:gradient-ascent}}
\index{gradient ascent}
In machine learning, it is often the case that we have to find either the \blue{maximum} or the \blue{minimum}
of a function 
\\[0.2cm]
\hspace*{1.3cm}
$f: \mathbb{R}^n \rightarrow \mathbb{R}$.
\\[0.2cm]
For example, when we discuss \blue{logistic regression} in the next section, we will have to find the maximum
of the \blue{likelihood} function.  To proceed, let us introduce the \blue{$\arg\max$} function.
\index{arg max function} The idea is that
\\[0.2cm]
\hspace*{1.3cm}
$\mathbf{\widehat{x}} = \arg\max\limits_{\mathbf{x}\in \mathbb{R}^n} f$
\\[0.2cm]
is that value of $\mathbf{x} \in \mathbb{R}^n$ that maximizes $ f(\mathbf{x})$.  Formally, we have
\\[0.2cm]
\hspace*{1.3cm}
$\forall \mathbf{x} \in \mathbb{R}^n : f(\mathbf{x}) \leq f\Bigl(\arg\max\limits_{\mathbf{x}\in \mathbb{R}^n} f\Bigr)$.
\\[0.2cm]
Of course, the expression $\arg\max\limits_{\mathbf{x}\in \mathbb{R}^n} f$ is only defined when the maximum of
$f$ is unique.  If the function $f$ is differentiable, we know that a necessary condition for a vector
$\mathbf{\widehat{x}} \in \mathbb{R}^n$ to satisfy
\\[0.2cm]
\hspace*{1.3cm}
$\mathbf{\widehat{x}} = \arg\max\limits_{\mathbf{x}\in \mathbb{R}^n} f$ \quad is that we must have \quad $\nabla f(\mathbf{\widehat{x}}) = \mathbf{0}$,
\\[0.2cm]
i.e.~the \href{https://en.wikipedia.org/wiki/Gradient}{gradient} of $f$, which we will write as $\nabla f$,
vanishes at the maximum $\mathbf{\widehat{x}}$.

\begin{figure}[!th]
\centering
\hspace*{-1.3cm}
\epsfig{file=Figures/sin-minus-square.pdf, scale=0.5}
\vspace*{-0.3cm}
\caption{The function $x \mapsto \sin(x) - \frac{1}{2} \cdot x^2$.}
\label{fig:sin-minus-square.pdf}
\end{figure}

\noindent
Remember that the gradient of the function $f$ is defined as the column vector
\\[0.2cm]
\hspace*{1.3cm}
$\ds \nabla f :=
    \left\langle \frac{\partial f}{\partial\, x_1}, \cdots, \frac{\partial f}{\partial\, x_n} \right\rangle^\top
$.
\vspace*{0.2cm}

\noindent
Unfortunately, in many cases the equation 
\\[0.2cm]
\hspace*{1.3cm}
$\nabla f(\mathbf{\widehat{x}}) = \mathbf{0}$
\\[0.2cm]
cannot be solved in \href{https://en.wikipedia.org/wiki/Closed-form_expression}{closed terms}.  This is already
true in the one-dimensional case, i.e.~if $n=1$.  For example, consider 
the function $f:\mathbb{R} \rightarrow \mathbb{R}$ that is defined as
\\[0.2cm]
\hspace*{1.3cm}
$\ds f(x) := \sin(x) - \frac{1}{2} \cdot x^2$.
\\[0.2cm]
This function is shown in Figure \ref{fig:sin-minus-square.pdf} on page \pageref{fig:sin-minus-square.pdf}.
From the graph of the function it is obvious that this function has a maximum somewhere between $0.6$ and
$0.8$.  In order to compute this maximum, we can compute the derivative of $f$.   This derivative is given as 
\\[0.2cm]
\hspace*{1.3cm}
$f'(x) = \cos(x) - x$
\\[0.2cm]
As it happens, the equation $\cos(x) - x = 0$ does not seem to have a solution in 
\href{https://en.wikipedia.org/wiki/Closed-form_expression}{closed form}.  Hence, we can only approximate
the solution numerically via a sequence of numbers $(x_n\bigr)_{n\in\mathbb{N}}$ such that the limit
$\ds \lim\limits_{n\rightarrow\infty} x_n$
exists and is a solution of the equation $\cos(x) - x = 0$, i.e.~we want to have
\\[0.2cm]
\hspace*{1.3cm}
$\ds \cos\Bigl(\lim\limits_{n\rightarrow\infty} x_n\Bigr) = \lim\limits_{n\rightarrow\infty} x_n$.
\\[0.2cm]
The method of \href{https://en.wikipedia.org/wiki/Gradient_descent}{gradient ascent} is a numerical
method that can be used to find the maximum of a function 
\\[0.2cm]
\hspace*{1.3cm}
$f: \mathbb{R}^n \rightarrow \mathbb{R}$
\\[0.2cm]
numerically.  The basic idea is to take a vector $\mathbf{x}_0 \in \mathbb{R}^n$ as the start value and define a sequence of
vectors $\bigl(\mathbf{x}_n\bigr)_{n\in\mathbb{N}}$ such that we have
\\[0.2cm]
\hspace*{1.3cm}
$f(\mathbf{x}_{n+1}) \geq f(\mathbf{x}_{n})$ \quad for all $n\in\mathbb{N}$.
\\[0.2cm]
Hopefully, this sequence will converge against $\widehat{\mathbf{x}} = \arg\max\limits_{\mathbf{x}\in \mathbb{R}^n}f$.
If we do not really know where to start our search, we define $\mathbf{x}_0 := \mathbf{0}$.  In order to
compute $\mathbf{x}_{n+1}$ given $\mathbf{x}_{n}$, the idea is to move from $\mathbf{x}_n$ in that direction
where we have the biggest change in the values of $f$.   This direction happens to be the gradient of $f$ at $\mathbf{x}_n$.
Therefore, the definition of $\mathbf{x}_{n+1}$ is given as follows:
\\[0.2cm]
\hspace*{1.3cm}
$\mathbf{x}_{n+1} := \mathbf{x}_n + \alpha \cdot \nabla f(\mathbf{x}_n)$ \quad for all $n \in \mathbb{N}_0$.
\\[0.2cm]
Here, $\alpha$ is called the \blue{step size} and is also known as the \blue{learning rate}.  It determines by how
much we move in the direction of the gradient.  In practise, it is best to adapt the step size dynamically
during the iterations.  The Python function shown in Figure \ref{fig:gradient-ascent.py} on page
\pageref{fig:gradient-ascent.py} demonstrates how this is done. 
The function \texttt{findMaximum} takes four arguments:
\begin{enumerate}
\item $\texttt{f}$ is the function that is to be maximized.  It is assumed that \texttt{f} takes a vector
      $\texttt{x}\in \mathbb{R}^n$ as its input and that it returns a real number.  Note that $n$ might be
      $1$.  In that case the input to $f$ is a real number.
\item $\texttt{gradF}$ is the gradient of \texttt{f}.  It takes a vector
      $\texttt{x}\in \mathbb{R}^n$ as its input and returns the vector $\nabla \mathtt{f}(\mathtt{x})$.
\item $\texttt{start}$ is a vector from $\mathbb{R}^n$ that is used as the value of $\mathbf{x}_0$.  
\item $\texttt{eps}$ is the precision that we want to obtain when locating the maximum.  We will have to say more on how \texttt{eps}
      is related to the precision later.  As we are using double precision floating point arithmetic, 
      it won't make sense to use a value for \texttt{eps} that is smaller than $10^{-15}$.
\end{enumerate}
Next, let us discuss the implementation of gradient ascent.
\begin{enumerate}
\item \texttt{x} is initialized with the parameter \texttt{start}.  Hence, \texttt{start} is really the same as
      $\mathbf{x}_0$. 
\item \texttt{fx} is the value $\texttt{f}(\texttt{x})$.
\item \texttt{alpha} is the \blue{learning rate}.  We initialize \texttt{alpha} as $1.0$.  The learning rate
      will be adapted dynamically. 
\item The body of the \texttt{while} loop starting in line 6 executes one iteration of gradient ascent.
\item In each iteration, we store the values of $\mathbf{x}_n$ and $f(\mathbf{x}_n)$ in the variables
      \texttt{xOld} and \texttt{fOld}.  This is needed since we need to ensure that the values of
      $f(\mathbf{x}_n)$ are increasing.  If this value of $f(\mathbf{x}_{n+1})$ is not bigger that
      $f(\mathbf{x}_n)$ we revert to the old values.
\item Next, we compute $\mathbf{x}_{n+1}$ in line 8 using the formula
      \\[0.2cm]
      \hspace*{1.3cm}
      $\mathbf{x}_{n+1} := \mathbf{x}_n + \alpha \cdot \nabla f(\mathbf{x}_n)$.
\item The corresponding value $f(\mathbf{x}_{n+1})$ is computed in line 9.
\item If we are unlucky, $f(\mathbf{x}_{n+1})$ is smaller than $f(\mathbf{x}_{n})$ instead of bigger.  This might happen if the learning
      rate $\alpha$ is too large.  Hence, in this case we decrease the value of $\alpha$, discard 
      both $\mathbf{x}_{n+1}$ and $f(\mathbf{x}_{n+1})$ and start over again via the \texttt{continue}
      statement in line 13.
\item Otherwise, if  $f(\mathbf{x}_{n+1})$ is indeed bigger than $f(\mathbf{x}_{n})$, the vector
  $\mathbf{x}_{n+1}$ is a better approximation of the maximum than the vector $\mathbf{x}_n$.  
      In this case, in order to increase the speed of the convergence of our algorithm we will then increase the learning rate
      $\alpha$ by $20\%$.    
\item The idea of our implementation is to stop the iteration when the relative difference  of 
      $f(\mathbf{x}_{n+1})$ and $f(\mathbf{x}_{n})$ is less than $\varepsilon$ or, to be more precise, if
      \\[0.2cm]
      \hspace*{1.3cm}
      $f(\mathbf{x}_{n+1}) < f(\mathbf{x}_{n}) \cdot (1 + \varepsilon)$.
      \\[0.2cm]
      As the sequence $\bigl(f(\mathbf{x}_n\bigr)_{n\in\mathbb{N}}$ will be monotonically
      increasing, i.e.~we have
      \\[0.2cm]
      \hspace*{1.3cm}
      $f(\mathbf{x}_{n+1}) \geq f(\mathbf{x}_{n})$ \quad for all $n\in\mathbb{N}$,
      \\[0.2cm]
      the condition given above is sufficient.  Now, if the increment of  $f(\mathbf{x}_{n+1})$ is less than $f(\mathbf{x}_{n}) \cdot (1 + \varepsilon)$ 
      we assume that we have reached the maximum with the required precision.  In this case we return both the
      value of \texttt{x} and the corresponding function value $f(\mathtt{x})$.
\end{enumerate}

\begin{figure}[!ht]
\centering
\begin{minted}[ frame         = lines, 
                framesep      = 0.3cm, 
                firstnumber   = 1,
                bgcolor       = sepia,
                mathescape,
                numbers       = left,
                numbersep     = -0.2cm,
                xleftmargin   = 0.8cm,
                xrightmargin  = 0.8cm,
              ]{python3}
    def findMaximum(f, gradF, start, eps):
        x     = start
        fx    = f(x)
        alpha = 1.0
        while True:
            xOld, fOld = x, fx
            x  += alpha * gradF(x)
            fx  = f(x)
            if fx <= fOld:   
                alpha *= 0.5
                x, fx = xOld, fOld
                continue
            else:
                alpha *= 1.2
            if abs(fx - fOld) <= abs(fx) * eps:
                return x, fx
\end{minted}

\caption{The gradient ascent algorithm.}
\label{fig:gradient-ascent.py}
\end{figure}
\FloatBarrier

The implementation of gradient ascent given above is not the most sophisticated variant of this algorithm.
Furthermore, there are algorithms that are more powerful than gradient ascent.  The first of these methods is the
\href{https://en.wikipedia.org/wiki/Conjugate_gradient_method}{conjugate gradient method}.  A
refinement of this method is the
\href{https://en.wikipedia.org/wiki/Broyden-Fletcher-Goldfarb-Shanno_algorithm}{BFGS-algorithm} that
has been invented by Broyden, Fletcher, Goldfarb, and Shanno.  Unfortunately, we do not have the
time to discuss these algorithms.
However, our implementation of gradient ascent is sufficient for our applications and as this is not a course on numerical
analysis but rather on artificial intelligence we will not delve deeper into this topic but, instead, we refer
readers interested in more efficient algorithms to the literature \cite{snyman:2005}.  If you ever need to find
the maximum of a function numerically, you should try to use a predefined library routine that implements a
state of the art algorithm.  For example, in \href{https://www.python.org}{Python} the method
\href{https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize}{minimize}
from the package \texttt{scipy.optimize} offers various algorithms for minimization.

\section{Logistic Regression}
\index{logistic regression}
If we have a model such that 
\\[0.2cm]
\hspace*{1.3cm}
$\mathtt{model}(\mathbf{x}, \mathbf{w}) \approx \mathtt{classify}(\mathbf{x})$
\\[0.2cm]
holds, then we want to choose the weight vector $\mathbf{w}$ in a way such that the accuracy 
\\[0.2cm]
\hspace*{1.3cm}
$\ds \mathtt{accuracy}(\mathbf{w}) := 
\frac{\;\mathtt{card}\bigl(\bigl\{ \mathbf{o} \in S \bigm| \mathtt{model}(\mathtt{feature}\bigl(\mathbf{o}),\mathbf{w}\bigr) = \mathtt{classify}(\mathbf{o})\bigr\}\bigr)\;}{\mathtt{card}(S)}$
\\[0.2cm]
is maximized.  However, there is a snag:  The accuracy is not a smooth function of the weight vector
$\mathbf{w}$.  It can't be a smooth function because the number of errors of our model is a natural number and not a real number
that could change smoothly when the weight vector $\mathbf{w}$ is changed.  Hence, the accuracy is not differentiable as a function
of the weight vector.  The way to proceed is to work with \blue{probabilities} instead.  Instead of assigning a
class to an object $\mathbf{o}$ we rather assign a \blue{probability} $p$ to the object $\mathbf{o}$ that measures how
probable it is that object $\mathbf{o}$ has a given class $c$.  Then we try to maximize this probability.  In
\href{https://en.wikipedia.org/wiki/Logistic_regression}{logistic regression} we use a linear model that is 
combined with the \blue{sigmoid function}.  Before we can discuss the details of logistic regression we need to
define this function and state some of its properties.  

\subsection{The Sigmoid Function}

\begin{figure}[!ht]
\centering
\epsfig{file=Figures/sigmoid.eps, scale=0.7}
\vspace*{-0.3cm}
\caption{The sigmoid function.}
\label{fig:sigmoid.eps}
\end{figure}
\FloatBarrier

\begin{Definition}[Sigmoid Function\label{def:sigmoid}]
  The \href{https://en.wikipedia.org/wiki/Sigmoid_function}{sigmoid function} $S: \mathbb{R} \rightarrow [0, 1]$
  \index{sigmoid function} is defined as 
\\[0.2cm]
\hspace*{1.3cm}
$\ds S(t) = \frac{1}{1 + \exp(-t)}$.  
\\[0.2cm]
Figure \ref{fig:sigmoid.eps} on page \pageref{fig:sigmoid.eps} shows the sigmoid function.
The sigmoid function is also known as the
\href{https://en.wikipedia.org/wiki/Logistic_function}{logistic function}. \index{logistic function} 
\eox
\end{Definition}


\noindent
Let us note some immediate consequences of the definition of the sigmoid function.  As we have
\\[0.2cm]
\hspace*{1.3cm}
$\ds\lim\limits_{x\rightarrow-\infty} \exp(-x) = \infty$, \quad 
$\ds\lim\limits_{x\rightarrow+\infty} \exp(-x) = 0$, \quad and \quad
$\ds\lim\limits_{x\rightarrow\infty} \frac{1}{x} = 0$, 
\\[0.2cm]
the sigmoid function has the following properties:
\\[0.2cm]
\hspace*{1.3cm}
$\ds \lim_{t\rightarrow-\infty} S(t) = 0$ \quad and \quad
$\ds \lim_{t\rightarrow+\infty} S(t) = 1$.
\\[0.2cm]
As the sigmoid function is monotonically increasing, this shows that indeed 
\\[0.2cm]
\hspace*{1.3cm}
$0 \leq S(t) \leq 1$  \quad for all $t \in \mathbb{R}$.
\\[0.2cm]
Therefore, the value of the sigmoid function can be interpreted as a \blue{probability}.
Another important property of the sigmoid function is its symmetry.  Figure \ref{fig:sigmoid.eps} shows that if the
sigmoid function is shifted down by $\frac{1}{2}$, the resulting function is 
\href{https://en.wikipedia.org/wiki/Point_reflection}{centrally symmetric}, \index{centrally symmetric}
i.e.~we have
\\[0.2cm]
\hspace*{1.3cm}
$\ds S(-t) - \frac{1}{2} = -\Bigl(S(t) - \frac{1}{2}\Bigr)$.
\\[0.2cm]
Adding $\ds\frac{1}{2}$ on both sides of this equation shows that this is equivalent to the equation
\\[0.2cm]
\hspace*{1.3cm}
\colorbox{red}{\framebox{\colorbox{orange}{
$S(-t) = 1 - S(t)$,}}}
\\[0.2cm]
The proof of this fact runs as follows:
\\[0.2cm]
\hspace*{1.3cm}
$
\begin{array}{lcll}
1 - S(t) & = & \ds 1 - \frac{1}{1 + \exp(-t)}             & \mbox{(by definition of $S(t)$)}           \\[0.5cm]
         & = & \ds \frac{1 + \exp(-t) - 1}{1 + \exp(-t)}  & \mbox{(common denominator)}                \\[0.5cm]
         & = & \ds \frac{\exp(-t)}{1 + \exp(-t)}          &                                          \\[0.5cm]
         & = & \ds \frac{1}{\exp(t) + 1}                  & \mbox{(expand fraction by $\exp(t)$)}      \\[0.5cm]
         & = & \ds \frac{1}{1 + \exp(+t)}                 &                                          \\[0.5cm]
         & = & S(-t)                                      & \mbox{(by definition of $S(-t)$)}. \qquad _\Box
\end{array}
$
\\[0.2cm]
The exponential function can be expressed via the sigmoid function.  Let us start with the definition of the sigmoid
function. 
\\[0.2cm]
\hspace*{1.3cm}
$\ds S(t) = \frac{1}{1 + \exp(-t)}$
\\[0.2cm]
Multiplying this equation with the denominator yields
\\[0.2cm]
\hspace*{1.3cm}
$\ds S(t) \cdot \bigl(1 + \exp(-t)\bigr) = 1$.
\\[0.2cm]
Dividing both sides by $S(t)$ gives:
\\[0.2cm]
\hspace*{1.3cm}
$
\begin{array}{cl}
                & \ds 1 + \exp(-t) = \frac{1}{S(t)}        \\[0.5cm]
\Leftrightarrow & \ds \exp(-t) = \frac{1}{S(t)} - 1        \\[0.5cm]
\Leftrightarrow & \ds \exp(-t) = \frac{1 - S(t)}{S(t)}      
\end{array}
$
\\[0.2cm]
We highlight this formula, as we need it later
\begin{equation}
\label{eq:1}
 \colorbox{red}{\framebox{\colorbox{orange}{\mbox{$\ds\exp(-t) = \frac{1 - S(t)}{S(t)}$.}}}}
\end{equation}
If we take the reciprocal of both sides of this equation, we have
\\[0.2cm]
\hspace*{1.3cm}
$\ds \exp(t) = \frac{S(t)}{1 - S(t)}$.
\\[0.2cm]
Applying the natural logarithm on both sides of this equation yields
\\[0.2cm]
\hspace*{1.3cm}
$\ds t = \ln\left(\frac{S(t)}{1-S(t)}\right)$.
\\[0.2cm]
This shows that the inverse of the sigmoid function is given as
\\[0.2cm]
\hspace*{1.3cm}
\colorbox{red}{\framebox{\colorbox{orange}{
$\ds S^{-1} (y) = \ln\left(\frac{y}{1-y}\right)$.}}} 
\\[0.2cm]
This function is known as the \href{https://en.wikipedia.org/wiki/Logit}{logit function}.
\index{logit function}
Next, let us compute the derivative of $S(t)$, i.e.~$\ds S'(t) =\frac{\mathrm{d}S}{\mathtt{d}t}$.  We have
\\[0.2cm]
\hspace*{1.3cm}
$
\begin{array}{lcll}
 S'(t) & = & \ds -\frac{-\exp(-t)}{\bigr(1+\exp(-t)\bigr)^2}   \\[0.5cm]
       & = & \ds \exp(-t) \cdot S(t)^2                         \\[0.2cm]
       & = & \ds \frac{1-S(t)}{S(t)} \cdot S(t)^2  & \mbox{(by Equation \ref{eq:1})} \\[0.4cm]
       & = & \ds \bigl(1 - S(t)\bigr) \cdot S(t)            
\end{array}
$
\\[0.2cm]
We have shown
\begin{equation}
  \label{eq:2}
  \colorbox{red}{\framebox{\colorbox{orange}{\mbox{$\ds S'(t) = \bigl(1 - S(t)\bigr) \cdot S(t)$.}}}}
\end{equation}
We will later need the derivative of the natural logarithm of the logistic function.  We define
\\[0.2cm]
\hspace*{1.3cm}
$L(t) := \ln\bigl(S(t)\bigr)$.
\\[0.2cm]
Then we have
\\[0.2cm]
\hspace*{1.3cm}
$
\begin{array}{lcll}
  L'(t) & = & \ds \frac{S'(t)}{S(t)}                  & \mbox{(by the chain rule)} \\[0.5cm]
        & = & \ds \frac{(1 - S(t)) \cdot S(t)}{S(t)}                               \\[0.5cm]
        & = & \ds 1 - S(t)                                                         \\[0.2cm]
        & = & \ds S(-t)                               & \mbox{(by symmetry)} 
\end{array}
$
\\[0.2cm]
% If we have the function $f(t) := L(-t)$, then we see that
%\\[0.2cm]
%\hspace*{1.3cm}
% $f'(t) = -S(t)$
%\\[0.2cm]
% holds.   
As this is our most important result, we highlight it:
\\[0.2cm]
\hspace*{1.3cm}
\colorbox{red}{\framebox{\colorbox{orange}{$L'(t) = S(-t)$ \quad where \quad $L(t) := \ln\bigl(S(t)\bigr)$.}}}


\subsection{The Model of Logistic Regression}
In logistic regression we deal with \blue{binary classification}, \index{binary classification}
i.e.~we assume that we just need to decide
whether a given object is a member of a given class or not.  We use the following model to compute the \blue{probability} that an
object $o$ with features $\mathbf{x}$ will be of the given class: 
\\[0.2cm]
\hspace*{1.3cm}
$P(y=+1\;|\;\mathbf{x},\mathbf{w}) = S(\mathbf{x} \cdot \mathbf{w})$.
\\[0.2cm]
Note that $P(y=+1\;|\;\mathbf{x},\mathbf{w})$ is the \blue{conditional probability} that $o$ has the given
class, given its features $\mathbf{x}$ and the weights $\mathbf{w}$.  The expression $\mathbf{x}
\cdot \mathbf{w}$ denotes the \blue{dot product} of the vectors $\mathbf{x}$ and $\mathbf{w}$,  
i.e. we have
\\[0.2cm]
\hspace*{1.3cm}
$\mathbf{x} \cdot \mathbf{w} = \sum\limits_{i=1}^d x_i \cdot w_i$,
\\[0.2cm]
where $D$ is the number of features.
To simplify the notation, it is assumed that $\mathbf{x}$ contains a \blue{constant feature} \index{constant feature}
that always takes the value of $1$.
If you see this model for the first time, you might think that it is not very general and that it can only be
used in very special circumstances.  However,  the features $x_i$ can be functions of arbitrary complexity
and hence this model is much more general than it appears initially.

We assume that $y$ can only take the values $+1$ or $-1$,  e.g.~in the example of spam detection $y = 1$ if the
email is spam and $y = -1$ otherwise.  Since complementary probabilities add up to $1$, we have
\\[0.2cm]
\hspace*{1.3cm}
$P(y=-1\;|\;\mathbf{x},\mathbf{w}) = 1 - P(y=+1\;|\;\mathbf{x},\mathbf{w}) 
  = 1 - S(\mathbf{x} \cdot \mathbf{w}) = S(-\mathbf{x} \cdot \mathbf{w})
$.
\\[0.2cm]
Hence, we can combine the equations for $P(y=-1\;|\;\mathbf{x},\mathbf{w})$ and $P(y=+1\;|\;\mathbf{x},\mathbf{w})$ into a
single equation
\\[0.2cm]
\hspace*{1.3cm}
\colorbox{red}{\framebox{\colorbox{orange}{$P(y\;|\;\mathbf{x},\mathbf{w}) = S\bigl(y \cdot(\mathbf{x} \cdot \mathbf{w})\bigr)$.}}}
\\[0.2cm]
Given $N$ objects $o_1, \cdots, o_n $ with feature vectors $\mathbf{x}_1, \cdots, \mathbf{x}_n$ and classes
$y_1,\cdots,y_n$, we
want to determine the weight vector $\mathbf{w}$ such that the \blue{likelihood} $\ell(\mathbf{X}, \mathbf{y})$
\index{likelihood} of all of our
observations is maximized, where $\mathbf{X}$ is the \blue{feature matrix} that is defined as
\\[0.2cm]
\hspace*{1.3cm}
$\mathbf{X} := \left(
  \begin{array}{c}
    \bigl(\mathbf{x}^{(1)}\bigr)^\top  \\
    \vdots                         \\
    \bigl(\mathbf{x}^{(n)}\bigr)^\top
  \end{array}
  \right).   
$
\\[0.2cm]
This approach is called the 
\href{https://en.wikipedia.org/wiki/Maximum_likelihood_estimation}{maximum likelihood estimation}
\index{maximum likelihood estimation} of the weights.
As we assume that the probabilities of different observations are independent, the individual
probabilities have to be multiplied to compute the overall likelihood $\ell(\mathbf{X}, \mathbf{y},\mathbf{w})$ 
of a given training set:
\\[0.2cm]
\hspace*{1.3cm}
$\ds \ell(\mathbf{X},\mathbf{y},\mathbf{w}) = \prod\limits_{i=1}^n P(y_i \;|\;\mathbf{x}_i,\mathbf{w})$.
\\[0.2cm]
Since it is easier to work with sums than with products, instead of maximizing the function
$\ell(\mathbf{X},\mathbf{y},\mathbf{w})$ we instead maximize the logarithm of
$\ell(\mathbf{X},\mathbf{y},\mathbf{w})$.  This logarithm is called the \blue{log-likelihood} \index{log-likelihood} and is defined as 
\\[0.2cm]
\hspace*{1.3cm}
$\ell\ell(\mathbf{X},\mathbf{y},\mathbf{w}) := \ln\bigl(\ell(\mathbf{X},\mathbf{y},\mathbf{w})\bigr)$. 
\\[0.2cm]
As the natural logarithm is a \href{https://en.wikipedia.org/wiki/Monotonic_function}{monotonically increasing}
function, the functions $\ell(\mathbf{X},\mathbf{y},\mathbf{w})$ and  $\ell\ell(\mathbf{X},\mathbf{y},\mathbf{w})$ take their maximum at the same value of $\mathbf{w}$.  As we have
\\[0.2cm]
\hspace*{1.3cm}
$\ln(a \cdot b) = \ln(a) + \ln(b)$,
\\[0.2cm]
the natural logarithm of the likelihood is 
\\[0.2cm]
\hspace*{1.3cm}
\colorbox{red}{\framebox{\colorbox{orange}{
$\ds \ell\ell(\mathbf{X},\mathbf{y},\mathbf{w}) = 
 \sum\limits_{i=1}^n \ln\Bigl(S\bigl(y_i \cdot(\mathbf{x}_i \cdot \mathbf{w})\bigr)\Bigr) =
 \sum\limits_{i=1}^n L\bigl(y_i \cdot(\mathbf{x}_i \cdot \mathbf{w})\bigr)
$.}}}
\\[0.2cm]
Our goal is to choose the weights $\mathbf{w}$ such that the likelihood is maximized.  Since this is the same as maximizing the log-likelihood, we
need to determine the gradient of the log-likelihood with respect to the weights $w_j$, i.e.~we need to compute
the partial derivatives
\\[0.2cm]
\hspace*{1.3cm}
$\ds \frac{\partial\quad}{\partial\, w_j}\ell\ell(\mathbf{X},\mathbf{y},\mathbf{w})$ \quad for all $j\in\{1,\cdots,d\}$.
\\[0.2cm]
In order to compute the partial derivative of $\ell\ell(\mathbf{X},\mathbf{y},\mathbf{w})$ with respect to the
coefficients $\mathbf{w}$ we need to compute the partial derivative of the dot product $\mathbf{x}_i \cdot
\mathbf{w}$ with respect to the weights $w_j$.
We define
\\[0.2cm]
\hspace*{1.3cm}
$\ds h(\mathbf{w}) := \mathbf{x}_i \cdot \mathbf{w} = \sum\limits_{k=1}^d x_{i,k} \cdot w_k$.
\\[0.2cm]
Then we have
\\[0.2cm]
\hspace*{1.3cm}
$\ds \frac{\partial\quad}{\partial\, w_j} h(\mathbf{w})
   = \frac{\partial\quad}{\partial\, w_j} \sum\limits_{k=1}^d x_{i,k} \cdot w_k
   = \sum\limits_{k=1}^d x_{i,k} \cdot \frac{\partial\quad}{\partial\, w_j} w_k
   = \sum\limits_{k=1}^d x_{i,k} \cdot \delta_{j,k} =  x_{i,j}
$.
\\[0.2cm]
Now we are ready to compute the partial derivative of $\ell\ell(\mathbf{X},\mathbf{y},\mathbf{w})$ with respect to $\mathbf{w}$:
\\[0.2cm]
\hspace*{1.3cm}
$
\begin{array}{cll}
  & \ds \frac{\partial\quad}{\partial\, w_j} \ell\ell(\mathbf{X},\mathbf{y},\mathbf{w}) \\[0.5cm]
= & \ds \frac{\partial\quad}{\partial\, w_j} 
    \sum\limits_{i=1}^n L\bigl(y_i \cdot(\mathbf{x}_i \cdot \mathbf{w})\bigr) 
    \\[0.5cm]
= & \ds\sum\limits_{i=1}^n y_i \cdot x_{i,j} \cdot  S\bigl(-y_i \cdot(\mathbf{x}_i \cdot \mathbf{w})\bigr),
  & \mbox{since} \quad \ds \frac{\mathrm{d}L(x)}{\mathrm{d}x} = S(-x).
\end{array}
$
\\[0.2cm]
Hence, the partial derivative of the log-likelihood function is given as follows:
\\[0.2cm]
\hspace*{1.3cm}
\colorbox{red}{\framebox{\colorbox{orange}{
$\ds \frac{\partial\quad}{\partial\, w_j}\ell\ell(\mathbf{X},\mathbf{y},\mathbf{w}) =
 \ds\sum\limits_{i=1}^n y_i \cdot x_{i,j} \cdot  S(-y_i \cdot \mathbf{x}_i \cdot \mathbf{w})
$}}} 
\\[0.2cm]
Next, we have to find the value of $\mathbf{w}$ such that
\\[0.2cm]
\hspace*{1.3cm}
$\ds\sum\limits_{i=1}^n y_i \cdot x_{i,j} \cdot  S(-y_i \cdot \mathbf{x}_i \cdot \mathbf{w}) = 0$
\quad for all $j \in \{1, \cdots, d\}$.
\\[0.2cm]
These are $d$ equations for the $d$ variables $w_1, \cdots, w_d$.  Due to the occurrence of the sigmoid function, these
equations are nonlinear.  Unfortunately, these equations do not have a solution in closed terms.  Nevertheless,
our computation of the 
gradient of the log-likelihood was not in vain:  We will use the method of \blue{gradient ascent} to find
the value of $\mathbf{w}$ that maximizes the log-likelihood.  This method has been outlined in the previous section.

\subsection{Implementing Logistic Regression}
In this section we will give a simple implementation of logistic regression.  We will use our implementation of
logistic regression to predict whether a student will pass or fail a given exam.  Figure \ref{fig:exam.csv}
shows a \href{https://en.wikipedia.org/wiki/Comma-separated_values}{\textsc{Csv}} file I have borrowed from the
Wikipedia page on \href{https://en.wikipedia.org/wiki/Logistic_regression}{logistic regression} that contains
the data we are going to explore.  Concretely, this file stores the hours a student has learned for a particular exam
and the fact whether the student has passed the exam or has failed.  A passed exam is encoded as the number
$1$, while a failed exam is encoded as $0$.  The first column of the file stores these numbers.  The second
column stores the number of hours that the student has learned in order to pass the exam.


\begin{figure}[!ht]
\centering
\begin{Verbatim}[ frame         = lines, 
                  framesep      = 0.3cm, 
                  firstnumber   = 1,
                  labelposition = bottomline,
                  numbers       = left,
                  numbersep     = -0.2cm,
                  xleftmargin   = 0.8cm,
                  xrightmargin  = 0.8cm,
                ]
   Pass, Hours
   0,    0.50
   0,    0.75
   0,    1.00
   0,    1.25
   0,    1.50
   0,    1.75
   1,    1.75
   0,    2.00
   1,    2.25
   0,    2.50
   1,    2.75
   0,    3.00
   1,    3.25
   0,    3.50
   1,    4.00
   1,    4.25
   1,    4.50
   1,    4.75
   1,    5.00
   1,    5.50
\end{Verbatim}
\vspace*{-0.3cm}
\caption{Results of an exam.}
\label{fig:exam.csv}
\end{figure}

The program shown in Figure \ref{fig:logistic_regression.py} on page
\pageref{fig:logistic_regression.py} implements logistic regression.  As there are a number of
subtle points that might easily be overlooked otherwise, we proceed to discuss this program line by line. 


\begin{figure}[!ht]
\centering

\begin{minted}[ frame         = lines, 
                framesep      = 0.3cm, 
                firstnumber   = 1,
                bgcolor       = sepia,
                numbers       = left,
                numbersep     = -0.2cm,
                xleftmargin   = 0.5cm,
                xrightmargin  = 0.5cm,
                escapeinside  = ||
              ]{python3}
    import numpy as np
  
    # compute |$\displaystyle \frac{1}{1+\exp(-t)}$|
    def sigmoid(t):
        return 1.0 / (1.0 + np.exp(-t))
  
    # compute |$\displaystyle \ln\left(\frac{1}{1+\exp(-t)}\right)$| and avoid overflow
    def logSigmoid(t):
        if t > -100:
            return -np.log(1.0 + np.exp(-t))
        else:
            return t
  
    def ll(X, y, w):
        """
        given the matrix X and the observations y,
        return the log likelihood for the weight vector w
        """
        return np.sum([logSigmoid(y[i] * (X[i] @ w)) for i in range(len(X))])
  
    def gradLL(X, y, w):
        """
        Compute the gradient of the log-likelihood with respect to w 
        """
        Gradient = []
        for j in range(len(X[1])):
            L = [y[i]*X[i][j]*sigmoid(-y[i] * (X[i] @ w)) for i in range(len(X))]
            Gradient.append(sum(L))
        return np.array(Gradient)
\end{minted}

\caption{An implementation of logistic regression.}
\label{fig:logistic_regression.py}
\end{figure}
\FloatBarrier


\begin{enumerate}
\item First, we \texttt{import} the module \texttt{numpy}.  This module provides us with the 
      functions \texttt{log} and \texttt{exp} for computing the logarithm and the exponential of a number or
      a vector.  Furthermore, we need this module because the gradient of the log-likelihood is a vector and for
      efficiency reasons this vector should be stored as a NumPy array.
\item Line 3 implements the sigmoid function

      \hspace*{1.3cm}
      $\ds S(x) = \frac{1}{1 + \exp(-x)}$.
      \\[0.2cm]
      Since we are using NumPy to compute the exponential function, the parameter $t$ that is used in our
      implementation can also be a vector.
\item Line 7 starts the implementation of the natural logarithm of the sigmoid function, i.e.~we implement
      \\[0.2cm]
      \hspace*{1.3cm}
      $\ds L(x) = \ln\bigl(S(x)\bigr) = \ln\left(\frac{1}{1 + \exp(-x)}\right) =- \ln\bigl(1 + \exp(-x)\bigr)$.
      \\[0.2cm]
      The implementation is more complicated than you might expect.  The reason has to do with
      \blue{numerical overflow}.  Consider values of $x$ that are smaller than, say, $-1000$.  The problem is that
      the expression $\mathtt{exp}(1000)$ evaluates to the value \texttt{inf}, which represents the
      mathematical concept of infinity, denoted as $\infty$.  But then $1 + \mathtt{exp}(1000))$ is also \texttt{inf} and
      finally \texttt{log(1 + exp(1000))} is still \texttt{inf}.  However, in reality we have
      \\[0.2cm]
      \hspace*{1.3cm}
      $\ln\bigl(1 + \exp(1000)\bigr) \approx 1000$
      \\[0.2cm] 
      because $\exp(1000)$ is so big that adding $1$ to it does not make much of a difference.
      In fact, if we use \textsl{Pythons} 64-bit double arithmetic, then even adding $1$ to $\exp(100)$
      does not make any difference at all as w.r.t.~machine arithmetic we have
      \\[0.2cm]
      \hspace*{1.3cm}
      $1.0 \oplus 2.6881171418161356 \cdot 10^{43} = 2.6881171418161356 \cdot 10^{43}$.
      \\[0.2cm]
      Here, $\oplus$ denotes machine addition.
    
      The argument that $\ln\bigl(1 + \exp(1000)\bigr) \approx 1000$ works as follows:
      \\[0.2cm]
      \hspace*{1.3cm}
      $
      \begin{array}{lcll}
        \ln\bigl(1+\exp(x)\bigr) & = & \ln\bigl(\exp(x) \cdot (1+\exp(-x))\bigr)          \\[0.2cm]
                                 & = & \ln\bigl(\exp(x)\bigr) + \ln\bigl(1+\exp(-x)\bigr) \\[0.2cm]
                                 & = & x + \ln\bigl(1+\exp(-x)\bigr) \\[0.2cm]
                                 & \approx & x + \ln(1) + \exp(-x) & \mbox{Taylor expansion of $\ln(1+x)$} \\[0.2cm]
                                 & = & x + 0 + \exp(-x)                                      \\[0.2cm]
                                 & \approx & x                & \mbox{since $\exp(-x) \approx 0$ for large $x$} 
      \end{array}
      $
      \\[0.2cm]
      This is the reason that \texttt{logSigmoid} returns \texttt{x} if the value of \texttt{x} is less than
      $-100$.
\item The function $\mathtt{ll}(\mathbf{X}, \mathbf{y}, \mathbf{w})$ defined in line 14 computes the
      log-likelihood of the parameter $\mathbf{w}$ given the available data $\mathbf{X}$ and $\mathbf{y}$.
      We have
      \\[0.2cm]
      \hspace*{1.3cm}
      $\ds \ell\ell(\mathbf{X},\mathbf{y},\mathbf{w}) = 
           \sum\limits_{i=1}^n L\bigl(y_i \cdot(\mathbf{x}_i \cdot \mathbf{w})\bigr)
      $.
      \\[0.2cm]
      Here $L$ denotes the natural logarithm of the sigmoid of the argument.
      It is assumed that $\mathbf{X}$ is the feature matrix.  Every observation corresponds to a row in this
      matrix, i.e.~the vector $\mathbf{x}_i$ is the feature vector containing the features of the
      $i$-th observation.  $\mathbf{y}$ is a vector describing the outcomes, i.e.~the elements
      of this vector are either $+1$ or $-1$.  Finally, $\mathbf{w}$ is the vector of coefficients.
\item The function $\mathtt{gradLL}(\mathbf{x}, \mathbf{y}, \mathbf{w})$ in line 21 computes the gradient of
      the log-likelihood according to the formula
      \\[0.2cm]
      \hspace*{1.3cm}
      $\ds \frac{\partial\quad}{\partial\, w_j}\ell\ell(\mathbf{X},\mathbf{y},\mathbf{w}) =
        \ds\sum\limits_{i=1}^n y_i \cdot x_{i,j} \cdot  S(-y_i \cdot \mathbf{x}_i \cdot \mathbf{w})
      $.
      \\[0.2cm]
      The different components of this gradient are combined into a vector.
      The arguments are the same as the arguments to the log-likelihood.
\item Finally, the function \texttt{logisticRegressionFile} that is shown in Figure
      \ref{fig:logistic_regression.py:2} takes one argument.  This argument
      is the name of the \textsc{Csv} file containing the data that are to be analysed.  
      The task of this function is to read the \textsc{Csv} file, convert the data in the feature matrix
      $\mathbf{X}$ and the vector $\mathbf{y}$, and then to use the method of gradient ascent to find the weight vector
      $\mathbf{w}$ that maximize the likelihood.  In detail this function works as follows.
      \begin{enumerate}
      \item The \texttt{with} statement that extends from line 34 to line 43 reads the data in the file that
            is specified by the parameter \texttt{name}.
      \item After the data has been read, the list \texttt{Pass} contains a list of floating point numbers that
            are either $0$ or $1$ specifying whether the student has passed the exam, while the list
            \texttt{Hours} contains the numbers of hours that the students have spent studying.
      \item These data are converted into the NumPy arrays \texttt{x} and \texttt{y} in line 44 and 45.
      \item \texttt{n} is the number of data points we have, i.e.~it is the number of students.
      \item We reshape the vector \texttt{x} into a matrix \texttt{X} in line 47.
            As there is only a single feature, namely the hours a student has studied, all rows of this matrix
            have a length of $1$. 
      \item Next, we prepend a column of ones to this matrix.  This is done in line 48.
            This frees us from dealing explicitly with a bias term in our model.      
      \item In logistic regression we assume that the entries of the vector $\mathbf{y}$ are either $+1$ or
            $-1$.  As the data provided in our input file contains $1$ and $0$, we need to apply a function that maps $1$ to $+1$ and $0$ to $-1$.
            The function
            \\[0.2cm]
            \hspace*{1.3cm}
            $y \mapsto 2 \cdot y - 1$
            \\[0.2cm]
            fits this job description and is applied to transform the vector $\mathbf{y}$ appropriately in line
            49.
      \item Now we are ready to run gradient ascent.  As the start vector we use a vector containing only
            zeros.  This vector is defined in line 50.  The precision we use is $10^{-8}$.
            We want to maximize the log-likelihood of a given weight vector $\mathtt{w}$.  Hence we define the
            function $\texttt{f}(\texttt{w})$ as $\mathtt{ll}(\mathtt{X}, \mathtt{y}, \mathtt{w})$ in line 52,
            while the gradient of this function is defined in line 53.
            Line 54 call the function \texttt{gradient\_ascent} that computes the value of \texttt{w} that
            maximizes the log-likelihood.            
      \end{enumerate}
\end{enumerate}

\begin{figure}[!ht]
\centering

\begin{minted}[ frame         = lines, 
                framesep      = 0.3cm,
                bgcolor       = sepia,
                firstnumber   = 1,
                numbers       = left,
                numbersep     = -0.2cm,
                xleftmargin   = 0.8cm,
                xrightmargin  = 0.8cm,
              ]{python3}              
    import csv
    import gradient_ascent

    def logisticRegression(name):
        with open(name) as file:
            reader = csv.reader(file, delimiter=',')
            count  = 0  # line count
            Pass   = []
            Hours  = []
            for row in reader:
                if count != 0:  # skip header
                    Pass .append(float(row[0]))
                    Hours.append(float(row[1]))
                count += 1
        y = np.array(Pass)
        x = np.array(Hours)
        n = len(y)
        X = np.reshape(x, (n,1))
        X = np.append(np.ones((n, 1)), X, axis=-1)
        y = 2 * y - 1
        start   = np.zeros((2,))
        eps     = 10 ** -8
        f       = lambda w: ll(X, y, w)
        gradF   = lambda w: gradLL(X, y, w)
        w, _, _ = gradient_ascent.findMaximum(f, gradF, start, eps)
        return w
\end{minted}

\caption{The function \texttt{logisticRegression}.}
\label{fig:logistic_regression.py:2}
\end{figure}
\FloatBarrier

If we run the function \texttt{logisticRegressionFile} using the data shown in Figure
\ref{fig:exam.csv} the resulting values of the weight vector \texttt{w} are
\\[0.2cm]
\hspace*{1.3cm}
\texttt{[-4.0746468959343405, 1.5033787070592017]}
\\[0.2cm]
This shows that the probability $P(h)$ that a student who has studied for $h$ hours will pass the
exam is given approximately as follows:
\\[0.2cm]
\hspace*{1.3cm}
$\ds P(h) \approx \frac{1}{1 + \exp(4.1 - 1.5 \cdot h)}$
\\[0.2cm]
Figure \ref{fig:exam-probability.pdf} shows a plot of this probability $P(x)$.  

\begin{figure}[!th]
\epsfig{file=Figures/exam-probability.pdf, scale=0.7}
\caption{Probability of passing an exam versus hours of studying.}
\label{fig:exam-probability.pdf}
\end{figure}
\FloatBarrier

\subsection{Logistic Regression with SciKit-Learn}
In this section we discuss how linear regression is done in the SciKit-Learn environment.
We will improve on the previous example and study the data that is shown in Figure \ref{fig:exam-iq.csv} on
page \pageref{fig:exam-iq.csv}.
This \textsc{Csv} file contains data about a fictional exam.  The first column indicates whether the student
has passed or failed the exam.  A passed exam is encoded as the integer 1, while a failed exam is encoded as the integer 0.
The second column contains the number of hours that the student has studied for the exam.  The third column
contains the \href{https://en.wikipedia.org/wiki/Intelligence_quotient}{intelligence quotient}, abbreviated as
IQ.  To better understand the data, we first plot it.  This plot is shown in Figure \ref{fig:exam-iq.pdf} on
page \pageref{fig:exam-iq.pdf}.  The horizontal axis is used for the hours of study, while the vertical axis
shows the IQ of the student.  Students who have passed the exam are shown as blue dots, while those students
who have failed their exam are shown as red dots.

\begin{figure}[!ht]
\centering
\begin{Verbatim}[ frame         = lines, 
                  framesep      = 0.3cm, 
                  firstnumber   = 1,
                  labelposition = bottomline,
                  numbers       = left,
                  numbersep     = -0.2cm,
                  xleftmargin   = 0.8cm,
                  xrightmargin  = 0.8cm,
                ]
    Pass,Hours,IQ
    0,0.50,110
    0,0.75,95
    0,1.00,118
    0,1.25,97
    0,1.50,100
    0,1.75,110
    0,1.75,115
    1,2.00,104
    1,2.25,120
    0,2.50,98
    1,2.75,118
    0,3.00,88
    1,3.25,108
    0,3.50,125
    1,4.00,109
    1,4.25,110
    1,4.50,112
    1,4.75,97
    1,5.00,102
    1,5.50,109
\end{Verbatim}
\vspace*{-0.3cm}
\caption{Results of an exam given hours of study and IQ.}
\label{fig:exam-iq.csv}
\end{figure}

When we inspect the diagram shown in Figure \ref{fig:exam-iq.pdf} we see that there are two outliers: There is
one student who failed although he has an IQ of 125 and he did study for $3.5$ hours.  Maybe he was still drunk
when he had to write the exam.  There is also a student with an IQ of 104 who did pass while only studying for
$2$ hours.  He just might have gotten lucky. We expect logistic regression to classify all other students
correctly.

\begin{figure}[!th]
\epsfig{file=Figures/exam-iq.pdf, scale=0.6}
\caption{Probability of passing an exam versus hours of studying.}
\label{fig:exam-iq.pdf}
\end{figure}



\begin{figure}[!ht]
\centering
\begin{Verbatim}[ frame         = lines, 
                  framesep      = 0.3cm, 
                  firstnumber   = 1,
                  labelposition = bottomline,
                  numbers       = left,
                  numbersep     = -0.2cm,
                  xleftmargin   = 0.8cm,
                  xrightmargin  = 0.8cm,
                  commandchars  = \\\{\},
                  codes         = {\catcode`$=3\catcode`^=7\catcode`_=8}
                ]
    import numpy                as np
    import pandas               as pd
    import sklearn.linear\_model as lm
  
    ExamDF = pd.read\_csv('exam-iq.csv')
    X      = np.array(ExamDF[['Hours','IQ']])
    Y      = np.array(ExamDF['Pass'], dtype=float)
    model  = lm.LogisticRegression(C=10\_000, tol=1e-6, solver='newton-cg')
    M      = model.fit(X, Y)
    \(\vartheta_0\)     = M.intercept\_[0]
    \(\vartheta_1, \vartheta_2\)  = M.coef\_[0]
    errors = np.sum(np.abs(Y - model.predict(X)))
    print((len(Y) - errors) / len(Y))
\end{Verbatim}
\vspace*{-0.3cm}
\caption{Logistic Regression using SciKit-Learn}
\label{fig:Logistic-Regression-with-SciKit-Learn.ipynb}
\end{figure}
\FloatBarrier

Figure \ref{fig:Logistic-Regression-with-SciKit-Learn.ipynb} on page
\pageref{fig:Logistic-Regression-with-SciKit-Learn.ipynb} shows a \textsl{Python} script that creates a
logistic regression classifier with the help of the SciKit-Learn package.
\begin{enumerate}
\item In the first three lines we import the necessary modules.
      The support for logistic regression is located in the module
      \texttt{sklearn.linear\_model}.
\item In line 5, the data from the file ``\texttt{exam-iq.csv}'' is read as a Pandas
      \href{https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html}{DataFrame}.
\item Line 6 creates the feature matrix \texttt{X} by extracting the two independent variables ``\texttt{Hours}''
      and ``\texttt{IQ}''.
\item Line 7 extracts the dependent variable ``\texttt{Pass}''.  Since this variable is stored as an integer in
      the \textsc{Csv} file, we convert it into a floating point number.  This is necessary because the method
      \texttt{fit} that we use later expects the dependent variable to be encoded as a floating point number.
\item Line 8 creates an object of class \texttt{LogisticRegression}.  This object is initialized with a number
      of parameters:
      \begin{enumerate}
      \item \texttt{C} specifies the amount of \blue{regularization}.  We will discuss the concept of
            regularization later when we discuss \blue{polynomial logistic regression in the next section}.
            In this example we do not need any regularization.  Setting $C$ to a high value like $10\,000$
            prevents regularization.
      \item \texttt{tol} is the tolerance that specifies when the iterative algorithm to maximize the
            log-likelihood should stop.
      \item \texttt{solver} specifies the numerical method that is used to find the maximum of the
            log-likelihood. By choosing ``\texttt{newton-cg}'' we specify that the
            \href{https://en.wikipedia.org/wiki/Conjugate_gradient_method}{conjugate gradient}
            method should be used.  This method is more sophisticated than gradient descent, but as this is not
            a course on numerical optimization we do not have the time to discuss it.
      \end{enumerate}
\item All the real work is happening in line 9, because there we use the method \texttt{fit} to create the
      logistic regression model.
\item The next two lines are needed to extract the coefficients $\vartheta_0$, $\vartheta_1$, and $\vartheta_2$
      that specify the logistic model.      
      According to the model we have learned, the probability $P(h)$ that a student, who has learned for $h$
      hours and has an IQ of $q$, will pass the exam, is given as 
      $$ P(Y=1|h,q) = S(\vartheta_0 + \vartheta_1 \cdot h + \vartheta_2 \cdot q) $$
      In general, the model predicts that she will pass the exam if
      $$ S(\vartheta_0 + \vartheta_1 \cdot h + \vartheta_2 \cdot q) \geq \frac{1}{2} $$
      and that is the case if and only if
      $$ \vartheta_0 + \vartheta_1 \cdot h + \vartheta_2 \cdot q \geq 0. $$
      This can be rewritten as follows:
      $$  q \geq -\frac{\vartheta_0 + \vartheta_1 \cdot h}{\vartheta_2}. $$
      The \blue{decision boundary} are those values of $(h, q)$ such that $P(h, q) = \frac{1}{2}$.  This set of
      values satisfies the linear equation
      $$  q = -\frac{\vartheta_0 + \vartheta_1 \cdot h}{\vartheta_2}. $$
      We have plotted this decision boundary as a green line in Figure \ref{fig:exam-iq-boundary.pdf} on page
      \pageref{fig:exam-iq-boundary.pdf}.  Everybody who is located to the right of this line is predicted to pass the exam,
      while everybody who is located to the left is predicted to fail.


\begin{figure}[!th]
\epsfig{file=Figures/exam-iq-boundary.pdf, scale=0.6}
\caption{Probability of passing an exam versus hours of studying.}
\label{fig:exam-iq-boundary.pdf}
\end{figure}
\FloatBarrier
\item Line 12 computes the number of data for which the predictions of the model are wrong.
      The method $\texttt{predict}(X)$ takes a design matrix $X$.  Each row of $X$ is assumed to be a feature
      vector. It creates a prediction vector.  The $i$-th entry of this vector is $1$ if the model predicts a
      pass for the $i$-th student.  Otherwise this entry is $0$.
\item Line 13 prints the accuracy.  As Figure \ref{fig:exam-iq-boundary.pdf} shows, 3 examples have been
      miss-predicted.  Two of these examples where bound to be miss-predicted, but the fact that the student
      with an IQ of 120 who has studied for $2.25$ hours has also been miss-predicted is disappointing.
      The reason is that logistic regression does not maximize the accuracy but rather
      maximizes the log-likelihood.  Later, we will discuss so called 
      \blue{support vector machines}.  Often, a support vector machine is able to achieve a higher accuracy
      than logistic regression.  However, support vector machines are also more complicated than logistic regression.
 \end{enumerate}
The jupyter notebook containing the computation discussed previously can be found at my \href{https://github.com/karlstroetmann/Artificial-Intelligence}{github repository}:
\\[0.2cm]
\hspace*{0.3cm}
\href{https://github.com/karlstroetmann/Artificial-Intelligence/blob/master/Python/6 Classification/04-Logistic-Regression-with-SciKit-Learn.ipynb}
{Artificial-Intelligence/Python/6 Classification/04-Logistic-Regression-with-SciKit-Learn.ipynb}

\exercise 
The file \href{https://github.com/karlstroetmann/Artificial-Intelligence/blob/master/Python/iris.csv}{iris.csv}
contains the sizes of both the \href{https://en.wikipedia.org/wiki/Sepal}{sepals} and the
\href{https://en.wikipedia.org/wiki/Pepal}{petals} of three different specimen of the iris flower.  The
data is described in more detail \href{https://en.wikipedia.org/wiki/Iris_flower_data_set}{here}. 
Use logistic regression to predict whether a given plant is of the species
\href{https://en.wikipedia.org/wiki/Iris_setosa}{iris setosa} (Deutsch: Borsten-Schwertlilie),
\href{https://en.wikipedia.org/wiki/Iris_virginica}{iris virginica}\footnote{
This plant is native to North America and hence has no German name.}, or
\href{https://en.wikipedia.org/wiki/Iris_versicolor}{iris versicolor} (Deutsch: verschiedenfarbige Schwertlilie).
As logistic regression is only able to distinguish between two different classes, you
have to build three different classifiers:
\begin{itemize}
\item The first classifier is able to distinguish \blue{iris setosa} from other irises. 
\item The second classifier is able to distinguish \blue{iris virginica} from other irises. 
\item The third classifier is able to distinguish \blue{iris versicolor} from other irises. 
\end{itemize}
Your task is to implement these classifiers and to evaluate their accuracy.  
You should  divide the data randomly into a \blue{training dataset}, which is used for computing the
coefficients of logistic regression and a \blue{test dataset}, which you should only use to predict the
accuracy of your model.  To this end, the function
\\[0.2cm]
\hspace*{1.3cm}
\href{https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html}{\texttt{sklearn.model\_selection.train\_test\_split}}
\\[0.2cm]
might be useful.  Once you have created these classifiers, proceed to implement a classifier that inputs a
feature vector and that outputs the class of the iris flower as a string.  If you do this correctly, you can
achieve an accuracy that exceeds $95\%$.

You should also plot the data and the decision boundary.  Since we now have four features, you need to restrict
yourself to use only two of the features.  The most important features are \emph{petal length} and \emph{petal width}.

\eox

\section{Polynomial Logistic Regression}
Sometimes logistic regression does not work because the data is not \blue{linearly separable}.  For example,
Figure \ref{fig:fake-data.pdf} on page \pageref{fig:fake-data.pdf} shows a classification problem where we have
two features $x$ and $y$ and, obviously, it is not possible to separate the blue dots from the red dots by a
vertical line.

\begin{figure}[!th]
\epsfig{file=Figures/fake-data.pdf, scale=0.8}
\caption{Some fake data.}
\label{fig:fake-data.pdf}
\end{figure}

If we try to separate the data in Figure \ref{fig:fake-data.pdf} by logistic regression, we get the result
shown in Figure \ref{fig:fake-data-line.pdf} on page \pageref{fig:fake-data-line.pdf}.  The data points above
the green line are classified as red, while the data points below the green line are classified as blue.  The
accuracy achieved is about $61\%$, so more than $38\%$ of the data have been miss-classified.

\begin{figure}[!th]
\epsfig{file=Figures/fake-data-line.pdf, scale=0.8}
\caption{Fake data with linear decision boundary.}
\label{fig:fake-data-line.pdf}
\end{figure}

We can arrive at a better model if we extend our data with \blue{second order polynomial features}, i.e. we will not only
consider the features $x$ and $y$ but also use $x^2$, $y^2$, and $x \cdot y$ as features.  Then the resulting
\blue{decision boundary} will be a \href{https://en.wikipedia.org/wiki/Conic_section}{conic section}, i.e.~it
might be an \href{https://en.wikipedia.org/wiki/Ellipse}{ellipse}, a
\href{https://en.wikipedia.org/wiki/Parabola}{parabola}, or a
\href{https://en.wikipedia.org/wiki/Hyperbola}{hyperbola}. 
Figure \ref{fig:Polynomial-Logistic-Regression.ipynb-quadratic} on page
\pageref{fig:Polynomial-Logistic-Regression.ipynb-quadratic} shows a \textsl{Python} script that reads the fake
data shown in Figure \ref{fig:fake-data.pdf}, adds second order polynomial features to this data and then
performs linear regression.


\begin{figure}[!ht]
\centering

\begin{minted}[ frame         = lines, 
                framesep      = 0.3cm, 
                firstnumber   = 1,
                numbers       = left,
                numbersep     = -0.2cm,
                xleftmargin   = 0.8cm,
                xrightmargin  = 0.8cm,
              ]{python3}
    import numpy  as np
    import pandas as pd
    import sklearn.linear_model as lm
    from sklearn.model_selection import train_test_split
  
    DF = pd.read_csv('fake-data.csv')
    X  = np.array(DF[['x','y']])
    Y  = np.array(DF['class'])
  
    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=1)
  
    def logistic_regression(X_train, Y_train, X_test, Y_test, reg=10000):
        model    = lm.LogisticRegression(C=reg, tol=1e-6, solver='newton-cg')
        M        = model.fit(X_train, Y_train)
        score    = M.score(X_train, Y_train)
        yPredict = M.predict(X_test)
        accuracy = np.sum(yPredict == Y_test) / len(Y_test)
        return M, score, accuracy
  
    def extend(X):
        n  = len(X)
        fx = np.reshape(X[:,0], (n, 1))
        fy = np.reshape(X[:,1], (n, 1))
        return np.hstack([fx, fy, fx*fx, fy*fy, fx*fy])
  
    X_train_quadratic, X_test_quadratic = extend(X_train), extend(X_test)
    logistic_regression(X_train_quadratic, Y_train, X_test_quadratic, Y_test)
\end{minted}

\caption{A script for second order logistic regression.}
\label{fig:Polynomial-Logistic-Regression.ipynb-quadratic}
\end{figure}

\begin{enumerate}
\item The first three lines import the modules needed for this task.
\item As we want to split our data into a \blue{training set} and a \blue{test set}, we import the function
      \texttt{train\_test\_split} from \texttt{sklearn.model\_selection}.
\item Line 6 reads the data from the file ``\texttt{fake-data.csv}''.
\item Line 7 and 8 extract the independent variables ``\texttt{x}'' and ``\texttt{y}'' and the dependent
      variable ``\texttt{class}'' and stores them in the \blue{design matrix} \texttt{X} and the vector
      \texttt{Y}, respectively.
\item Line 10 splits the data into a \blue{training set} and a \blue{test set}.  We allocate $20\%$ of our data
      to the test set, while the remaining $80\%$ are used as training data.
      In order to be able to reproduce our results, we set \texttt{random\_state} to a fixed value.
      This forces the random number generator to always produce the same split of the data.
\item Line 12 defines the function \texttt{logistic\_regression}.  This function takes 5 arguments.
      \begin{enumerate}
      \item \texttt{X\_train} and \texttt{Y\_train} are the training data.
      \item \texttt{X\_test} and \texttt{Y\_} are the test data.
      \item \texttt{reg} is a \blue{regularization} parameter.  By default this parameter is set to a high
            value.  Setting this parameter to a high value ensures that there is no regularization.
            The default is to use next to no regularization.
      \end{enumerate}
      The function returns a triple of the form $(M, \mathtt{score}, \mathtt{accuracy})$ where
      \begin{enumerate}
      \item $M$ is the model that has been found.
      \item \texttt{score} is the fraction of data points from the training set that have been
            classified correctly.
      \item \texttt{accuracy} is the fraction of data points from the test set that have been
            classified correctly.
      \end{enumerate}
\item The function $\texttt{extend}(X)$ takes a design matrix $X$ as its argument.  In order to keep the
      implementation of this function simple, we assume that $X$ has just two features, i.e.~the matrix
      $\texttt{X}$ has shape $(n, 2)$ where $n$ is the number of rows of $X$.
      \begin{enumerate}
      \item We extract the two features of \texttt{X} in line 22 and 23.
      \item In line 24 the new feature matrix is created by stacking the original features \texttt{fx} and
            \texttt{fy} together with the new features $\mathtt{fx}^2$, $\mathtt{fy}^2$, $\mathtt{fx} \cdot \mathtt{fy}$, 
      \end{enumerate}
\item Line 26 extends both the training set and the test set with second order features.
\item Line 27 performs logistic regression using the new features.
      The use of second order feature improves the accuracy on the training set to $84\%$, while the accuracy
      on the test set improves to $76\%$.
      Figure \ref{fig:fake-data-ellipse.pdf} on page \pageref{fig:fake-data-ellipse.pdf} shows the resulting
      decision boundary.
\end{enumerate}


\begin{figure}[!th]
\epsfig{file=Figures/fake-data-ellipse.pdf, scale=0.8}
\caption{Elliptical decision boundary for fake data.}
\label{fig:fake-data-ellipse.pdf}
\end{figure}


As adding second order features has increased the accuracy considerably, we proceed to add higher order features.
Figure \ref{fig:Polynomial-Logistic-Regression.ipynb-quartic} on page
\pageref{fig:Polynomial-Logistic-Regression.ipynb-quartic} shows how we can add arbitrary polynomial features
of higher degree.  This script is a continuation of the script shown in Figure \ref{fig:Polynomial-Logistic-Regression.ipynb-quadratic}.

\begin{figure}[!th]
  \centering

\begin{minted}[ frame         = lines, 
                framesep      = 0.3cm, 
                firstnumber   = 1,
                numbers       = left,
                numbersep     = -0.2cm,
                xleftmargin   = 0.8cm,
                xrightmargin  = 0.8cm,
              ]{python3}
    from sklearn.preprocessing import PolynomialFeatures

    quartic = PolynomialFeatures(4, include_bias=False)
    X_train_quartic = quartic.fit_transform(X_train)
    X_test_quartic  = quartic.fit_transform(X_test)

    logistic_regression(X_train_quartic, Y_train, X_test_quartic, Y_test)
\end{minted}

\caption{Polynomial Logistic Regression.}
\label{fig:Polynomial-Logistic-Regression.ipynb-quartic}
\end{figure}

\begin{enumerate}
\item Line 28 imports the function \texttt{PolynomialFeatures} from \texttt{sklearn.preprocessing}.
      This function can be used to add all polynomial features up to a given order.
      We do not need a bias term here as it is automatically added by the function \texttt{LogisticRegression}.
\item Line 30 creates an object that can be used to extend a design matrix with polynomial features up to degree 4
\item Line 31 and 32 extend the training set and the test set with all polynomial features up to degree 4.
\item When we perform logistic regression with these new features, we achieve an accuracy of $88.4\%$ on the
      training set.  However, the accuracy on the test set does not improve.
      Figure \ref{fig:fake-data-quartic.pdf} on page \pageref{fig:fake-data-quartic.pdf} shows the data with
      the resulting decision boundary.
\end{enumerate}

\begin{figure}[!th]
\epsfig{file=Figures/fake-data-quartic.pdf, scale=0.8}
\caption{Fake data with a decision boundary of fourth order.}
\label{fig:fake-data-quartic.pdf}
\end{figure}

Nothing stops us from cranking the order of the polynomial features to be added higher.
Figure \ref{fig:fake-data-14.pdf} on page \pageref{fig:fake-data-14.pdf} shows what happens when we include all
polynomial features up to a degree of 14.  In this case, we $100\%$ accuracy on the training set.  However, the
accuracy on the test set is only $80\%$.  Clearly, we are overfitting the data.

\begin{figure}[!th]
\epsfig{file=Figures/fake-data-14.pdf, scale=0.8}
\caption{Fake data with a decision boundary of order 14.}
\label{fig:fake-data-14.pdf}
\end{figure}

In order to combat overfitting we need to \blue{regularize}, i.e.~we need to penalize high values of the
parameters.  If we lower the regularization parameter down to $100$, then the accuracy on the training set
drops down to $89.6\%$, but the accuracy on the test set increases to $88\%$.  The resulting decision boundary
is shown in Figure \ref{fig:fake-data-14-reg.pdf} on page \pageref{fig:fake-data-14-reg.pdf}.  Clearly, this
decision boundary looks less complicated than the boundary shown in Figure \ref{fig:fake-data-14.pdf}.
Contrary to the previous figures, this figure shows all the data.  The previous figures had only shown the
training data.

\begin{figure}[!th]
\epsfig{file=Figures/fake-data-14-reg.pdf, scale=0.8}
\caption{Fake data with a decision boundary of order 14, regularized.}
\label{fig:fake-data-14-reg.pdf}
\end{figure}

\exercise
\begin{enumerate}[(a)]
\item Assume that a design matrix $X$ has two features $x_1$ and $x_2$.  Given a natural number $n$, you want to
      extend this design matrix by adding all features of the form $x_1^{k_1} \cdot x_2^{k_2}$ such that
      $k_1 + k_2 \leq n$, i.e. you want to add all features up to a degree of $n$.
      How many features will the extended design matrix have?
\item Next, assume that the design matrix $X$ has three features $x_1$, $x_2$, and $x_3$.
      This time, you want to extend the design matrix by adding all features of the form
      $x_1^{k_1} \cdot x_2^{k_2} \cdot x_3^{k_3}$ where $k_1 + k_2 + k_3 \leq n$.
      How many features will the extended design matrix have in this case?
\item In the general case, the design matrix has $d$ features $x_1$, $\cdots$, $x_d$.  Assume you want to add all
     polynomial terms up to order $n$ as new features, i.e. you want to add all features of the form
     \\[0.2cm]
     \hspace*{1.3cm}
     $x_1^{k_1} \cdot x_2^{k_2} \cdot {\dots} \cdot x_d^{k_d}$ \quad such that $k_1 + k_2 + \cdots + k_d \leq n$.
     \\[0.2cm]
     How many features will the extended design matrix have in the general case?

     \textbf{Hint}:  You might find it helpful to revisit your old lecture notes on statistics.
     \eox
\end{enumerate}


\section{Naive Bayes Classifiers}
In this section we discuss \href{https://en.wikipedia.org/wiki/Naive_Bayes_classifier}{naive Bayes classifiers}.
Naive Bayes classifiers are an alternative method for classification which is appropriate in cases where the
features are not numerical but rather are categorical.  It starts with \href{https://en.wikipedia.org/wiki/Bayes%27_theorem}{Bayes' theorem}:
Assume we have some evidence $E$ about an object $o$ and want to know whether $o$ belongs to some class $C$.
Bayes' theorem tell us that the \blue{conditional probability} $P(C|E)$, i.e. the probability that $o$ has class $C$
given that we have observed the evidence $E$, is related to the conditional probability $P(E|C)$, which is the probability that we
observe the evidence $E$ given that $o$ has class $C$, in the following way:
\\[0.2cm]
\hspace*{1.3cm}
$\ds P(C|E) = \frac{P(E|C) \cdot P(C)}{P(E)}$.
\\[0.2cm]
For example, the evidence $E$ could be the fact that the email contains the string \texttt{"viagra"} and the
class $C$ could be the class \texttt{spam}.

This theorem is useful because often the conditional probability $P(E|C)$ that we observe some evidence $E$ in an object
$o$ of class $C$ is readily available, but the conditional probability $P(C|E)$ that an object has class $C$ if
we have observed the evidence $E$ is unknown.  For example, we know from experience how many spam mails contain
the word \texttt{"viagra"} and hence can estimate the conditional probability
\\[0.2cm]
\hspace*{1.3cm}
$P(\mathtt{"viagra"} \in m\,|\,\mathtt{spam})$
\\[0.2cm]
for a given mail $m$, 
but we do not know the conditional probability that a given mail $m$ is \texttt{spam} if it contains the word
\texttt{"viagra"}, i.e.~we do not know
\\[0.2cm]
\hspace*{1.3cm}
$P(\mathtt{spam}\,|\,\mathtt{"viagra"}\in m)$.
\\[0.2cm]
However, the later probability $P(\mathtt{spam}\,|\,\mathtt{"viagra"}\in m)$ is really what we are interested in.
Bayes' theorem show us how to compute $P(\mathtt{spam}\,|\,\mathtt{"viagra"}\in m)$ from $P(\mathtt{"viagra"} \in m\,|\,\mathtt{spam})$
In the context of machine learning, the evidence $E$ is often not just a single feature but is 
given as a list of features $f_1$, $\cdots$, $f_m$ that we are able to observe or compute.  In this case we
have to rewrite Bayes' theorem as follows:
\\[0.2cm]
\hspace*{1.3cm}
$\ds P(C\;|\;f_1 \wedge \cdots \wedge f_m) = \frac{P(f_1 \wedge \cdots \wedge f_m\;|\;C) \cdot P(C)}{P(f_1 \wedge \cdots \wedge f_m)}$.
\\[0.2cm]
In order to apply this form of Bayes' theorem to the problem of classification, we have to rewrite the expression
\\[0.2cm]
\hspace*{1.3cm}
$P(f_1 \wedge \cdots \wedge f_m\;|\;C)$.
\\[0.2cm]
In order to be able to do this, we need some theory which will be developed next.  The conditional probability
$P(A|B)$ that an event $A$ happens when it is already known that $B$ has happened is defined as 
\\[0.2cm]
\hspace*{1.3cm}
$\ds P(A|B) = \frac{P(A \wedge B)}{P(B)}$.
\\[0.2cm]
This equation can be rewritten as
\\[0.2cm]
\hspace*{1.3cm}
$P(A \wedge B) = P(A|B) \cdot P(B)$.
\\[0.2cm]
Because conditional probabilities are probabilities and hence obey all the laws for probabilities, this
equation is also true for conditional probabilities: 
\\[0.2cm]
\hspace*{1.3cm}
$P(A \wedge B\;|\;C) = P(A\;|\;B \wedge C) \cdot P(B\;|\;C)$.
\\[0.2cm]
This equation can be generalized to the so called \href{https://en.wikipedia.org/wiki/Chain_rule_(probability)}{chain rule of probability}:
\\[0.2cm]
\hspace*{1.2cm}
$
\begin{array}[t]{cl}
    &   P(A_1 \wedge \cdots \wedge A_m \;|\;C) \\[0.2cm]
  = & P(A_1 \wedge  \cdots \wedge A_{m-1}\;|\;A_{m} \wedge C) \cdot P(A_{m}\;|\;C) \\[0.2cm]
  = & P(A_1 \wedge  \cdots \wedge A_{m-2}\;|\;A_{m-1} \wedge A_{m} \wedge C) \cdot P(A_{m-1} \;|\;A_m \wedge C)\cdot P(A_{m}\;|\;C) \\
  = & \cdots \\
  = &  P(A_1 \;|\; A_2 \wedge  \cdots \wedge A_{m} \wedge C) \cdot \,\dots\, \cdot P(A_{m-1} \;|\;A_m \wedge C)\cdot P(A_{m}\;|\;C) \\[0.2cm]
  = & \prod\limits_{i=1}^m P(A_i \;|\; A_{i+1} \wedge  \cdots \wedge A_{m} \wedge C) 
\end{array}
$
\\[0.2cm]
Two events $A$ and $B$ are defined to be \href{https://en.wikipedia.org/wiki/Conditional_independence}{conditionally independent}
given an event $C$ if and only if we have
\\[0.2cm]
\hspace*{1.3cm}
$P(A \;|\; C) = P(A \;|\; B \wedge C)$.
\\[0.2cm]
To put this equation differently, once we know that $C$ holds, when it comes to the estimating the probability of $A$,
then it does not matter whether $B$ holds or not.  Now the important assumption that a \blue{naive Bayes classifier} 
makes is that in order to estimate the class $C$ of an object $o$ that has features $f_1$, $\cdots$, $f_m$ it
is assumed that the features $f_1, \cdots, f_m$ are conditionally independent once the class is known.  In
most applications of naive Bayes classifiers this assumption is not true because there might be some weak
correlation between the features.  That explains why naive Bayes classifiers are called \blue{naive}.  Still,
in practise the conditional independence of the features given 
the class is often approximately true and therefore these classifiers are useful.  If we make the
assumption of conditional independence, then the probability $P(C\;|\; f_1 \wedge \cdots \wedge f_m)$ of an
object $o$ with features $f_1$, $\cdots$, $f_m$ to be of class $C$ is given as
\\[0.2cm]
\hspace*{1.3cm}
$
\begin{array}[t]{lcll}
      P(C\;|\; f_1 \wedge \cdots \wedge f_m) 
& = & \ds \frac{P(f_1 \wedge \cdots \wedge f_m \;|\; C)}{P(f_1 \wedge \cdots \wedge f_m)} \cdot P(C) \\[0.2cm]
& = & \ds \frac{\prod\limits_{i=1}^m P(f_i \;|\; f_{i+1} \wedge \cdots \wedge f_m \wedge C)}{P(f_1 \wedge \cdots \wedge f_m)} \cdot P(C) \\[0.4cm]
& = & \ds \frac{\prod\limits_{i=1}^m P(f_i \;|\; C)}{P(f_1 \wedge \cdots \wedge f_m)} \cdot P(C) 
\end{array}
$
\\[0.2cm]
In the last line of the previous chain of equations we have used the fact that the features $f_1$, $\cdots$,
$f_m$ are conditionally independent given $C$.  Now a naive Bayes classifier works as follows: 
Assume we have a set of $n$ classes $\mathcal{C} = \{ C_1, \cdots, C_n \}$ from which we have to choose the
class of an object $o$ given the features  $f_1$, $\cdots$, $f_m$.  We assume that $o$ has class $C_k$ if
and only if the probability $P(C_k\;|\; f_1 \wedge \cdots \wedge f_m)$ is maximal with respect to all classes
of $\mathcal{C}$.  In order to be able to specify this in a more formal
way, we define the \blue{$\arg\max$} function: Given a set $S$ and a function $g:S \rightarrow \mathbb{R}$ that has
exactly one maximum, we define
\\[0.2cm]
\hspace*{1.3cm}
$\arg\max\limits_{x \in S} g(x) := \texttt{arb}\Bigl(\bigl\{ x \in S \mid \forall y \in S: g(y) \leq g(x)\bigr\}\Bigr)$,
\\[0.2cm]
where the function $\texttt{arb}(M)$ returns an arbitrary element from the set $M$. 
Since we assume that $g$ has exactly one maximum on the set $S$, the expression 
$\arg\max\limits_{x \in S} g(x)$ is well defined.  To
put the definition of  $\arg\max\limits_{x \in S} g(x)$ differently, the idea is that
$\arg\max\limits_{x \in S} g(x)$ computes the value of $x \in S$ that maximizes $g$.  Given the features  $f_1$, $\cdots$,
$f_m$, the naive Bayes classifier computes the most probable class as follows:
\\[0.3cm]
\hspace*{1.3cm}
$\ds \texttt{NaiveBayes}(f_1,\cdots,f_m) :=
  \arg\max\limits_{C \in \mathcal{C}}  \frac{\prod\limits_{i=1}^m P(f_i \;|\; C)}{P(f_1 \wedge \cdots \wedge f_m)} \cdot P(C) 
$
\\[0.3cm]
It is important to observe that the denominator $P(f_1 \wedge \cdots \wedge f_m)$ does not depend on the class
$C$.  As we only need to determine the class with the maximal probability, not the exact probability of the
class, we can simplify the definition by dropping this denominator.  Therefore the definition of the naive
Bayes classifier can be rewritten as follows:
\\[0.2cm]
\hspace*{1.3cm}
$\ds \texttt{NaiveBayes}(f_1,\cdots,f_m) :=
  \arg\max\limits_{C \in \mathcal{C}}  \left(\prod\limits_{i=1}^m P(f_i \;|\; C)\right) \cdot P(C) 
$
\\[0.2cm]
This equation can be implemented once we have a training set $T$ of objects with known classes:  
The probability $P(C)$ is the probability that an object $o$ has class $C$
if nothing else is known about this object.  $P(C)$ is estimated as the proportion of objects in $T$ that are
of class $C$:
\\[0.2cm]
\hspace*{1.3cm}
$ \ds P(C) \approx \frac{\texttt{card}\bigl(\{t \in T \;|\; \texttt{class}(t) = C \}\bigr)}{\texttt{card}(T)}$.
\\[0.2cm]
This expression is called the \blue{prior probability} of $C$ or sometimes just the \blue{prior} of $C$.
In this equation, given an object $t\in T$ the function $\texttt{class}(t)$ determines the class of the object
$t$, while $\texttt{card}(M)$ returns the number of elements of the set $M$.

Next, given a feature $f$ and a class $C$, we have to determine the \blue{conditional probability} that an
object of class $C$ exhibits the feature $f_i$, i.e. we have to determine $P(f_i \;|\; C)$.
This probability can be estimated as the proportion of those objects of class $C$ in the training set $T$ that
possess the feature $f$:
\\[0.2cm]
\hspace*{1.3cm}
$\ds P(f\;|\;C) \approx 
 \frac{\texttt{card}\bigl(\{t \in T \;|\; \texttt{class}(t) = C \wedge \texttt{has}(t, f) \}\bigr)}{
       \texttt{card}\bigl(\{t \in T \;|\; \texttt{class}(t) = C \}\bigr)} 
$
\\[0.2cm]
Here, for an object $t$ and a feature $f$ the expression $\texttt{has}(t, f)$ is true if and only if $t$ has
the feature $f$.

\subsection{Example: Spam Detection}
\href{https://en.wikipedia.org/wiki/Naive_Bayes_spam_filtering}{Spam detection} is an important application of classification.
We will see in this subsection that naive Bayes classifiers work well for spam detection.
The directory
\\[0.2cm]
\hspace*{-0.3cm}
\href{https://github.com/karlstroetmann/Artificial-Intelligence/tree/master/Python/6\%20Classification/EmailData}{ https://github.com/karlstroetmann/Artificial-Intelligence/tree/master/6\ Classification/Python/EmailData}
\\[0.2cm]
contains 960 emails that are partitioned into four subdirectories:
\begin{enumerate}
\item \texttt{spam-train} contains 350 spam emails for training,
\item \texttt{ham-train} contains 350 non-spam emails for training,
\item \texttt{spam-test} contains 130 spam emails for testing,
\item \texttt{ham-test} contains 130 non-spam emails for testing.
\end{enumerate}
This data has been collected by Ion Androutsopoulos.
Figure \ref{fig:Spam-Detection.ipynb-1} on page \pageref{fig:Spam-Detection.ipynb-1} shows the first part of the notebook that implements a naive Bayes classifier.

\begin{figure}[!ht]
\centering
\begin{minted}[ frame         = lines,
                framesep      = 0.3cm,
                firstnumber   = 1,
                numbers       = left,
                numbersep     = -0.2cm,
                xleftmargin   = 0.8cm,
                xrightmargin  = 0.8cm,
              ]{python3}
    import os
    import re
    import math
    from collections import Counter
    
    spam_dir_train = 'EmailData/spam-train/'
    ham__dir_train = 'EmailData/ham-train/'
    spam_dir_test  = 'EmailData/spam-test/'
    ham__dir_test  = 'EmailData/ham-test/'
    Directories    = [spam_dir_train, ham__dir_train, spam_dir_test, ham__dir_test]
    
    no_spam    = len(os.listdir(spam_dir_train))
    no_ham     = len(os.listdir(ham__dir_train))
    spam_prior = no_spam / (no_spam + no_ham)
    ham__prior = no_ham  / (no_spam + no_ham)
    
    def get_words(fn: str) -> set[str]:
        with open(fn, 'r') as file:
            text: str = file.read().lower()
            return set(re.findall(r"[\w']+", text))
    
    def read_all_files(Directories: list[str]) -> Counter:
        Words: Counter = Counter()
        for directory in Directories:
            for file_name in os.listdir(directory):
                Words.update(get_words(directory + file_name))
        return Words
    
    Word_Counter = read_all_files(Directories)

    Common_Words = { w for w in Word_Counter if Word_Counter[w] >= 10 }
\end{minted}
\caption{A Naive Bayes Classifier for Spam Detection: Part \texttt{I}}
\label{fig:Spam-Detection.ipynb-1}
\end{figure}

The code performs the following steps:
\begin{enumerate}
\item It imports the necessary modules. \texttt{os} is used to read directories, \texttt{re} for regular
  expressions, \texttt{math} for the logarithmic function, and \texttt{Counter} for counting word occurrences.
\item It defines the directories containing the data and computes the prior probabilities \texttt{spam\_prior} and \texttt{ham\_\_prior}.
\item The function \texttt{get\_words} reads a file, converts the text to lower case, and extracts all words using a regular expression. It returns a set of words, ensuring each word is counted only once per email.
\item \texttt{read\_all\_files} reads all emails and creates a global counter \texttt{Word\_Counter} of all words found.
\item Finally, \texttt{Common\_Words} is defined as the set of words that appear in at least 10 emails. This helps in feature selection by ignoring rare words.
\end{enumerate}

Figure \ref{fig:Spam-Detection.ipynb-2} on page \pageref{fig:Spam-Detection.ipynb-2} shows the second part of the implementation.

\begin{figure}[!ht]
\centering
\begin{minted}[ frame         = lines,
                framesep      = 0.3cm,
                firstnumber   = last,
                numbers       = left,
                numbersep     = -0.2cm,
                xleftmargin   = 0.8cm,
                xrightmargin  = 0.8cm,
              ]{python3}
    def get_common_words(fn: str) -> set[str]:
        return get_words(fn) & Common_Words
    
    def count_commmon_words(directory: str) -> Counter:
        Words: Counter = Counter()
        for file_name in os.listdir(directory):
            Words.update(get_common_words(directory + file_name))
        return Words
    
    Spam_Counter = count_commmon_words(spam_dir_train)
    Ham__Counter = count_commmon_words(ham__dir_train)
    
    Spam_Probability = {}
    Ham__Probability = {}
    for w in Common_Words:
        Spam_Probability[w] = (Spam_Counter[w] + 1) / (no_spam + 1)
        Ham__Probability[w] = (Ham__Counter[w] + 1) / (no_ham  + 1)
\end{minted}
\caption{A Naive Bayes Classifier for Spam Detection: Part \texttt{II}}
\label{fig:Spam-Detection.ipynb-2}
\end{figure}

\begin{enumerate}
\item \texttt{get\_common\_words} extracts only the common words from a given file.
\item \texttt{count\_common\_words} counts the occurrences of common words in all files within a directory.
\item \texttt{Spam\_Counter} and \texttt{Ham\_\_Counter} store the counts of each common word in spam and ham emails, respectively.
\item We then compute the conditional probabilities $P(w | \texttt{spam})$ and $P(w | \texttt{ham})$ for each common word. Laplace smoothing is applied by adding 1 to the counts and the denominators to avoid zero probabilities.
\end{enumerate}

The final part of the classifier implements the classification logic and evaluates the model.
In order to compute whether a mail is spam or ham we have to compute
\\[0.2cm]
\hspace*{1.3cm}
$\ds \arg\max\limits_{C \in \{\mathtt{Spam},\, \mathtt{Ham}\}}  \left(\prod\limits_{i=1}^m P(f_i \;|\; C)\right) \cdot P(C) $
\\[0.2cm]
Therefore, we have to check, whether
\\[0.2cm]
\hspace*{1.3cm}
$\ds \left(\prod\limits_{i=1}^m P(f_i \;|\; \mathtt{Spam})\right) \cdot P(\mathtt{Spam}) > \left(\prod\limits_{i=1}^m P(f_i \;|\; \mathtt{Ham})\right) \cdot P(\mathtt{Ham}) $
\\[0.2cm]
holds. When implementing the formula
\\[0.2cm]
\hspace*{1.3cm}
$\ds \arg\max\limits_{C \in \mathcal{C}}  \left(\prod\limits_{i=1}^m P(f_i \;|\; C)\right) \cdot P(C) $
\\[0.2cm]
we have to be careful, because a naive implementation will evaluate the product
\\[0.2cm]
\hspace*{1.3cm}
$\ds \prod\limits_{i=1}^m P(f_i \;|\; C)$
\\[0.2cm]
as the number $0$ due to numerical underflow.  The trick to compute this product is to remember that
\\[0.2cm]
\hspace*{1.3cm}
$ \ln(a \cdot b) = \ln(a) + \ln(b) $
\\[0.2cm]
and to transform the product into a sum of logarithms.  As the logarithm is a monotone function, we have
\\[0.2cm]
\hspace*{1.3cm}
$ \begin{array}{llcl}
  & \ds \left(\prod\limits_{i=1}^m P(f_i \;|\; \mathtt{Spam})\right) \cdot P(\mathtt{Spam}) & > & \ds \left(\prod\limits_{i=1}^m P(f_i \;|\; \mathtt{Ham})\right) \cdot P(\mathtt{Ham}) \\[0.5cm]
  \Leftrightarrow \qquad &
  \ds \sum\limits_{i=1}^m \ln\bigl(P(f_i \;|\; \mathtt{Spam})\bigr) + \ln\bigl(P(\mathtt{Spam})\bigr) & > & \ds \sum\limits_{i=1}^m \ln\bigl(P(f_i \;|\; \mathtt{Ham})\bigr) + \ln\bigl(P(\mathtt{Ham}) \bigr)
  \end{array}
$
\\[0.2cm]
The function \texttt{log\_probabilities(fn)} takes a filename \texttt{fn} as its first argument and returns the pair
\\[0.2cm]
\hspace*{1.3cm}
$\ds \left(\sum\limits_{i=1}^m \ln\bigl(P(f_i \;|\; \mathtt{Ham})\bigr) + \ln\bigl(P(\mathtt{Ham}) \bigr),\quad
         \sum\limits_{i=1}^m \ln\bigl(P(f_i \;|\; \mathtt{Spam})\bigr) + \ln\bigl(P(\mathtt{Spam})\bigr) \right)$
\\[0.2cm]
as its result.  It should be noted that these numbers are not really the logarithms of probabilities.  The reason is that
the formula for the probability of a class $C$ is
\\[0.2cm]
\hspace*{1.3cm}
$\ds \frac{\prod\limits_{i=1}^m P(f_i \;|\; C)}{P(f_1 \wedge \cdots \wedge f_m)} \cdot P(C) $
\\[0.2cm]
and we are not computing the denominator $P(f_1 \wedge \cdots \wedge f_m)$.  However, this denominator is the same for spam and ham and hence we don't need it when comparing the respective probabilities.  Therefore, what the function \texttt{log\_probabilities} is really computing are pairs of \blue{relative logarithmic probabilities}.

\begin{figure}[!ht]
\centering
\begin{minted}[ frame         = lines,
                framesep      = 0.3cm,
                firstnumber   = last,
                numbers       = left,
                numbersep     = -0.2cm,
                xleftmargin   = 0.8cm,
                xrightmargin  = 0.8cm,
                escapeinside  = ||
              ]{python3}
    def log_probabilities(fn: str) -> tuple[float, float]:
        log_p_spam: float = math.log(spam_prior)
        log_p_ham:  float = math.log(ham__prior)
        words: set[str] = get_common_words(fn)
        for w in Common_Words:
            if w in words:
                log_p_spam += math.log(Spam_Probability[w])
                log_p_ham  += math.log(Ham__Probability[w])
        return (log_p_ham, log_p_spam)
\end{minted}
\caption{A Naive Bayes Classifier for Spam Detection: Part \texttt{III}}
\label{fig:Spam-Detection.ipynb-3}
\end{figure}

\begin{figure}[!ht]
\centering
\begin{minted}[ frame         = lines,
                framesep      = 0.3cm,
                firstnumber   = last,
                numbers       = left,
                numbersep     = -0.2cm,
                xleftmargin   = 0.8cm,
                xrightmargin  = 0.8cm,
                escapeinside  = ||
              ]{python3}
    def precission_recall(spam_dir: str,
                          ham_dir:  str,
                          |$\vartheta$|: float) -> tuple[float, float, float]:
        TN: int = 0 # true negatives
        FP: int = 0 # false positives
        for email in os.listdir(spam_dir):
            log_p_ham, log_p_spam = log_probabilities(spam_dir + email)
            if log_p_spam > |$\vartheta$| * log_p_ham:
                TN += 1
            else:
                FP += 1
        FN: int = 0 # false negatives
        TP: int = 0 # true positives
        for email in os.listdir(ham_dir):
            log_p_ham, log_p_spam = log_probabilities(ham_dir + email)
            if log_p_spam > |$\vartheta$| * log_p_ham:
                FN += 1
            else:
                TP += 1
        precision: float = TP / (TP + FP)
        recall:    float = TP / (TP + FN)
        accuracy:  float = (TN + TP) / (TN + TP + FN + FP)
        return precision, recall, accuracy
\end{minted}
\caption{A Naive Bayes Classifier for Spam Detection: Part \texttt{III}}
\label{fig:Spam-Detection.ipynb-4}
\end{figure}



The function \texttt{precission\_recall} takes two directories as arguments: \texttt{spam\_dir} is supposed to contain spam emails, while \texttt{ham\_dir} contains ham emails.  It computes the \blue{precision} and the \blue{recall} of our spam classifier with respect to these test data.
Since it would be quite bad when we misclassify a valid email as spam but we can tolerate an occasional spam email that gets through our filter, we add a third parameter $\vartheta$.  We will only classify an email as spam if
\\[0.2cm]
\hspace*{1.3cm}
$\ds \sum\limits_{i=1}^m \ln\bigl(P(f_i \;|\; \mathtt{Spam})\bigr) + \ln\bigl(P(\mathtt{Spam})\bigr) \; > \;
\vartheta \cdot \left( \sum\limits_{i=1}^m \ln\bigl(P(f_i \;|\; \mathtt{Ham})\bigr) + \ln\bigl(P(\mathtt{Ham}) \bigr) \right)
$
\\[0.2cm]
The third parameter $\vartheta$ is the logarithmic threshold. Later, we will use a logarithmic threshold of $0.8$.
To make things concrete, assume that
\begin{itemize}
\item The relative logarithmic probability $p_1$ of a mail being ham is $p_1 = -250$, while
\item the relative logarithmic probability $p_2$ of this mail being spam is $p_2 = -200$.
\end{itemize}
As we have
\\[0.2cm]
\hspace*{1.3cm}
$p_1 = -250 < -200 = p_2$
\\[0.2cm]
we would classify this mail as spam if we would just compare the relative logarithmic probabilities.
However, if we use the logarithmic threshold of $0.8$ we have
\\[0.2cm]
\hspace*{1.3cm}
$\vartheta \cdot p_1 = 0.8 \cdot (-250) = -200 = p_2 $
\\[0.2cm]
and hence the inequality
\\[0.2cm]
\hspace*{1.3cm}
$\vartheta \cdot p_1 < p_2$
\\[0.2cm]
would not be valid.  Therefore, we would conservatively classify the email as ham.

\subsection{Naive Bayes Classifier with Numerical Features}
We can build a naive Bayes classifier even if some of our features are numerical.  Assume we have a feature $f$
that is a numerical attribute.  The tricky part is to come up with a way to compute the conditional probability
\\[0.2cm]
\hspace*{1.3cm}
$P(f = x \mid C)$
\\[0.2cm]
which is the conditional probability that the feature $f$ has the value $x$ if the object to classify has class
$C$.  The idea is to assume that for every class $C$ the values of the feature $f$ have a Gaussian
distribution.  Then we have
\\[0.2cm]
\hspace*{1.3cm}
$\ds P(f = x \mid C) = \frac{1}{\sqrt{2\cdot\pi\;} \cdot \sigma_{f,C}} \cdot \exp\left(-\frac{\bigl(x-\mu_{f,C}\bigr)^2}{2 \cdot \sigma_{f,C}^2}\right)$,
\\[0.2cm]
where $\mu_{f,C}$ is the \blue{mean value} of the feature $f$ for those objects that have class $C$, while
$\sigma_{f,C}^2$ is the \blue{variance} of the feature $f$ for objects of class $C$.

\exercise
We have already investigated the file
\href{https://github.com/karlstroetmann/Artificial-Intelligence/blob/master/Python/iris.csv}{iris.csv}
in a previous exercise.   This time, your task is to implement a \blue{naive Bayes classifier} that is able to
classify iris flowers.  You can achieve an accuracy that exceeds $95\%$.
\eox

\section{Support Vector Machines}
\blue{Support Vector Machines} (abbreviated as \textsc{SVM}s) had been in invented in 1963 by Vladimir
Naumovich Vapnik and Alexey Yakovlevich Chervonenkis.   However, they only got widespread acceptance in 1995
when Cortes and Vapnik published a paper explaining the \blue{kernel trick} \cite{cortes:1995}.  This section will introduce
support vector machines.  In order to motivate \textsc{SVMs}, we first explain why logistic regression
sometimes behaves suboptimally.  After that, we explain the mathematical theory of support vector machines.
Finally, we show how we can use the support vector machines provided by SciKit Learn.

\subsection{Non-Optimality of Logistic Regression}

\begin{figure}[!th]
\epsfig{file=Figures/largest-margin.pdf, scale=0.6}
\caption{Three points to separate.}
\label{fig:largest-margin.pdf}
\end{figure}

Figure \ref{fig:largest-margin.pdf} on page \pageref{fig:largest-margin.pdf} shows three points that belong to
two different classes.  The blue dot at position $(3.5, 3.5)$ belongs to class $1$, while the two red crosses
at position $(1,2)$ and $(2,1)$ belong to the class $-1$.  When we build a classifier to separate these two
classes, we would ideally like the decision boundary to be the green line that passes through the points $(0,5)$ and
$(5,0)$, since this line has the biggest distance from both classes.  Figure
\ref{fig:largest-margin-logistic.pdf} on page \pageref{fig:largest-margin-logistic.pdf} shows how logistic regression
deals with with this problem.

\begin{figure}[!th]
\epsfig{file=Figures/largest-margin-logistic.pdf, scale=0.6}
\caption{Three points separated by logistic regression.}
\label{fig:largest-margin-logistic.pdf}
\end{figure}

A close inspection of Figure \ref{fig:largest-margin-logistic.pdf} shows that the decision boundary calculated
by logistic regression is nearer to the blue dot than it is to the red crosses.  If we add a large number of
blue points right next to the first blue points, the decision boundary found by logistic regression moves away
from the blue points, as shown in Figure \ref{fig:largest-margin-more.pdf} on
page \pageref{fig:largest-margin-more.pdf},
then the decision boundary moves away from the first blue point that marks the margin of the two classes.
These new blue points do not add real information, since they are further away from the red crosses than the
first blue point.  Hence it is counter-intuitive that the addition of these points changes the decision boundary.

\begin{figure}[!th]
\epsfig{file=Figures/largest-margin-more.pdf, scale=0.5}
\caption{Points separated by logistic regression.}
\label{fig:largest-margin-more.pdf}
\end{figure}


\FloatBarrier

\vspace*{\fill}
\pagebreak


\section{Digression: The Method of Lagrange Multipliers}
\label{section:lagrange-multipliers}
\index{Lagrange multipliers}

In machine learning, we often need to find the maximum or minimum of a function $f(\mathbf{x})$ where the variable $\mathbf{x}$ is not allowed to vary freely but is constrained to a specific set of values. This is known as a \blue{constrained optimization problem}.

The \href{https://en.wikipedia.org/wiki/Lagrange_multiplier}{method of Lagrange multipliers} is a strategy for finding the local maxima and minima of a function subject to equality constraints.

\subsection{Geometric Intuition}
To understand the method, let us look at a concrete example in 2D space.
We want to maximize the function:
\\[0.2cm]
\hspace*{1.3cm}
$f(x, y) = x \cdot y$
\\[0.2cm]
subject to the constraint that the point $(x, y)$ must lie on a circle of radius $\sqrt{2}$:
\\[0.2cm]
\hspace*{1.3cm}
$g(x, y) = x^2 + y^2 - 2 = 0$.

\begin{figure}[ht]
    \centering
    \begin{tikzpicture}[scale=2.2]
        % Draw grid and axes
        \draw[step=0.5, lightgray, very thin] (-1.8,-1.8) grid (1.8,1.8);
        \draw[->] (-2,0) -- (2,0) node[right] {$x$};
        \draw[->] (0,-2) -- (0,2) node[above] {$y$};

        % Draw the constraint g(x,y) = x^2 + y^2 = 2 (Circle radius sqrt(2) approx 1.414)
        \draw[blue, thick] (0,0) circle (1.414cm);
        \node[blue, fill=white, inner sep=1pt] at (1.2, -1.4) {$g(x,y)=0$};

        % --- POINT B (Optimal) in 1st Quadrant ---
        % Tangency at (1,1) where f(x,y) = 1
        % Level Curve f=1
        \draw[green!60!black, thick, domain=0.4:1.8] plot (\x, {1/\x});
        \node[green!60!black, right] at (1.8, 0.55) {$f=1$};
        
        \coordinate (B) at (1, 1);
        \fill[black] (B) circle (1.5pt) node[above right] {$B$};
        
        % Gradients at B (Parallel)
        % grad f = (1, 1)
        \draw[->, red, thick] (B) -- ++(0.5, 0.5) node[right] {$\nabla f$};
        % grad g = (2, 2) -> scaled to (0.4, 0.4) for visual
        \draw[->, blue, thick] (B) -- ++(0.35, 0.35) node[left, yshift=5pt] {$\nabla g$};


        % --- POINT A (Non-optimal) in 3rd Quadrant ---
        % Point on circle: x approx -0.5, y approx -1.32. (xy approx 0.66)
        % Level Curve f=0.66
        \draw[magenta!60!black, thick,  domain=-1.8:-0.35] plot (\x, {0.66/\x});
        \node[magenta!60!black, left] at (-1.8, -0.36) {$f \approx 0.66$};

        \draw[green!60!black, thick,  domain=-1.8:-0.47] plot (\x, {1/\x});
        \node[green!60!black, left] at (-1.8, -0.58) {$f = 1$};

        \coordinate (A) at (-0.5, -1.32);
        \fill[black] (A) circle (1.5pt) node[below left] {$A$};
        
        % Gradient of f at A: (y, x) -> (-1.32, -0.5). Scaled by 0.5 -> (-0.66, -0.25)
        \draw[->, red, thick] (A) -- ++(-0.66, -0.25) node[left] {$\nabla f$};
        
        % Gradient of g at A: (2x, 2y) -> (-1, -2.64). Scaled by 0.2 -> (-0.2, -0.53)
        \draw[->, blue, thick] (A) -- ++(-0.2, -0.53) node[below] {$\nabla g$};

    \end{tikzpicture}
    \caption{Maximizing $f(x,y)=xy$ subject to $x^2+y^2=2$. At point $A$ (3rd quadrant), the level curve cuts through the constraint; the gradients are not parallel. At point $B$ (1st quadrant), the level curve is tangent to the constraint; the gradients are parallel.}
    \label{fig:lagrange_concrete}
\end{figure}


\noindent
Figure \ref{fig:lagrange_concrete} on page \pageref{fig:lagrange_concrete} illustrates this problem.
\begin{enumerate}
    \item The \blue{blue circle} represents our constraint $g(x, y) = 0$.  We have to stay on this line.
    \item The \green{green lines} (hyperbolas) represent the \blue{level curves} of $f(x,y)$. Every point on a
          green curve has the same value for $x \cdot y$.  For the green curve in the first quadrant $f(x,y) = 1$,
          while on the green curve in the third quadrant this value is approximately $0.66$.          
\end{enumerate}

Consider the point $A$ in the third quadrant. At this point, the value of our function is approximately $f=0.66$.
Notice that the circle \blue{crosses} the dashed level curve.
\begin{itemize}
    \item The red arrow ($\nabla f$) points in the direction where $f$ increases most rapidly.
    \item The blue arrow ($\nabla g$) points in the direction perpendicular to the circle (the constraint).
\end{itemize}
At point $A$, these vectors point in different directions. This geometry implies that if we move along the blue
circle, we can still find a path that increases the value of $f$. Therefore, the point $A$ cannot be the
maximum of the function $f$.


Now consider point $B$ at $(1,1)$. Here, the value of the function is $f=1.0$.
Notice that the circle is \blue{tangent} to the green level curve.
At this optimal point, the direction of greatest increase for $f$ is exactly parallel to the normal vector of the circle. We cannot move along the constraint without decreasing $f$.

Mathematically, this condition of parallel gradients is written as:
\\[0.2cm]
\hspace*{1.3cm}
$\nabla f(x, y) = \lambda \cdot \nabla g(x, y)$.
\\[0.2cm]
Here, $\lambda$ is a real number called the \blue{Lagrange multiplier}.
We can rewrite this as finding the stationary points of a new function, the \blue{Lagrangian}:
\\[0.2cm]
\hspace*{1.3cm}
$\mathcal{L}(x, y, \lambda) := f(x, y) - \lambda \cdot g(x, y)$.

\subsection{Solving the Example}
Let us solve the specific example from Figure \ref{fig:lagrange_concrete} mathematically.
\begin{itemize}
    \item We want to maximize the function $f(x, y) = x \cdot y$
    \item subject to the constraint: $g(x, y) = x^2 + y^2 - 2 = 0$.
\end{itemize}
The \blue{Lagrangian} is:
\\[0.2cm]
\hspace*{1.3cm}
$\mathcal{L}(x, y, \lambda) = x \cdot y - \lambda \cdot (x^2 + y^2 - 2)$.
\\[0.2cm]
We compute the partial derivatives of the Lagrangian $\mathcal{L}$ and set them to zero:
\begin{enumerate}
    \item $\ds \frac{\partial \mathcal{L}}{\partial x} = y - 2\cdot\lambda\cdot x = 0 \quad \Rightarrow \quad y = 2\cdot\lambda\cdot x$
    \item $\ds \frac{\partial \mathcal{L}}{\partial y} = x - 2\cdot\lambda\cdot y = 0 \quad \Rightarrow \quad x = 2\cdot\lambda\cdot y$
    \item $\ds \frac{\partial \mathcal{L}}{\partial \lambda} = -(x^2 + y^2 - 2) = 0 \quad \Rightarrow \quad x^2 + y^2 = 2$
\end{enumerate}
Substituting the first equation ($y=2\cdot\lambda\cdot x$) into the second equation gives:
\\[0.2cm]
\hspace*{1.3cm}
$x = 2\cdot\lambda\cdot(2\cdot\lambda\cdot x) = 4\cdot\lambda^2\cdot x$.
\\[0.2cm]
This implies $x\cdot(1 - 4\cdot\lambda^2) = 0$. Since $x=0$ would imply $y=0$ (which violates the constraint), we must have:
\\[0.2cm]
\hspace*{1.3cm}
$\ds1 - 4\cdot\lambda^2 = 0 \quad \Rightarrow \quad \lambda^2 = \frac{1}{4} \quad \Rightarrow \quad \lambda = \pm \frac{1}{2}$.
\\[0.2cm]
If $\lambda = 1/2$, then $y = x$. Substituting into the constraint $x^2 + y^2 = 2$:
\\[0.2cm]
\hspace*{1.3cm}
$2\cdot x^2 = 2 \quad \Rightarrow \quad x = \pm 1$.
\\[0.2cm]
This gives us the maxima at $(1,1)$ and $(-1,-1)$, where $f(x,y)=1$.
This matches point $B$ in our figure.

If $\lambda = -1/2$, then $y = -x$. Substituting into the constraint $x^2 + y^2 = 2$ yields:
\\[0.2cm]
\hspace*{1.3cm}
$2\cdot x^2 = 2 \quad \Rightarrow \quad x = \pm 1$.
\\[0.2cm]
This gives us the minima at $(1,-1)$ and $(-1,1)$, where $f(x,y)=-1$.

\exercise
Assume that $\mathbf{w}\in \mathbb{R}^n$ is a vector of Euclidean length $1$, that is the equation
$\mathbf{w} \cdot \mathbf{w} = 1$ holds.  Furthermore, assume $b \in \mathbb{R}$.  Let us define the hyperplane $P$ as the set
\\[0.2cm]
\hspace*{1.3cm}
$P := \bigl\{\, \mathbf{x} \in \mathbb{R}^n \mid \mathbf{w}\cdot \mathbf{x} + b = 0 \,\bigr\}$.
\\[0.2cm]
What is the distance between the hyperplane $P$ and the origin, i.e.~the point $\mathbf{0}$?
\vspace*{0.1cm}

\noindent
\textbf{Hint}: Solve this problem using the method of Lagrangian multipliers.  For any vector
$\mathbf{x} \in \mathbb{R}$, the Euclidean length of $\mathbf{x}$ is defined as $\sqrt{\mathbf{x} \cdot\mathbf{x}}$.
If we want to find the distance of the plane to the origin, we have to find a vector $\mathbf{x} \in P$
such that the length of $\mathbf{x}$ is minimal.  The constraint is that the vector $\mathbf{x}$
has to be a member of the hyperplane and therefore we must have $\mathbf{w}\cdot\mathbf{x} + b = 0$.

Furthermore, we can simplify the calculation if we minimize the function $\mathbf{x} \cdot\mathbf{x}$
instead of the function $\sqrt{\mathbf{x} \cdot\mathbf{x}}$.  This is possible because the function
$x \mapsto \sqrt{x}$ is a monotonous function.
\eox


\subsection{The Mathematical Theory of Support Vector Machines}
The main idea of support vector machines is to have a decision boundary that is \blue{as simple as possible} and that
\blue{separates the different classes as much as possible}.  In two dimensions, the simplest decision boundary is a
line.  In $n$ dimensions, the simplest decision boundary is an $(n-1)$-dimensional hyperplane.
A hyperplane separates two different classes as much as possible if the distance to both classes is maximized.

A hyperplane can be defined by a vector $\mathbf{w}$ that is perpendicular to the hyperplane together with a
bias $b$:  A vector $\mathbf{x}$ is an element of the hyperplane if and only if
\\[0.2cm]
\hspace*{1.3cm}
$\mathbf{w} \cdot \mathbf{x} + b = 0$.
\\[0.2cm]
In order for the decision boundary to separate the positive examples from the negative examples, we add the
following two conditions.  If $\mathbf{x}^{(i)}$ is a positive example, then we don't just want that 
\\[0.2cm]
\hspace*{1.3cm}
$\mathbf{w} \cdot \mathbf{x}^{(i)} + b \geq 0$.
\\[0.2cm]
Instead, we demand that 
\begin{equation}
  \label{eq:svm-1}
  \mathbf{w} \cdot \mathbf{x}^{(i)} + b \geq 1
\end{equation}
holds.  Similarly, if $\mathbf{x}^{(i)}$ is a negative example, we want to have that
\begin{equation}
  \label{eq:svm-2}
    \mathbf{w} \cdot \mathbf{x}^{(i)} + b \leq -1
\end{equation}
holds.  Let us define the class of a positive example to be $+1$ and the
class of a negative example to be $-1$.  Let $y_i$ denotes the class of example $\mathbf{x}^{(i)}$.
Let us multiply equation \ref{eq:svm-1} by $y_i = 1$.  Unsurprisingly, we get
\begin{equation}
  \label{eq:svm-3}
  y_i \cdot \bigl(\mathbf{w} \cdot \mathbf{x}^{(i)} + b) \geq 1.
\end{equation}
Similarly, let us multiply equation \ref{eq:svm-1} by $y_i = -1$.  This time, things get more interesting
as the direction of the inequality is flipped:
\begin{equation}
  \label{eq:svm-4}
    y_i \cdot \bigl(\mathbf{w} \cdot \mathbf{x}^{(i)} + b) \geq 1.
\end{equation}
Notice that the equations \ref{eq:svm-3} and \ref{eq:svm-4} are the same!  Hence
inequation \ref{eq:svm-3} holds for both positive and negative examples.  We rewrite the last inequation as
\begin{equation}
  \label{eq:svm-5}
    y_i \cdot \bigl(\mathbf{w} \cdot \mathbf{x}^{(i)} + b\bigr) - 1 \geq 0.
\end{equation}
Those vectors $\mathbf{x}^{(i)}$ that satisfy the equality
\\[0.2cm]
\hspace*{1.3cm}
$y_i \cdot \bigl(\mathbf{w} \cdot \mathbf{x}^{(i)} + b\bigr) - 1 = 0$
\\[0.2cm]
are at the \blue{margins} of their respective classes and are called \blue{support vectors}.  These vectors
have the smallest distance to the hyperplane defined by $\mathbf{w}$ and $b$.  Let us compute the width of the
separation of the positive class from the negative class if the decision boundary is given by the  equation
$\mathbf{w} \cdot \mathbf{x} + b = 0$.  To this end, assume that $\mathbf{x}_+$ is a positive support vector,
i.e.~we have
\begin{equation}
  \label{eq:svm-6}
  \mathbf{w} \cdot \mathbf{x}_+ + b = 1,  
\end{equation}
while $\mathbf{x}_-$ is a negative support vector and therefore satisfies
\begin{equation}
  \label{eq:svm-7}
  \mathbf{w} \cdot \mathbf{x}_- + b = -1.  
\end{equation}
Since the vector $\mathbf{w}$ is perpendicular to the hyperplane that defines the decision boundary, the $\mathtt{width}$
between the positive and the negative example is given by the projection $\mathbf{x}_+ - \mathbf{x}_-$ on the normalized vector $\mathbf{w}$:
\begin{equation}
  \label{eq:svm-8}
 \mathtt{width} = \bigl(\mathbf{x}_+ - \mathbf{x}_-\bigr) \cdot \frac{\mathbf{w}}{||\mathbf{w}||} =
\bigl(\mathbf{x}_+ \cdot \mathbf{w} - \mathbf{x}_-\cdot \mathbf{w}\bigr) \cdot \frac{1}{||\mathbf{w}||}   
\end{equation}
If we subtract equation \ref{eq:svm-7} from equation \ref{eq:svm-6}, the constant $b$ cancels and we are left
with 
\\[0.2cm]
\hspace*{1.3cm}
$\mathbf{x}_+ \cdot \mathbf{w} - \mathbf{x}_- \cdot \mathbf{w} = 2$.  
\\[0.2cm]
Substituting this equation into equation \ref{eq:svm-8} yields the equation
\\[0.2cm]
\hspace*{1.3cm}
$\ds \mathtt{width} = \frac{2}{||\mathbf{w}||}$.
\\[0.2cm]
Hence in order to maximize the width of the separation of the two classes from the decision boundary we have to
minimize the size of the vector $\textbf{w}$ subject to the constraints given in equation \ref{eq:svm-5}.  Now minimizing $\mathbf{w}$
is the same as minimizing
\\[0.2cm]
\hspace*{1.3cm}
$\ds \frac{1}{2} \cdot ||\mathbf{w}||^2 = \frac{1}{2} \cdot \sum\limits_{k=1}^d w_k^2$,
\\[0.2cm]
where $d$ is the number of features.
Determining a minimum of a function that is subject to a set of constraints requires us to use 
\href{https://en.wikipedia.org/wiki/Lagrange_multiplier}{Lagrange multipliers}.  Assuming our training set 
has the form $\{ \mathbf{x}^{(1)}, \cdots \mathbf{x}^{(n)} \}$,
we define the \blue{Lagrangian} $\mathcal{L}(\mathbf{w}, b, \alpha_1, \cdots, \alpha_n)$ as follows:
\\[0.2cm]
\hspace*{1.3cm}
$\ds \mathcal{L}(\mathbf{w}, b, \alpha_1, \cdots, \alpha_n) := 
 \frac{1}{2} \cdot ||\mathbf{w}||^2 - 
\sum\limits_{i=1}^n \alpha_i \cdot \bigl(y_i \cdot(\mathbf{w} \cdot \mathbf{x}_i + b) - 1\bigr).
$ 
\\[0.2cm]
The sum in this Lagrangian sums over all training examples although not all training examples have to satisfy
the constraint
\\[0.2cm]
\hspace*{1.3cm}
$y_i \cdot(\mathbf{w} \cdot \mathbf{x}_i + b) - 1 = 0$.
\\[0.2cm]
This is not a problem because for those $i \in \{1,\cdots,n\}$
where we only have the inequality
\\[0.2cm]
\hspace*{1.3cm}
$y_i \cdot(\mathbf{w} \cdot \mathbf{x}_i + b) - 1 \geq 0$
\\[0.2cm]
we can just assume that the corresponding Lagrange multiplier $\alpha_i$ is  equal to $0$.
A necessary condition for the values of $\mathbf{w}$, $b$ and $\alpha_i$ that minimize $\mathcal{L}$ is that
the partial derivatives of $\mathcal{L}$ with respect to $w_k$, $b$ and $\alpha_i$ are all $0$.  Let us first compute the partial derivative of $\mathcal{L}$
with respect to $w_k$:
\\[0.2cm]
\hspace*{1.3cm}
$\ds \frac{\partial \mathcal{L}}{\partial w_k} = 
 w_k - \sum\limits_{i=1}^n \alpha_i \cdot y_i \cdot x^{(i)}_k = 0
$
\\[0.2cm]
Therefore, we must have that
\\[0.2cm]
\hspace*{1.3cm}
$w_k = \sum\limits_{i=1}^n \alpha_i \cdot y_i \cdot x^{(i)}_k $ 
\\[0.2cm]
and this implies
\begin{equation}
  \label{eq:svm-9}
  \mathbf{w} = \sum\limits_{i=1}^n \alpha_i \cdot y_i \cdot \mathbf{x}^{(i)}
\end{equation}
Therefore the vector $\mathbf{w}$ is a linear combination of the \blue{support vectors}, where a vector
$\mathbf{x}^{(i)}$ is a \blue{support vector} \index{support vector} iff  $\alpha_i \not= 0$.  Hence a support vector $\mathbf{x}^{(i)}$ 
must satisfy the equality
\\[0.2cm]
\hspace*{1.3cm}
$y_i \cdot(\mathbf{w} \cdot \mathbf{x}^{(i)} + b) - 1 = 0$.
\\[0.2cm]
Next, let us compute the partial derivative of $\mathcal{L}$ with respect to $b$. We have
\\[0.2cm]
\hspace*{1.3cm}
$\ds \frac{\partial \mathcal{L}}{\partial b} = - \sum\limits_{i=1}^n \alpha_i \cdot y_i = 0$
\\[0.2cm]
which implies that
\begin{equation}
  \label{eq:svm-10}
  \sum\limits_{i=1}^n \alpha_i \cdot y_i = 0  
\end{equation}
Let us rewrite the Lagrangian by substituting $\mathbf{w}$ with the right hand side of equation \ref{eq:svm-9}:
\\[0.2cm]
\hspace*{-0.5cm}
$
\begin{array}[t]{lcl}
  \mathcal{L} & = &
                    \ds \frac{1}{2} \cdot \left(\sum\limits_{i=1}^n \alpha_i \cdot y_i \cdot \mathbf{x}^{(i)}\right)
                    \cdot \left(\sum\limits_{j=1}^n \alpha_j \cdot y_j \cdot \mathbf{x}^{(j)}\right) 
               - \sum\limits_{i=1}^n \alpha_i \cdot \left( y_i \cdot\left( \mathbf{x}^{(i)} \cdot
                    \left(\sum\limits_{j=1}^n \alpha_j \cdot y_j \cdot \mathbf{x}^{(j)} \right) + b\right) -1 \right) \\[0.6cm]
              & = &
                    \ds \frac{1}{2} \cdot \sum\limits_{i=1}^n \sum\limits_{j=1}^n \alpha_i \cdot \alpha_j \cdot
                    y_i \cdot y_j \cdot \bigl(\mathbf{x}^{(i)} \cdot \mathbf{x}^{(j)}\bigr) 
                    - \sum\limits_{i=1}^n \sum\limits_{j=1}^n \alpha_i \cdot \alpha_j \cdot y_i \cdot y_j \cdot
                     \bigl( \mathbf{x}^{(i)} \cdot \mathbf{x}^{(j)} \bigr) - b \cdot\underbrace{\sum\limits_{i=1}^n \alpha_i \cdot y_i}_{=0}
                    + \sum\limits_{i=1}^n \alpha_i \\[0.6cm]
              & = & \ds \sum\limits_{i=1}^n \alpha_i - \frac{1}{2} \cdot \sum\limits_{i=1}^n
                    \sum\limits_{j=1}^n \alpha_i \cdot \alpha_j \cdot y_i \cdot y_j \cdot
                    \bigl(\mathbf{x}^{(i)} \cdot \mathbf{x}^{(j)}\bigr)
\end{array}
$
\\[0.2cm]
Now, the \blue{crucial observation} is the following:  The Lagrangian $\mathcal{L}$ only depends on the dot
products $\mathbf{x}^{(i)} \cdot \mathbf{x}^{(j)}$.  Why is this a big deal?  Often, a set of data point is not
linearly separable in the given space.  However, it might be possible to transform the feature vectors
$\mathbf{x} \in \mathbb{R}^d$ into some higher dimensional space $\mathbb{R}^h$ where $h > d$ and in this
higher dimensional space the two
classes are separable.  Concretely, it is sometimes possible to define a transformation function
\\[0.2cm]
\hspace*{1.3cm}
$\Phi: \mathbb{R}^d \rightarrow \mathbb{R}^h$
\\[0.2cm]
such that the set of transformed data points
$\bigl\{\langle\Phi(\mathbf{x}^{(1)}),y_1 \rangle, \cdots, \langle\Phi(\mathbf{x}^{(n)}), y_n\rangle\bigr\}$ is linearly separable.
The question then is to find such a transformation $\Phi$.  Here is the punch line:  As the Lagrangian does
only depend on the dot products $\Phi(x^{(i)}) \cdot \Phi(x^{(j)})$, it is sufficient to define the transformed
dot products
\\[0.2cm]
\hspace*{1.3cm}
$\Phi\bigl(\mathbf{x}\bigr) \cdot \Phi\bigl(\mathbf{y}\bigr) $.
\\[0.2cm]
This is done with the help of so called \blue{kernel functions}:  We define the dot product
$\Phi\bigl(\mathbf{x}\bigr) \cdot \Phi\bigl(\mathbf{y}\bigr)$ as
\\[0.2cm]
\hspace*{1.3cm}
$\Phi\bigl(\mathbf{x}\bigr) \cdot \Phi\bigl(\mathbf{y}\bigr) := k(\mathbf{x}, \mathbf{y})$
\\[0.2cm]
where $k$ is called a \blue{kernel function}\index{kernel function}.  There are two kernel functions that are
quite popular:
\begin{enumerate}
\item \blue{Polynomial kernels} have the form
      \\[0.2cm]
      \hspace*{1.3cm}
      $k(\mathbf{x}, \mathbf{y}) = (\mathbf{x} \cdot \mathbf{y} + c)^n$,
      \\[0.2cm]
      where $n$ is a natural number called the \blue{degree} of the kernel.
      The number $c$ is a hyperparameter that is often set to $1$.
\item \blue{Gaussian kernels} have the form
      \\[0.2cm]
      \hspace*{1.3cm}
      $\ds k(\mathbf{x}, \mathbf{y}) = \exp\left( \frac{||\mathbf{x}-\mathbf{y}||^2}{2 \cdot \sigma^2}\right)$.
      \\[0.2cm]
      Here, $\sigma$ is a hyperparameter.  
\end{enumerate}
Using a kernel function to simulate a parameter transformation is known as the
\blue{kernel trick}.\index{kernel trick}    
Experience has shown that the kernel functions given above often enable us to transform a data set into a space where the
data set is linearly separable.  Figure \ref{fig:strange-data.pdf} on page \pageref{fig:strange-data.pdf} shows
a set of point that is separable using a support vector machine with a Gaussian kernel.


If you want to know more about support vector machines, the free book
\href{https://www.syncfusion.com/ebooks/support_vector_machines_succinctly}{Support Vector Machines Succinctly}
by Alexandre Kowalczyk \cite{kowalczyk:2017} is a good place to start.

\begin{figure}[!th]
\epsfig{file=Figures/strange-data.pdf, scale=0.6}
\caption{Points separated by a support vector machine.}
\label{fig:strange-data.pdf}
\end{figure}
\pagebreak

\section{Support Vector Machines on Medical Data}

In this section, we apply our implementation of Support Vector Machines to a real-world medical dataset: the \href{https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(diagnostic)}{Breast Cancer Wisconsin (Diagnostic) Dataset}.
This problem involves classifying breast mass samples as either \blue{Malignant} (harmful) or \blue{Benign} (non-harmful) based on features computed from digitized images of a fine needle aspirate (FNA) of a breast mass.

\subsection{Data Preprocessing: Standardization}
Real-world data often presents a challenge: features have vastly different scales. For example, the mean area of a cell nucleus might range from $200$ to $2500$, while the smoothness might range from $0.05$ to $0.16$.
This is problematic for the \blue{Gaussian Kernel} (RBF), which relies on the Euclidean distance $||\mathbf{x} - \mathbf{y}||^2$.
If we do not normalize the data, features with large numerical values (like Area) will dominate the distance calculation, effectively rendering features with small values (like Smoothness) irrelevant.
To fix this, we apply \blue{standardization} (also known as Z-score normalization) to scale the data such that each feature has a mean of $0$ and a standard deviation of $1$.

\subsection{From Hard Margin to Soft Margin}
The SVM theory derived in Section \ref{chapter:classification} assumed that the data is \blue{linearly separable} (potentially after a kernel transformation). This is known as the \blue{Hard Margin SVM}. The constraints derived were:
\\[0.2cm]
\hspace*{1.3cm}
$y_i \cdot(\mathbf{w} \cdot \mathbf{x}_i + b) \geq 1$.
\\[0.2cm]
In medical diagnosis, data often overlaps significantly. A Hard Margin SVM would fail to find a solution
because no hyperplane can perfectly separate the classes. To handle this, we introduce \blue{slack variables}
\index{slack variables} $\xi_i \geq 0$ which allow individual data points to violate the margin constraint: 
\\[0.2cm]
\hspace*{1.3cm}
$y_i \cdot(\mathbf{w} \cdot \mathbf{x}_i + b) \geq 1 - \xi_i$.
\\[0.2cm]
Our goal is still to maximize the margin (minimize $||\mathbf{w}||^2$), but we must now also penalize the slack variables. The new \blue{primal objective} becomes:
\\[0.2cm]
\hspace*{1.3cm}
$\ds \text{Minimize } \frac{1}{2}||\mathbf{w}||^2 + C \cdot \sum_{i=1}^n \xi_i$.
\\[0.2cm]
Here, $C$ is a \blue{regularization parameter}.  If $C$ is large, we penalize mistakes heavily.
On the other hand, if $C$ is small, we allow more margin violations to obtain a simpler boundary.
Remarkably, when we solve the Lagrangian for this new problem, the Lagrangian remains
exactly the same as derived previously: 
\\[0.2cm]
\hspace*{1.3cm}
$\ds \mathcal{L}(\alpha) =
\sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \cdot\alpha_j\cdot y_i\cdot y_j\cdot
K(\mathbf{x}_i, \mathbf{x}_j)$.
\\[0.2cm]
Here, $K(\mathbf{x}_i, \mathbf{x}_j)$ is the kernel function discussed earlier.
However, the constraints on the Lagrange multipliers $\alpha_i$ change. Instead of just $\alpha_i \geq 0$, we
obtain the \blue{box constraints}: 
\\[0.2cm]
\hspace*{1.3cm}
$0 \leq \alpha_i \leq C$.
\\[0.2cm]
This result allows us to implement a so called \blue{soft margin SVM} \index{soft margin SVM}
by simply adding an upper bound to our Lagrange multipliers $\alpha_i$.

\subsection*{Derivation of the Soft Margin Lagrangian}
\label{subsec:soft-margin-derivation}

In this subsection, we formally derive the \blue{Dual Optimization Problem} for the Soft Margin SVM. This derivation explains why the objective function remains unchanged compared to the Hard Margin case, while the constraints on $\alpha_i$ change to $0 \leq \alpha_i \leq C$.

\subsubsection{The Primal Problem}
Recall that for the Soft Margin SVM, we introduce \blue{slack variables} $\xi_i \geq 0$ to allow misclassifications. We want to maximize the margin (minimize $||\mathbf{w}||^2$) while penalizing the sum of the slack variables.
The \blue{Primal Optimization Problem} is defined as follows:
\\[0.2cm]
\hspace*{1.3cm}
$\ds \text{Minimize } \quad \frac{1}{2} ||\mathbf{w}||^2 + C \cdot\sum_{i=1}^n \xi_i$
\\[0.2cm]
Subject to the constraints:
\begin{enumerate}
    \item $y_i \cdot(\mathbf{w} \cdot \mathbf{x}_i + b) \geq 1 - \xi_i$ \quad for all $i=1, \dots, n$
    \item $\xi_i \geq 0$ \quad for all $i=1, \dots, n$
\end{enumerate}

\subsubsection{Constructing the Lagrangian}
To solve this constrained optimization problem, we use the method of \blue{Lagrange multipliers}. We need to introduce a multiplier for every constraint:
\begin{itemize}
    \item We associate a multiplier $\alpha_i \geq 0$ with the margin constraint:
    \\[0.2cm]
    \hspace*{1.3cm}
    $y_i \cdot(\mathbf{w} \cdot \mathbf{x}_i + b) - 1 + \xi_i \geq 0$.
    \item We associate a multiplier $\beta_i \geq 0$ with the non-negativity constraint of the slack variable:
    \\[0.2cm]
    \hspace*{1.3cm}
    $\xi_i \geq 0$.
\end{itemize}
The \blue{Lagrangian} $\mathcal{L}(\mathbf{w}, b, \boldsymbol{\xi}, \boldsymbol{\alpha}, \boldsymbol{\beta})$ is constructed by subtracting the weighted sum of constraints from the objective function:
\\[0.2cm]
\hspace*{1.3cm}
$\ds \mathcal{L} = \underbrace{\frac{1}{2} \cdot||\mathbf{w}||^2 + C \cdot\sum_{i=1}^n \xi_i}_{\text{Objective}} - \sum_{i=1}^n \alpha_i \bigl[ y_i \cdot(\mathbf{w} \cdot \mathbf{x}_i + b) - 1 + \xi_i \bigr] - \sum_{i=1}^n \beta_i \cdot\xi_i$

\subsubsection{Minimizing with respect to Primal Variables}
We must minimize $\mathcal{L}$ with respect to the primal variables $\mathbf{w}$, $b$, and $\xi_i$. To do this, we compute the partial derivatives and set them to zero.
\begin{enumerate}
\item Derivative with respect to $\mathbf{w}$:
      \\[0.2cm]
      \hspace*{1.3cm}
      $\ds \frac{\partial \mathcal{L}}{\partial \mathbf{w}} = \mathbf{w} - \sum_{i=1}^n \alpha_i \cdot y_i\cdot
      \mathbf{x}_i = 0 \quad \Rightarrow \quad \mathbf{w} = \sum_{i=1}^n \alpha_i \cdot y_i \cdot\mathbf{x}_i$
      \\[0.2cm]
      This result is identical to the hard margin case.
\item Derivative with respect to $b$:
      \\[0.2cm]
      \hspace*{1.3cm}
      $\ds \frac{\partial \mathcal{L}}{\partial b} = - \sum_{i=1}^n \alpha_i \cdot y_i = 0 \quad
         \Rightarrow \quad \sum_{i=1}^n \alpha_i \cdot y_i = 0$
      \\[0.2cm]
      This is also identical to the hard margin case.
\item Derivative with respect to $\xi_i$:
      \\[0.2cm]
      \hspace*{1.3cm}
      $\ds \frac{\partial \mathcal{L}}{\partial \xi_i} = C - \alpha_i - \beta_i = 0 \quad
         \Rightarrow \quad \alpha_i + \beta_i = C$
      \\[0.2cm]
      This equation is the key to the Soft Margin formulation. It relates the \blue{regularization parameter} $C$ to
      our Lagrange multipliers $\alpha_i$ and $\beta_i$. 
\end{enumerate}

\subsubsection{Deriving the Box Constraints}
We now show how the constraints $0 \leq \alpha_i \leq C$ for $i=1,\cdots,n$ are derived.
From the definition of the Lagrangian, we know that all Lagrangian multipliers must be non-negative, i.e.~we
have 
\\[0.2cm]
\hspace*{1.3cm}
$\alpha_i \geq 0 \quad \text{and} \quad \beta_i \geq 0$ \quad for all $i=1,\cdots,n$.
\\[0.2cm]
The derivative with respect to $\xi_i$ gave us the relation $\alpha_i + \beta_i = 0$.  Therefor we have
\\[0.2cm]
\hspace*{1.3cm}
$\beta_i = C - \alpha_i$ \quad for all $i=1,\cdots,n$.
\\[0.2cm]
Substituting this into the inequality $\beta_i \geq 0$, we get:
\\[0.2cm]
\hspace*{1.3cm}
$C - \alpha_i \geq 0 \quad \Rightarrow \quad \alpha_i \leq C$.
\\[0.2cm]
Combining this with $\alpha_i \geq 0$, we obtain the \blue{box constraints}
\\[0.2cm]
\hspace*{1.3cm}
\colorbox{red}{\framebox{\colorbox{orange}{$0 \leq \alpha_i \leq C$}}} \quad for all $i=1,\cdots,n$.
\\[0.2cm]
Finally, we substitute the relations back into the Lagrangian to eliminate the variables
$\mathbf{w} = (w_1,\cdots, w_d)^\top$.
Let us collect the terms involving the slack variables $\xi_i$:
\\[0.2cm]
\hspace*{1.3cm}
$\ds \sum_{i=1}^n \xi_i \cdot (C - \alpha_i - \beta_i)$.
\\[0.2cm]
Since $\alpha_i + \beta_i = C$, the term inside the parenthesis is zero.  Therefore all terms involving $\xi_i$ vanish
and we are left with the exact same terms as previously when we had not introduced slack variables!
Therefore, the \blue{optimization problem} is again
\\[0.2cm]
\hspace*{1.3cm}
$\ds \text{Maximize } \quad \sum_{i=1}^n \alpha_i - \frac{1}{2}
 \cdot\sum_{i=1}^n \sum_{j=1}^n \alpha_i \cdot\alpha_j\cdot y_i\cdot y_j \cdot (\mathbf{x}_i \cdot \mathbf{x}_j)$
\\[0.2cm]
subject to the constraints
\\[0.2cm]
\hspace*{1.3cm}
$\ds \sum_{i=1}^n \alpha_i \cdot y_i = 0 \quad \text{and} \quad 0 \leq \alpha_i \leq C$.
\\[0.2cm]
Of course, the constraints have changed now, as we have added the constraints $\alpha_i \leq C$.

\subsection{Implementation}
Finally, we are ready to present the implementation of the class \texttt{SVM}. This class implements soft
margin support vector machines.
We have split the code into several figures to make it easier to digest. 

\subsubsection{Initialization}
Figure \ref{fig:svm_init} shows the initialization of the class. The class accepts a \blue{kernel} function and
the regularization parameter \texttt{C}. 
If \texttt{C} is set to \texttt{None}, we assume a hard margin.

\begin{figure}[!ht]
\centering
\begin{minted}[ frame         = lines, 
                framesep      = 0.3cm, 
                bgcolor       = sepia,
                mathescape,
                numbers       = left,
                numbersep     = -0.2cm,
                xleftmargin   = 0.8cm,
                xrightmargin  = 0.8cm,
                fontsize      = \small
              ]{python3}
    class SVM:
        def __init__(self, kernel=gaussian_kernel, C=1.0):
            self.kernel = kernel
            self.C      = C  # Regularization parameter
            self.alpha  = None
            self.support_vectors       = None
            self.support_vector_labels = None
            self.b = 0
\end{minted}
\caption{SVM Initialization.}
\label{fig:svm_init}
\end{figure}

\subsubsection{Optimization (The fit Method)}
The core training logic is implemented in the \texttt{fit} method, shown in Figure \ref{fig:svm_fit}.
The optimization process involves the following mathematical details:

\begin{enumerate}
\item Lines 11--14 compute the matrix $K$ where $K_{i,j} = K(\mathbf{x}_i, \mathbf{x}_j)$.
      In the literature $K$ is called the \blue{Gram matrix}\index{Gram matrix}.
      Pre-computing $K$ is essential for performance, as these values are used $n^2$ times in every step of the
      optimization. 
\item Lines 17--18 define the function to minimize.
      The mathematical goal is to \blue{maximize} $\mathcal{L}(\alpha)$.
      However, standard numerical solvers like \texttt{minimize}` are designed to find the \blue{minimum}. Therefore, we minimize the \blue{negative} of the dual objective:
    \\[0.2cm]
    \hspace*{1.3cm}
    $\ds \text{minimize } J(\alpha) = \frac{1}{2} \cdot \sum_{i,j} \alpha_i \cdot\alpha_j \cdot y_i \cdot y_j \cdot K_{ij} - \sum_i \alpha_i$.
    
    \item \textbf{Box Constraints}: Lines 21--22 implement the Soft Margin constraints derived earlier.
    \begin{itemize}
        \item The equality constraint \texttt{np.dot(alpha, y)} corresponds to $\sum \alpha_i \cdot y_i = 0$.
        \item The bounds \texttt{(0, self.C)} correspond to $0 \leq \alpha_i \leq C$.
    \end{itemize}
\end{enumerate}

\begin{figure}[!ht]
\centering
\begin{minted}[ frame         = lines, 
                framesep      = 0.3cm, 
                bgcolor       = sepia,
                mathescape,
                numbers       = left,
                numbersep     = -0.2cm,
                xleftmargin   = 0.8cm,
                xrightmargin  = 0.8cm,
                fontsize      = \small,
                firstnumber   = 9
              ]{python3}
    def fit(self, X, y):
        n_samples, n_features = X.shape
        K = np.zeros((n_samples, n_samples))
        for i in range(n_samples):
            for j in range(n_samples):
                K[i, j] = self.kernel(X[i], X[j])
        def objective(alpha):
            return 0.5 * np.sum((np.outer(alpha, alpha) * \
                   np.outer(y, y)) * K) - np.sum(alpha)
        constraints = ({'type': 'eq', 'fun': lambda alpha: np.dot(alpha, y)})
        bounds      = [(0, self.C) for _ in range(n_samples)]
        initial_alpha = np.zeros(n_samples)
        result = minimize(objective, initial_alpha, method='SLSQP', 
                          bounds=bounds, constraints=constraints)
        self.alpha = result.x
        sv_indices = self.alpha > 1e-4
        self.support_vectors = X[sv_indices]
        self.support_vector_labels = y[sv_indices]
        self.alpha = self.alpha[sv_indices]
        self.compute_bias()
\end{minted}
\caption{The training method \texttt{fit}.}
\label{fig:svm_fit}
\end{figure}

\subsubsection{Computing the Bias}
Calculating the bias $b$ in a Soft Margin SVM requires careful consideration of the
\blue{Karush-Kuhn-Tucker (KKT) conditions}.
The KKT conditions for the primal problem imply the following complementarity conditions for the optimal solution:
\begin{align*}
    \alpha_i \cdot (y_i \cdot(\mathbf{w} \cdot \mathbf{x}_i + b) - 1 + \xi_i) &= 0 \\
    (C - \alpha_i) \cdot \xi_i &= 0
\end{align*}
These conditions divide our support vectors into two types:
\begin{enumerate}
\item \blue{Bound Support Vectors} ($\alpha_i = C$):
       Here $\xi_i > 0$, meaning the point violates the margin. We cannot use these to calculate $b$ precisely
       because the exact value of $\xi_i$ is unknown. 
 \item \blue{Free Support Vectors} ($0 < \alpha_i < C$):
       Here $C - \alpha_i > 0$, which implies $\xi_i = 0$. These points lie \underline{exactly} on the margin.
\end{enumerate}
For any free support vector $\mathbf{x}_k$, we have:
\\[0.2cm]
\hspace*{1.3cm}
$y_k \cdot (\mathbf{w} \cdot \mathbf{x}_k + b) = 1$.
\\[0.2cm]
Multiplying by $y_k$ (and noting $y_k^2=1$) gives:
\\[0.2cm]
\hspace*{1.3cm}
$\mathbf{w} \cdot \mathbf{x}_k + b = y_k \quad \Rightarrow \quad b = y_k - \mathbf{w} \cdot \mathbf{x}_k$.
\\[0.2cm]
Substituting the dual form $\mathbf{w} = \sum \alpha_j \cdot y_j \cdot \mathbf{x}_j$,
we get the formula implemented in Figure \ref{fig:svm_bias}:
\\[0.2cm]
\hspace*{1.3cm}
$\ds b = y_k - \sum_{j \in SV} \alpha_j \cdot y_j \cdot K(\mathbf{x}_j, \mathbf{x}_k)$.
\\[0.2cm]
We compute this value for all free support vectors and take the average to ensure numerical stability.

\begin{figure}[!ht]
\centering
\begin{minted}[ frame         = lines, 
                framesep      = 0.3cm, 
                bgcolor       = sepia,
                mathescape,
                numbers       = left,
                numbersep     = -0.2cm,
                xleftmargin   = 0.8cm,
                xrightmargin  = 0.8cm,
                fontsize      = \small,
                firstnumber   = 29
              ]{python3}
    def compute_bias(self):
        free_sv_indices = (self.alpha < self.C - 1e-4)
        if np.any(free_sv_indices):
            b_values = []
            free_alphas = self.alpha[free_sv_indices]
            free_vecs   = self.support_vectors[free_sv_indices]
            free_labels = self.support_vector_labels[free_sv_indices]
            for i in range(len(free_alphas)):
                sum_term = 0
                for j in range(len(self.alpha)):
                    sum_term += self.alpha[j] * self.support_vector_labels[j] * \
                                self.kernel(self.support_vectors[j], free_vecs[i])
                b_values.append(free_labels[i] - sum_term)
            self.b = np.mean(b_values)
        else:
            self.b = 0
\end{minted}
\caption{Computing the bias $b$.}
\label{fig:svm_bias}
\end{figure}
\FloatBarrier

\subsubsection{Prediction}
Finally, Figure \ref{fig:svm_predict} shows the prediction logic. The decision boundary is determined by the
sign of the hyperplane equation: 
\\[0.2cm]
\hspace*{1.3cm}
$\ds f(\mathbf{x}) = \text{sign}\left( \sum_{i \in SV} \alpha_i \cdot y_i \cdot K(\mathbf{x}_i, \mathbf{x}) + b \right)$.
\\[0.2cm]
If the result is positive, we classify the tumour as $+1$, i.e.~benign, otherwise we classify it as $-1$,
i.e.~malignant. 

\begin{figure}[!ht]
\centering
\begin{minted}[ frame         = lines, 
                framesep      = 0.3cm, 
                bgcolor       = sepia,
                mathescape,
                numbers       = left,
                numbersep     = -0.2cm,
                xleftmargin   = 0.8cm,
                xrightmargin  = 0.8cm,
                fontsize      = \small,
                firstnumber   = 46
              ]{python3}
    def predict(self, X):
        y_pred = []
        for x in X:
            prediction = 0
            for i in range(len(self.alpha)):
                prediction += self.alpha[i] * self.support_vector_labels[i] * \
                              self.kernel(self.support_vectors[i], x)
            y_pred.append(prediction + self.b)
        return np.sign(y_pred)
\end{minted}
\caption{The  method \texttt{predict}.}
\label{fig:svm_predict}
\end{figure}
\FloatBarrier



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "artificial-intelligence"
%%% End:
 
