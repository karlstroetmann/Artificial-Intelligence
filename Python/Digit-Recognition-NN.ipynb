{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<style>\n",
    ".container { width:100% } \n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handwritten Digit Recognition\n",
    "\n",
    "In this notebook we show how feed-forward neural networks can be used to recognize handwritten digits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data that we are using is stored in a <a href=\"https://docs.python.org/3/library/gzip.html\">gzipped</a>, \n",
    "<a href=\"https://docs.python.org/3/library/pickle.html\">pickled</a> file.  Therefore, we need to import the corresponding libraries to access the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our data is stored as tuples of `numpy` arrays, we have to import numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to be able to show the images of the handwritten digits, we use `matplotlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to import the module `random` as we are using <em style=\"color:blue\">stochastic gradient descent</em> to compute the weights of our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function $\\texttt{vectorized_result}(d)$ converts a digit $d \\in \\{0,\\cdots,9\\}$ into a `numpy` array $\\mathbf{x}$ of shape $(10, 1)$ such that we have\n",
    "$$\n",
    "\\mathbf{x}[i] = \n",
    "\\left\\{\n",
    "  \\begin{array}{ll}\n",
    "     1 & \\mbox{if $i = d$;} \\\\\n",
    "     0 & \\mbox{otherwise}\n",
    "  \\end{array}  \n",
    "\\right.\n",
    "$$\n",
    "for all $i \\in \\{0,\\cdots,9\\}$.\n",
    "This function is used to convert a digit $d$ into the expected output of a neural network that has an output unit for every digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorized_result(d):\n",
    "    e    = np.zeros((10, 1), dtype=np.float32)\n",
    "    e[d] = 1.0\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_result(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file `mnist.pkl.gz` contains a triple of the form\n",
    "```\n",
    "train, validate, test\n",
    "```\n",
    "Here `train` is a pair of the form `(X, y)` where\n",
    "- `X` is a numpy array of shape `(50000, 784)`,\n",
    "- `y` is a numpy array of shape `(50000, )`.\n",
    "\n",
    "For every $i \\in \\{0,\\cdots, 49,000\\}$ we have that $\\textbf{X}[i]$ is an image of a handwritten digit and $\\textbf{y}[i]$ is a digit, i.e. an element of the set \n",
    "$\\{0,\\cdots,9\\}$.\n",
    "\n",
    "The structure of `validate` and `test` is similar, but these contain only $10,000$ images each.\n",
    "\n",
    "The function $\\texttt{load_data}()$ returns a pair of the form\n",
    "$$ (\\texttt{training_data}, \\texttt{test_data}) $$\n",
    "where \n",
    "- $\\texttt{training_data}$ is a list containing 50,000 pairs $(\\textbf{x}, \\textbf{y})$       s.t. \n",
    "  - $\\textbf{x}$ is a 784-dimensional `numpy.ndarray` containing the input image, and   \n",
    "  - $\\textbf{y}$ is a 10-dimensional `numpy.ndarray` corresponding to the correct digit for \n",
    "    $\\textbf{x}$.   \n",
    "- To keep things simple, we do not use the validation data.\n",
    "- $\\texttt{test_data}$ is a list containing 10,000 pairs $(\\textbf{x}, y)$.  In each case, \n",
    "  $\\textbf{x}$ is a 784-dimensional `numpy.ndarray` containing the input image, \n",
    "  and $y \\in \\{0,\\cdots,9\\}$ is the corresponding digit value.\n",
    "\n",
    "Note that the formats for training data and test data are different.  For the training data $\\textbf{y}$ is a vector, but for the test data $y$ is a number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    with gzip.open('mnist.pkl.gz', 'rb') as f:\n",
    "        train, validate, test = pickle.load(f, encoding=\"latin1\")\n",
    "    print(f'shape of training data: {(train[0].shape, train[1].shape)}')\n",
    "    training_inputs    = [np.reshape(x, (784, 1)) for x in train[0]]\n",
    "    training_results   = [vectorized_result(y) for y in train[1]]\n",
    "    training_data      = list(zip(training_inputs, training_results))\n",
    "    test_inputs        = [np.reshape(x, (784, 1)) for x in test[0]]\n",
    "    test_data          = list(zip(test_inputs, test[1]))\n",
    "    return training_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We store the data in two variables: `training_data` and `test_data`. \n",
    "- `training_data` is a list of pairs of the form $(\\textbf{x}, \\textbf{y})$ where \n",
    "   $\\textbf{x}$ is a `numpy` array of shape $(784, 1)$ representing the image of a digit, \n",
    "   while $\\textbf{y}$ is a `numpy` array of shape $(10, 1)$ that is a \n",
    "   <a href=\"https://en.wikipedia.org/wiki/One-hot\">one-hot encoding</a> \n",
    "   of the digit shown in $\\textbf{x}$.\n",
    "- `test_data` is a list of pairs of the form $(\\textbf{x}, y)$ where \n",
    "   $\\textbf{x}$ is a `numpy` array of shape $(784, 1)$ representing the image of a digit, \n",
    "   while $y$ is an element of the set $\\{0,\\cdots,9\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "training_data, test_data = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function $\\texttt{show_digit}(\\texttt{row}, \\texttt{columns}, \\texttt{offset})$ \n",
    "shows $\\texttt{row} \\cdot \\texttt{columns}$ images of the training data.  The first image shown is the image at index $\\texttt{offset}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_digits(rows, columns, offset=0):\n",
    "    f, axarr = plt.subplots(rows, columns)\n",
    "    for r in range(rows):\n",
    "        for c in range(columns):\n",
    "            i     = r * columns + c + offset\n",
    "            image = 1 - training_data[i][0]\n",
    "            image = np.reshape(image, (28, 28))\n",
    "            axarr[r, c].imshow(image, cmap=\"gray\")\n",
    "            axarr[r, c].axis('off')\n",
    "    # plt.savefig(\"digits.pdf\")    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_digits(3, 6, 18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to find the <em style=\"color:blue;\">weight matrices</em> and <em style=\"color:blue;\">biases</em> for a neural net that is \n",
    "able to recognize the digits shown in these images.  We initialize these weight matrices randomly. The function $\\texttt{rndMatrix}(\\texttt{rows}, \\texttt{cols})$ returns a matrix of shape $(\\texttt{rows}, \\texttt{cols})$ that is filled with random numbers that have a Gaussian distribution with mean $0$ and variance $\\displaystyle\\frac{1}{\\texttt{rows}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rndMatrix(rows, cols):\n",
    "    return np.random.randn(rows, cols) / np.sqrt(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rndMatrix(2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function $\\texttt{sigmoid}(x)$ computes the sigmoid of $x$, which is defined as\n",
    "$$ \\texttt{sigmoid}(x) = S(x) := \\frac{1}{1 + \\texttt{exp}(-x)}. $$ \n",
    "Since we are using NumPy to compute the exponential function, this function also works when $x$ is a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid(np.array([-1, 0, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function $\\texttt{sigmoid_prime}(x)$ computes the derivative of the sigmoid function for $x$.  The implementation is based on the equation:\n",
    "$$ S'(x) = S(x) \\cdot \\bigl(1 - S(x)\\bigr) $$\n",
    "where $x$ can either be a number or a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_prime(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid_prime(np.array([-5, 0, 5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class `Network` is used to represent a \n",
    "<em style=\"color:blue\">feedforward neural network</em> with one hidden layer.\n",
    "The constructor is called with the argument `hiddenSize`.  This parameter specifies the number of neurons in the hidden layer.  The network has $28 \\cdot 28 = 784$ input nodes.  Each of the input nodes corresponds to the gray scale value of a single pixel in a $28 \\cdot 28$ gray scale image of the digit that is to be recognized.  The number of output neurons is 10.  For $i \\in \\{0,\\cdots,9\\}$, the $i$th output neuron tries to recognize the digit $i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    def __init__(self, hiddenSize):\n",
    "        self.mInputSize  = 28 * 28\n",
    "        self.mHiddenSize = hiddenSize\n",
    "        self.mOutputSize = 10\n",
    "        self.mBiasesH    = np.zeros((self.mHiddenSize, 1))   # biases hidden layer\n",
    "        self.mBiasesO    = np.zeros((self.mOutputSize, 1))   # biases output layer\n",
    "        self.mWeightsH   = rndMatrix(self.mHiddenSize, self.mInputSize)  # weights hidden layer\n",
    "        self.mWeightsO   = rndMatrix(self.mOutputSize, self.mHiddenSize) # weights output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a neural network $n$ and an input vector $x$ for this neural network, the function $n.\\texttt{feedforward}(x)$ compute the output of the neural network.\n",
    "The code is a straightforward implementation of the feedforward equations.  \n",
    "These equations are repeated here for convenience:\n",
    "- $\\mathbf{a}^{(1)}(\\mathbf{x}) = \\mathbf{x}$ \n",
    "- $\\mathbf{a}^{(l)}(\\mathbf{x}) = S\\Bigl( W^{(l)} \\cdot \\mathbf{a}^{(l-1)}(\\mathbf{x}) + \\mathbf{b}^{(l)}\\Bigr)$\n",
    "  for all $l \\in \\{2, 3\\}$.\n",
    "\n",
    "The input `x` is the activation of the input layer and therefore is equal to $\\mathbf{a}^{(1)}(\\mathbf{x})$.\n",
    "`AH` is the activation of the hidden layer and hence equal to $\\mathbf{a}^{(2)}(\\mathbf{x})$, while \n",
    "`AO` is the activation of the output layer and therefore equal to $\\mathbf{a}^{(3)}(\\mathbf{x})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedforward(self, x):\n",
    "    AH = sigmoid(self.mWeightsH @ x  + self.mBiasesH) # hidden layer\n",
    "    AO = sigmoid(self.mWeightsO @ AH + self.mBiasesO) # output layer\n",
    "    return AO\n",
    "\n",
    "Network.feedforward = feedforward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a neural network $n$, the method $\\texttt{sgd}(\\texttt{training_data}, \\texttt{epochs}, \\texttt{mbs}, \\texttt{eta}, \\texttt{test_data})$ uses stochastic gradient descent to train the network.  The parameters are as follows:\n",
    "<ul>\n",
    "<li> $\\texttt{training_data}$ is a list of tuples of the form $(x, y)$ where $x$ is an \n",
    "     input of the neural net and $y$ is a vector of length 10 representing the desired output. </li>\n",
    "<li> $\\texttt{epochs}$ is the number of epochs to train,</li>\n",
    "<li> $\\texttt{mbs}$ is the size of the minibatches,</li>\n",
    "<li> $\\texttt{eta}$ is the learning rate</li>\n",
    "<li> $\\texttt{test_data}$ is a list of tuples of the form $(x, y)$ where $x$ is an \n",
    "     input and $y$ is the desired output digit. \n",
    "</ul>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(self, training_data, epochs, mbs, eta, test_data):\n",
    "    n_test = len(test_data)\n",
    "    n      = len(training_data)\n",
    "    for j in range(epochs):\n",
    "        random.shuffle(training_data)\n",
    "        mini_batches = [training_data[k : k+mbs] for k in range(0, n, mbs)]\n",
    "        for mini_batch in mini_batches:\n",
    "            self.update_mini_batch(mini_batch, eta)    \n",
    "        print('Epoch %2d: %d / %d' % (j, self.evaluate(test_data), n_test))\n",
    "        \n",
    "Network.sgd = sgd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method `update_mini_batch` performs one step of gradient descent for the data from one \n",
    "mini-batch.  It receives two arguments.\n",
    "- `mini_batch` is the list of training data that constitute one mini-batch.\n",
    "- `eta` is the <em style=\"color:blue\">learning rate</em>.\n",
    "\n",
    "The implementation of `update_mini_batch` works as follows:\n",
    "- First, we initialize the vectors `nabla_BH`, `nabla_BO` and the matrices\n",
    "  `nabla_WH`, `nabla_WO` to contain only zeros.\n",
    "  - `nabla_BH` will store the gradient of the bias vector of the hidden layer.\n",
    "  - `nabla_BO` will store the gradient of the bias vector of the output layer.\n",
    "  - `nabla_WH` will store the gradient of the weight matrix of the hidden layer.\n",
    "  - `nabla_WO` will store the gradient of the weight matrix of the output layer.\n",
    "- Next, we iterate of all training examples in the mini-batch and for every training \n",
    "  example `x, y` we compute the contribution of this training example to the gradients of \n",
    "  the cost function $C$, i.e. we compute\n",
    "  $$ \\nabla_{\\mathbf{b}^{(l)}} C_{\\mathbf{x}, \\mathbf{y}} \\quad \\mbox{and} \\quad \n",
    "     \\nabla_{W^{(l)}} C_{\\mathbf{x}, \\mathbf{y}}\n",
    "  $$ \n",
    "  for the hidden layer and the output layer.  These gradients are computed by the function\n",
    "  `backprop`.\n",
    "- Finally, the bias vectors and the weight matrices are updated according to the learning \n",
    "  rate and the computed gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_mini_batch(self, mini_batch, eta):\n",
    "    nabla_BH = np.zeros((self.mHiddenSize, 1))  # gradient of biases  of hidden layer\n",
    "    nabla_BO = np.zeros((self.mOutputSize, 1))  # gradient of biases  of output layer\n",
    "    nabla_WH = np.zeros((self.mHiddenSize, self.mInputSize))  # gradient of weights of hidden layer\n",
    "    nabla_WO = np.zeros((self.mOutputSize, self.mHiddenSize)) # gradient of weights of output layer\n",
    "    for x, y in mini_batch:\n",
    "        dltNbl_BH, dltNbl_BO, dltNbl_WH, dltNbl_WO = self.backprop(x, y)\n",
    "        nabla_BH += dltNbl_BH\n",
    "        nabla_BO += dltNbl_BO\n",
    "        nabla_WH += dltNbl_WH\n",
    "        nabla_WO += dltNbl_WO      \n",
    "    alpha = eta / len(mini_batch)\n",
    "    self.mBiasesH  -= alpha * nabla_BH\n",
    "    self.mBiasesO  -= alpha * nabla_BO\n",
    "    self.mWeightsH -= alpha * nabla_WH\n",
    "    self.mWeightsO -= alpha * nabla_WO\n",
    "\n",
    "Network.update_mini_batch = update_mini_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a neural network $n$, the method $n.\\texttt{backprop}(x, y)$ takes a training example $(x,y)$ and calculates the gradient of the cost function with respect to this training example.  This is done by implementing the \n",
    "<em style=\"color:blue\">backpropagation equations</em> shown below:\n",
    "\n",
    "$$\n",
    "\\begin{array}[h]{llr}\n",
    "  \\boldsymbol{\\varepsilon}^{(L)} = (\\mathbf{a}^{(L)} - \\mathbf{y}) \\odot S'\\bigl(\\mathbf{z}^{(L)}\\bigr)\n",
    "     & & \\mbox{(BP1v)}  \\\\\n",
    "  \\boldsymbol{\\varepsilon}^{(l)} = \\Bigl(\\bigl(W^{(l+1)}\\bigr)^\\top \\cdot \\boldsymbol{\\varepsilon}^{(l+1)}\\Bigr) \\odot\n",
    "  S'\\bigl(\\mathbf{z}^{(l)}\\bigr) & \\mbox{for all $l \\in \\{2, \\cdots, L-1\\}$} &\n",
    "  \\mbox{(BP2v)}  \\\\\n",
    "  \\nabla_{\\mathbf{b}^{(l)}} C_{\\mathbf{x}, \\mathbf{y}} = \\boldsymbol{\\varepsilon}^{(l)}\n",
    "  & \\mbox{for all $l \\in \\{2, \\cdots,L\\}$}\n",
    "  & \\mbox{(BP3v)}\n",
    "  \\\\\n",
    "  \\nabla_{W^{(l)}} C_{\\mathbf{x}, \\mathbf{y}} = \\boldsymbol{\\varepsilon}^{(l)} \\cdot \\bigl(\\mathbf{a}^{(l-1)}\\bigr)^\\top\n",
    "  & \\mbox{for all $l \\in \\{2, \\cdots,L\\}$}\n",
    "  & \\mbox{(BP4v)}\n",
    "\\end{array}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(self, x, y):\n",
    "    # feedforward pass\n",
    "    ZH = self.mWeightsH @ x  + self.mBiasesH\n",
    "    AH = sigmoid(ZH)\n",
    "    ZO = self.mWeightsO @ AH + self.mBiasesO\n",
    "    AO = sigmoid(ZO)\n",
    "    # backwards pass, output layer\n",
    "    epsilonO = (AO - y) * sigmoid_prime(ZO)\n",
    "    nabla_BO = epsilonO\n",
    "    nabla_WO = epsilonO @ AH.transpose()\n",
    "    # backwards pass, hidden layer\n",
    "    epsilonH = (self.mWeightsO.transpose() @ epsilonO) * sigmoid_prime(ZH)\n",
    "    nabla_BH = epsilonH\n",
    "    nabla_WH = epsilonH @ x.transpose()\n",
    "    return nabla_BH, nabla_BO, nabla_WH, nabla_WO\n",
    "\n",
    "Network.backprop = backprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a neural network $n$, the method $n.\\texttt{evaluate}(\\texttt{test_data})$ uses the test data to compute  the number of examples that are predicted correctly by the neural network $N$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(self, test_data):\n",
    "    test_results = \\\n",
    "        [(np.argmax(self.feedforward(x)), y) for x, y in test_data]\n",
    "    return sum(y1 == y2 for y1, y2 in test_results)\n",
    "\n",
    "Network.evaluate = evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "np.random.seed(1)\n",
    "net = Network(40)\n",
    "net.sgd(training_data, 60, 10, 0.1, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the number of parameters of our network? \n",
    "- The hidden layer has 40 neurons that each have a bias parameter and 784 \n",
    "  weight parameters for the connections to the input nodes.\n",
    "- The output layer has 10 neurons that each have a bias parameter and 40 \n",
    "  weight parameters for the connections to the hidden layer.\n",
    "  \n",
    "Therefore the network has \n",
    "$$ 40 \\cdot (1 + 784) + 10 \\cdot (1 + 40) = 31,810 $$\n",
    "parameters.  As we have $50,000$ training data and every training datum gives rise to 10 equations, we shouldn't be too worried about over-fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "40 * (1 + 784) + 10 * (1 + 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
