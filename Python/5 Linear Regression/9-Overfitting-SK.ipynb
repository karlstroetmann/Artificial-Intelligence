{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(open(\"../style.css\", \"r\").read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting in Linear Regression (with Scikit-Learn)\n",
    "\n",
    "In this notebook, we demonstrate **overfitting** using the **Hitters** dataset (baseball statistics).\n",
    "\n",
    "We will incrementally add features to our model, starting with the most \"important\" ones, to see how the model's performance changes on the **Training Set** versus the **Test Set**.\n",
    "\n",
    "We will use the **Scikit-Learn** (`sklearn`) library, which is the industry standard for machine learning in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to be able to read the data from the web we import the module `request` from`urllib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Loading and Preprocessing\n",
    "\n",
    "The data is available at https://raw.githubusercontent.com/vincentarelbundock/Rdatasets/master/csv/ISLR/Hitters.csv.\n",
    "We download it via the function `urlretrieve`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/vincentarelbundock/Rdatasets/master/csv/ISLR/Hitters.csv\"\n",
    "urllib.request.urlretrieve(url, \"Hitters.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is now stored locally in the file `'Hitters.csv'`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat Hitters.csv || type Hitters.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We put the data into a *data frame*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Hitters.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we drop the player names as they are useless for our statistical investigation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=[\"rownames\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our aim is to predict the players *salary* from the other attributes, we have to drop rows where the target `Salary` is missing, i.e. has the value `NaN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['Salary'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that out of 322 rows only 263 have survived."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot Encoding with `pd.get_dummies`\n",
    "\n",
    "One-hot encoding is a technique used to convert categorical data (variables that contain label values rather than numbers) into a numerical format that machine learning algorithms can understand.\n",
    "\n",
    "In the context of the Python library **pandas**, the function `pd.get_dummies` is the standard tool for performing this transformation. It takes a column of categorical data and expands it into multiple new columnsâ€”one for each unique category found in the original column.\n",
    "\n",
    "### How It Works\n",
    "\n",
    "1. **Identify Unique Categories:** The function scans the specified column to find all unique values (e.g., \"Red\", \"Blue\", \"Green\").\n",
    "2. **Create New Columns:** It creates a new binary column for *each* unique category.\n",
    "3. **Assign Binary Values:**\n",
    "* It places a **1** (or `True`) in the column corresponding to the observation's category.\n",
    "* It places a **0** (or `False`) in all other category columns for that row.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Example: Baseball Divisions\n",
    "\n",
    "Let's look at your specific example regarding the `Division` attribute for baseball players.\n",
    "\n",
    "#### 1. The Original Data\n",
    "\n",
    "Imagine you have a DataFrame of baseball players. One of the columns is `Division`, and it contains three possible values representing the division the team plays in:\n",
    "\n",
    "* **W** (Western)\n",
    "* **C** (Central)\n",
    "* **E** (Eastern)\n",
    "\n",
    "Here is what the raw data looks like:\n",
    "\n",
    "| Player | Division |\n",
    "| --- | --- |\n",
    "| Player A | W |\n",
    "| Player B | C |\n",
    "| Player C | E |\n",
    "| Player D | W |\n",
    "\n",
    "#### 2. Applying `pd.get_dummies`\n",
    "\n",
    "When you run `pd.get_dummies()` on the `Division` column, pandas creates three new columns, usually prefixed with the original column name.\n",
    "\n",
    "The resulting table looks like this:\n",
    "\n",
    "| Player | Division_W | Division_C | Division_E |\n",
    "| --- | --- | --- | --- |\n",
    "| Player A | **1** | 0 | 0 |\n",
    "| Player B | 0 | **1** | 0 |\n",
    "| Player C | 0 | 0 | **1** |\n",
    "| Player D | **1** | 0 | 0 |\n",
    "\n",
    "#### 3. Interpretation\n",
    "\n",
    "* **Player A** was in the Western division, so `Division_W` is 1, while `Division_C` and `Division_E` are 0.\n",
    "* **Player B** was in the Central division, so `Division_C` is 1.\n",
    "* **Player C** was in the Eastern division, so `Division_E` is 1.\n",
    "\n",
    "The algorithm can now treat these columns as independent numerical features rather than a single text string.\n",
    "\n",
    "---\n",
    "\n",
    "### Important Note: The Dummy Variable Trap\n",
    "\n",
    "In many statistical models (like linear regression), including all three columns creates a problem called **multicollinearity** (or the \"Dummy Variable Trap\"). This happens because the variables are perfectly correlated: if you know a player is *not* in the West and *not* in the Central, you automatically know they *must* be in the East.\n",
    "\n",
    "To solve this, we often drop one column (usually the first one) to serve as the \"baseline\" or \"reference\" category. Pandas supports this via the `drop_first=True` parameter.\n",
    "\n",
    "If we used `drop_first=True` on the example above, the result would look like this:\n",
    "\n",
    "| Player | Division_W | Division_E |\n",
    "| --- | --- | --- |\n",
    "| Player A | 1 | 0 |\n",
    "| Player B | 0 | 0 |\n",
    "| Player C | 0 | 1 |\n",
    "\n",
    "* **Player B (Central)** is now represented by 0s in both columns. The model infers that if it's not West and not East, it is Central.\n",
    "\n",
    "Now the dataset we are inspecting only has two attributes for the attribute `Division`: `W` and `E`.  There is no player from the Central Division.\n",
    "Hence in our case, the attribute `Division` is replaced with just one new binary attribute `Divion_W`.  If this attribute is `True`, the player is\n",
    "in the `West` divison, else he is in the `East` division."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, drop_first=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Feature Sorting by Importance\n",
    "\n",
    "Before we train, we want to determine which features are the most important.\n",
    "A simple heuristic for \"importance\" in linear regression is the **correlation** between a feature and the target variable.\n",
    "Therefore, we \n",
    "  * calculate the correlation matrix and then\n",
    "  * sort the features based on the absolute value of their correlation with `Salary`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = df.corr()['Salary'].abs().sort_values(ascending=False)\n",
    "correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `Salary` is the target variable, we drop the feature `Salary` from this list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_features = correlations.drop('Salary').index.tolist()\n",
    "sorted_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Features sorted by importance (Correlation with Salary):\")\n",
    "for i, f in enumerate(sorted_features):\n",
    "    print(f\"{i+1}. {f} ({correlations[f]:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Splitting the Data\n",
    "\n",
    "We use `train_test_split` from `sklearn.model_selection`.\n",
    "\n",
    "**Explanation of the function:**\n",
    "- `train_test_split(X, y, test_size=..., random_state=...)`: This function randomly shuffles the data and splits it into two buckets.\n",
    "- `train_size=50`: We only take 50 samples for training to intentionally make it easier to overfit the model for this demonstration.\n",
    "- `random_state=42`: This is used to seed the random number generator and ensures that the train/test split is reproducible \n",
    "   (we get the same random split every time we run the code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[sorted_features]  # Features ordered by importance\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['Salary']         # Target\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we split the data. We keep only 50 samples for training to simulate a 'low data' scenario where overfitting is \n",
    "easy to observe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=50, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training samples: {X_train.shape[0]}\")\n",
    "print(f\"Test samples:     {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. The Experiment\n",
    "\n",
    "We will now loop through our sorted features. In each iteration $k$, we utilize the top $k$ features to train a linear regression model.\n",
    "\n",
    "**Explanation of Scikit-Learn functions used:**\n",
    "1.  `LinearRegression()`: Creates an instance of the model. It is mathematically equivalent to solving the Normal Equation.\n",
    "2.  `.fit(X, y)`: This function trains the model, i.e. it solves the *normal equation* and thereby \n",
    "    finds the optimal weights that minimize the *mean squared error* on the given data `X` and `y`.\n",
    "3.  `.score(X, y)`: This evaluates the model. For regression, it returns the $R^2$ score (Coefficient of Determination). $1.0$ is perfect, $0.0$ is equivalent to guessing the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores = []\n",
    "test_scores  = []\n",
    "num_features = list(range(1, 19+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We iterate over all 19 features.  The $k^{\\mathrm{th}}$ iteration uses the $k$ most important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(1, len(sorted_features) + 1):\n",
    "    # Select the top k features\n",
    "    top_k_features = sorted_features[:k]\n",
    "    X_train_k = X_train[top_k_features]\n",
    "    X_test_k  = X_test[top_k_features]\n",
    "    # 1. Create the model\n",
    "    model = LinearRegression()\n",
    "    # 2. Train the model with the training data\n",
    "    model.fit(X_train_k, y_train)\n",
    "    # 3. Evaluate the model (Score)\n",
    "    # We first record one score on the training data.\n",
    "    r2_train = model.score(X_train_k, y_train)\n",
    "    train_scores.append(r2_train)\n",
    "    # Next, we record the accuracy on both the test data.\n",
    "    # Note that the model hasn't seen the test data during training.\n",
    "    r2_test = model.score(X_test_k, y_test)   \n",
    "    test_scores.append(r2_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Visualization\n",
    "\n",
    "We plot the training and test scores. \n",
    "\n",
    "**What to look for:**\n",
    "- The **Training Score** (Blue) should generally go up. Adding information allows the model to explain the specific training data better.\n",
    "- The **Test Score** (Red) will eventually peak and then drop. This drop indicates **overfitting**: the model is using the additional (less important) features to memorize noise in the training set, which hurts its ability to predict real salaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(num_features, train_scores, 'o-', color='blue', label='Training Score ($R^2$)')\n",
    "plt.plot(num_features, test_scores,  'o-', color='red' , label='Test Score ($R^2$)')\n",
    "\n",
    "plt.title('Overfitting Analysis: Baseball Salaries', fontsize=16)\n",
    "plt.xlabel('Number of Features (Sorted by Importance)', fontsize=12)\n",
    "plt.ylabel('$R^2$ Score', fontsize=12)\n",
    "plt.xticks(num_features)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend(fontsize=12)\n",
    "\n",
    "# Annotate the \"Sweet Spot\"\n",
    "best_k = np.argmax(test_scores) + 1\n",
    "plt.axvline(x=best_k, color='green', linestyle='--', alpha=0.7)\n",
    "plt.text(best_k + 0.5, 0.4, 'Sweet Spot', color='green', fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
