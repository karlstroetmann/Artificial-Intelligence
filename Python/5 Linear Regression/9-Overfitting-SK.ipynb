{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(open(\"../style.css\", \"r\").read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting in Linear Regression (with Scikit-Learn)\n",
    "\n",
    "In this notebook, we demonstrate **overfitting** using the **Hitters** dataset (baseball statistics).\n",
    "\n",
    "We will incrementally add features to our model, starting with the most \"important\" ones, to see how the model's performance changes on the **Training Set** versus the **Test Set**.\n",
    "\n",
    "We will use the **Scikit-Learn** (`sklearn`) library, which is the industry standard for machine learning in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Loading and Preprocessing\n",
    "\n",
    "We use the `pandas` library to load the data. `pandas` is excellent for handling tabular data (DataFrames)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the file\n",
    "url = \"https://raw.githubusercontent.com/vincentarelbundock/Rdatasets/master/csv/ISLR/Hitters.csv\"\n",
    "urllib.request.urlretrieve(url, \"Hitters.csv\")\n",
    "\n",
    "# Load into a DataFrame\n",
    "df = pd.read_csv(\"Hitters.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the name column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=[\"rownames\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop rows where the target `Salary` is missing, i.e. has the value `NaN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['Salary'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-Hot Encoding\n",
    "Machine Learning models generally require numerical input. Our dataset contains categorical text data (e.g., `League` is 'A' or 'N'). We use `pd.get_dummies` to convert these into numbers (0 or 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, drop_first=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Feature Sorting by Importance\n",
    "\n",
    "Before we train, we want to determine which features are the most important.\n",
    "A simple heuristic for \"importance\" in linear regression is the **correlation** between a feature and the target variable.\n",
    "\n",
    "- We calculate the correlation matrix.\n",
    "- We sort the features based on the absolute value of their correlation with `Salary`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the correlation Matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = df.corr()['Salary'].abs().sort_values(ascending=False)\n",
    "correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the feature `Salary` from the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_features = correlations.drop('Salary').index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Features sorted by importance (Correlation with Salary):\")\n",
    "for i, f in enumerate(sorted_features):\n",
    "    print(f\"{i+1}. {f} ({correlations[f]:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Splitting the Data\n",
    "\n",
    "We use `train_test_split` from `sklearn.model_selection`.\n",
    "\n",
    "**Explanation of the function:**\n",
    "- `train_test_split(X, y, test_size=..., random_state=...)`: This function randomly shuffles the data and splits it into two buckets.\n",
    "- `test_size=0.5`: We set a very large test set (and consequently a **small training set**) to intentionally make it easier to overfit the model for this demonstration.\n",
    "- `random_state=42`: Ensures the split is reproducible (we get the same random split every time we run the code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df[sorted_features]  # Features ordered by importance\n",
    "y = df['Salary']         # Target\n",
    "\n",
    "# Split the data\n",
    "# We keep only 50 samples for training to simulate a 'low data' scenario where overfitting is common\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=50, random_state=42)\n",
    "\n",
    "print(f\"Training samples: {X_train.shape[0]}\")\n",
    "print(f\"Test samples:     {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. The Experiment\n",
    "\n",
    "We will now loop through our sorted features. In each iteration $k$, we utilize the top $k$ features to train a linear regression model.\n",
    "\n",
    "**Explanation of Scikit-Learn functions used:**\n",
    "1.  `LinearRegression()`: Creates an instance of the model. It is mathematically equivalent to solving the Normal Equation.\n",
    "2.  `.fit(X, y)`: This trains the model. It finds the optimal weights $\\beta$ that minimize the error on the given data `X` and `y`.\n",
    "3.  `.score(X, y)`: This evaluates the model. For regression, it returns the $R^2$ score (Coefficient of Determination). $1.0$ is perfect, $0.0$ is equivalent to guessing the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "num_features = []\n",
    "\n",
    "# Iterate from using 1 feature to using all 19 features\n",
    "for k in range(1, len(sorted_features) + 1):\n",
    "    # Select the top k features\n",
    "    top_k_features = sorted_features[:k]\n",
    "    \n",
    "    X_train_k = X_train[top_k_features]\n",
    "    X_test_k  = X_test[top_k_features]\n",
    "    \n",
    "    # 1. Create the model\n",
    "    model = LinearRegression()\n",
    "    \n",
    "    # 2. Train the model (Fit)\n",
    "    model.fit(X_train_k, y_train)\n",
    "    \n",
    "    # 3. Evaluate the model (Score)\n",
    "    # We record accuracy on both the data it studied (Train) and the data it hasn't seen (Test)\n",
    "    r2_train = model.score(X_train_k, y_train)\n",
    "    r2_test  = model.score(X_test_k, y_test)\n",
    "    \n",
    "    train_scores.append(r2_train)\n",
    "    test_scores.append(r2_test)\n",
    "    num_features.append(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Visualization\n",
    "\n",
    "We plot the training and test scores. \n",
    "\n",
    "**What to look for:**\n",
    "- The **Training Score** (Blue) should generally go up. Adding information allows the model to explain the specific training data better.\n",
    "- The **Test Score** (Red) will eventually peak and then drop. This drop indicates **overfitting**: the model is using the additional (less important) features to memorize noise in the training set, which hurts its ability to predict real salaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(num_features, train_scores, 'o-', color='blue', label='Training Score ($R^2$)')\n",
    "plt.plot(num_features, test_scores, 'o-', color='red', label='Test Score ($R^2$)')\n",
    "\n",
    "plt.title('Overfitting Analysis: Baseball Salaries', fontsize=16)\n",
    "plt.xlabel('Number of Features (Sorted by Importance)', fontsize=12)\n",
    "plt.ylabel('$R^2$ Score', fontsize=12)\n",
    "plt.xticks(num_features)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend(fontsize=12)\n",
    "\n",
    "# Annotate the \"Sweet Spot\"\n",
    "best_k = np.argmax(test_scores) + 1\n",
    "plt.axvline(x=best_k, color='green', linestyle='--', alpha=0.7)\n",
    "plt.text(best_k + 0.5, 0.4, 'Sweet Spot', color='green', fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
