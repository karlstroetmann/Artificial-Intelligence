{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "with open (\"../style.css\", \"r\") as file:\n",
    "    css = file.read()\n",
    "HTML(css)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial Regression and Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy                as np\n",
    "import sklearn.linear_model as lm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we want to discuss both <em style=\"color:blue;\">polynomial regression</em> and <em style=\"color:blue;\">overfitting</em>.\n",
    "One possible reason causing overfitting is a correlation between features.  Let us create a dataset with two feature `x1` and `x2` that are more or less the same. Actually, `x2` is `x1` plus some random noise.  The dependent variable `y` is the square root of `x1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "N  = 20                     # number of data points\n",
    "X1 = np.array([k for k in range(N)])\n",
    "X2 = np.array([k + 0.2 * (np.random.rand() - 0.5) for k in range(N)])\n",
    "Y  = np.sqrt(X1)            # Y is the square root of X1\n",
    "X1 = np.reshape(X1, (N, 1)) # turn X1 into an N x 1 matrix\n",
    "X2 = np.reshape(X2, (N, 1)) # turn X2 into an N x 1 matrix\n",
    "X  = np.hstack([X1, X2])    # combine X1 and X2 into an N x 2 matrix\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot the data.  We will use colors to distinguish between `x1` and `x2`.  The pairs $(x_1, y)$ are plotted in blue, while the pairs  $(x_2, y)$ are plotted in yellow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn           as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 12))\n",
    "sns.set(style='whitegrid')\n",
    "plt.title('A Regression Problem')\n",
    "plt.axvline(x=0.0, c='k')\n",
    "plt.axhline(y=0.0, c='k')\n",
    "plt.xlabel('x axis')\n",
    "plt.ylabel('y axis')\n",
    "plt.xticks(np.arange(0.0, N, step=1.0))\n",
    "plt.yticks(np.arange(0.0, np.sqrt(N) + 1, step=0.25))\n",
    "plt.scatter(X1, Y, color='b') \n",
    "plt.scatter(X2, Y, color='y') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to split the data into a <em style=\"color:blue;\">training set</em> and a <em style=\"color:blue;\">test set</em>.\n",
    "The <em style=\"color:blue;\">training set</em> will be used to compute the parameters of our model, while the\n",
    "<em style=\"color:blue;\">testing set</em> is only used to check the <em style=\"color:blue;\">accuracy</em>.  SciKit-Learn has a predefined method\n",
    "`sklearn.model_selection.train_test_split` that can be used to randomly split data into a training set and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will split the data at a ratio of $4:1$, i.e. $80\\%$ of the data will be used for training, while the remaining $20\\%$ is used to test the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=1)\n",
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to build a <em style=\"color:blue;\">linear regression</em> model, we import the module `linear_model` from SciKit-Learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function $\\texttt{linear_regression}(\\texttt{X_train}, \\texttt{Y_train}, \\texttt{X_test}, \\texttt{Y_test})$ takes a feature matrix $\\texttt{X_train}$ and a corresponding vector $\\texttt{Y_train}$ and computes a linear regression model $M$ that best fits these data.  Then, the explained variance of the model is computed both for the training set and for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(X_train, Y_train, X_test, Y_test):\n",
    "    M = lm.LinearRegression()\n",
    "    M.fit(X_train, Y_train)\n",
    "    train_score = M.score(X_train, Y_train)\n",
    "    test_score  = M.score(X_test , Y_test)\n",
    "    return M, train_score, test_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the function `linear_regression` to build a model for our data.  Note that this function uses only the <em style=\"color:blue;\">training data</em> to create the model.  The <em style=\"color:blue;\">test data</em> is only used for evaluating the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M, train_score, test_score = linear_regression(X_train, Y_train, X_test, Y_test)\n",
    "train_score, test_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the explained variance is a lot worse on the test set. Let's plot the linear model. The coefficients are stored in `M.intercept_` and `M.coef_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ϑ0     = M.intercept_\n",
    "ϑ1, ϑ2 = M.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "sns.set(style='whitegrid')\n",
    "plt.title('A Regression Problem')\n",
    "plt.axvline(x=0.0, c='k')\n",
    "plt.axhline(y=0.0, c='k')\n",
    "plt.xlabel('x axis')\n",
    "plt.ylabel('y axis')\n",
    "plt.xticks(np.arange(0.0, N + 1, step=1.0))\n",
    "plt.yticks(np.arange(0.0, np.sqrt(N) + 1, step=0.25))\n",
    "plt.scatter(X_train[:,0], Y_train, color='b') \n",
    "plt.scatter(X_test [:,0], Y_test , color='g') \n",
    "plt.plot([0, N], [ϑ0, ϑ0 + (ϑ1 + ϑ2) * N], c='r')\n",
    "#plt.savefig('sqrt-linear.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to improve the explained variance of our model, we extend it with polynomial features, i.e. we add features like $x_1^2$ and $x_1\\cdot x_2$ etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quadratic = PolynomialFeatures(2, include_bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_quadratic = quadratic.fit_transform(X_train)\n",
    "X_test_quadratic  = quadratic.fit_transform(X_test)\n",
    "quadratic.get_feature_names(['x1', 'x2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_quadratic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us fit this quadratic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M, train_score, test_score = linear_regression(X_train_quadratic, Y_train, X_test_quadratic, Y_test)\n",
    "train_score, test_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy on the training set and on the test set have both increased.  Let us plot the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ϑ0                 = M.intercept_\n",
    "ϑ1, ϑ2, ϑ3, ϑ4, ϑ5 = M.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the regression curve starts to get tedious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(0.0, N+1, 0.01)\n",
    "b = ϑ0 + (ϑ1 + ϑ2 ) * a + (ϑ3 + ϑ4 + ϑ5) * a**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 8))\n",
    "sns.set(style='darkgrid')\n",
    "plt.title('A Regression Problem: Second Order Terms included')\n",
    "plt.axvline(x=0.0, c='k')\n",
    "plt.axhline(y=0.0, c='k')\n",
    "plt.xlabel('x axis')\n",
    "plt.ylabel('y axis')\n",
    "plt.xticks(np.arange(0.0, N + 1, step=1.0))\n",
    "plt.yticks(np.arange(0.0, np.sqrt(N) + 1, step=0.25))\n",
    "plt.scatter(X_train[:,0], Y_train, color='b') \n",
    "plt.scatter(X_test [:,0], Y_test , color='g') \n",
    "plt.plot(a, b, c='r')\n",
    "#plt.savefig('sqrt-quadratic.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, the quadratic curve is a much better match than the linear model. Let's try to use higher order polynomials. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\texttt{polynomial}(n)$ creates a polynomial in the variables `a` and `b` that contains all terms of the form \n",
    "that contains all terms of the form $\\Theta[k] \\cdot a^i \\cdot b^j$ where $i+j \\leq n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial(n):\n",
    "    sum = '   Θ[0]' \n",
    "    cnt = 0\n",
    "    for k in range(1, n+1):\n",
    "        for i in range(0, k+1):\n",
    "            cnt += 1\n",
    "            sum += f' + Θ[{cnt}] * a**{k-i} * b**{i}'\n",
    "        if k < n:\n",
    "            sum += '\\\\\\n'\n",
    "    return sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check this out for $n=4$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(polynomial(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function $\\texttt{polynomial_vector}(n, M)$ takes a number $n$ and a model $M$.  It returns a pair of vectors that can be used to plot the nonlinear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_vector(n, M):\n",
    "    Θ = [M.intercept_] + list(M.coef_)\n",
    "    a = np.reshape(X1, (N, ))\n",
    "    b = np.reshape(X2, (N, ))\n",
    "    return 0.5*(a + b), eval(polynomial(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function $\\texttt{plot_nth_degree_boundary}(n)$ creates a polynomial regression model of degree $n$.  It plots both the data and the polynomial model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_nth_degree_polynomial(n):\n",
    "    poly         = PolynomialFeatures(n, include_bias=False)\n",
    "    X_train_poly = poly.fit_transform(X_train)\n",
    "    X_test_poly  = poly.fit_transform(X_test)\n",
    "    M, s1, s2    = linear_regression(X_train_poly, Y_train, X_test_poly, Y_test)\n",
    "    print('The explained variance on the training set is:', s1)\n",
    "    print('The explained variance on the test     set is:', s2)\n",
    "    a, b = polynomial_vector(n, M)\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    sns.set(style='darkgrid')\n",
    "    plt.title('A Regression Problem')\n",
    "    plt.axvline(x=0.0, c='k')\n",
    "    plt.axhline(y=0.0, c='k')\n",
    "    plt.xlabel('x axis')\n",
    "    plt.ylabel('y axis')\n",
    "    plt.xticks(np.arange(0.0, N + 1, step=1.0))\n",
    "    plt.yticks(np.arange(0.0, 2*np.sqrt(N), step=0.25))\n",
    "    plt.scatter(X_train[:,0], Y_train, color='b') \n",
    "    plt.scatter(X_test [:,0], Y_test , color='g') \n",
    "    plt.plot(a, b, c='r')\n",
    "    #plt.savefig('sqrt-' + str(n) + '.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us test this for the polynomial logistic regression model of degree $4$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_nth_degree_polynomial(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to be working and the explained variance has improved. Let's try to use even higher order polynomials.  Hopefully, we can get a $100\\%$ explained variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_nth_degree_polynomial(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that we can get $100\\%$ of explained variance, but only for the training set.  The explained variance of the test set has decreased and apparently the curve is starting to get wiggly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(X_train, Y_train, X_test, Y_test, alpha):\n",
    "    M = lm.Ridge(alpha, solver='svd')\n",
    "    M.fit(X_train, Y_train)\n",
    "    train_score = M.score(X_train, Y_train)\n",
    "    test_score  = M.score(X_test , Y_test)\n",
    "    return M, train_score, test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_nth_degree_polynomial_ridge(n, alpha):\n",
    "    poly         = PolynomialFeatures(n, include_bias=False)\n",
    "    X_train_poly = poly.fit_transform(X_train)\n",
    "    X_test_poly  = poly.fit_transform(X_test)\n",
    "    M, s1, s2    = ridge_regression(X_train_poly, Y_train, X_test_poly, Y_test, alpha)\n",
    "    print('The explained variance on the training set is:', s1)\n",
    "    print('The explained variance on the test     set is:', s2)\n",
    "    a, b = polynomial_vector(n, M)\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    sns.set(style='darkgrid')\n",
    "    plt.title('A Regression Problem')\n",
    "    plt.axvline(x=0.0, c='k')\n",
    "    plt.axhline(y=0.0, c='k')\n",
    "    plt.xlabel('x axis')\n",
    "    plt.ylabel('y axis')\n",
    "    plt.xticks(np.arange(0.0, N + 1, step=1.0))\n",
    "    plt.yticks(np.arange(0.0, 2*np.sqrt(N), step=0.25))\n",
    "    plt.scatter(X_train[:,0], Y_train, color='b') \n",
    "    plt.scatter(X_test [:,0], Y_test , color='g') \n",
    "    plt.plot(a, b, c='r')\n",
    "    #plt.savefig('sqrt-' + str(n) + 'ridge.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to use a polynomial of degree 6 but without regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_nth_degree_polynomial_ridge(6, 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks like the model that we had found before.  Let us try to add a bit of regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_nth_degree_polynomial_ridge(6, 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the model is much smoother and the explained variance has also increased considerably on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_nth_degree_polynomial_ridge(6, 100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we increase the regularization parameter too much, the model is no longer able to fit the data and the explained variance decreases."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
