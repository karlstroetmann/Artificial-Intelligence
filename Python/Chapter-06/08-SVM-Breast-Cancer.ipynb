{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(open(\"../style.css\", \"r\").read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM on Medical Data: Breast Cancer Diagnosis\n",
    "\n",
    "In this notebook, we apply our \"First Principles\" Support Vector Machine to the **Breast Cancer Wisconsin (Diagnostic) Dataset**. This is a classic binary classification dataset used to predict whether a breast mass is **Malignant** (harmful) or **Benign** (non-harmful) based on characteristics of cell nuclei present in a digitized image of a fine needle aspirate (FNA).\n",
    "\n",
    "---\n",
    "\n",
    "## The Challenge: Feature Scaling\n",
    "\n",
    "Unlike the Iris dataset, this real-world data presents a common challenge for SVMs: **Vastly different scales**.\n",
    "\n",
    "* **Mean Area** might range from 200 to 2500.\n",
    "* **Mean Smoothness** might range from 0.05 to 0.16.\n",
    "\n",
    "The Gaussian Kernel depends on the Euclidean distance $||\\mathbf{x} - \\mathbf{y}||^2$. If we don't normalize the data, the \"Area\" feature (with differences in the hundreds) will completely drown out the \"Smoothness\" feature (with differences in the hundredths). The SVM would essentially ignore the small features.\n",
    "\n",
    "Therefore, we will implement **Standardization** (Z-score normalization) from scratch before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "import scipy.optimize as scp\n",
    "\n",
    "# Set visual style\n",
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. SVM Class Implementation\n",
    "\n",
    "We use our established implementation with the Gaussian Kernel and Soft Margin support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kernel(x1, x2, sigma=1.0):\n",
    "    dist = np.linalg.norm(x1 - x2) ** 2\n",
    "    return np.exp(-dist / (2 * (sigma ** 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM:\n",
    "    def __init__(self, kernel=gaussian_kernel, C=1.0):\n",
    "        self.kernel = kernel\n",
    "        self.C = C\n",
    "        self.alpha = None\n",
    "        self.support_vectors = None\n",
    "        self.support_vector_labels = None\n",
    "        self.b = 0\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        K = np.zeros((n_samples, n_samples))\n",
    "        for i in range(n_samples):\n",
    "            for j in range(n_samples):\n",
    "                K[i, j] = self.kernel(X[i], X[j])\n",
    "        def objective(alpha):\n",
    "            return 0.5 * np.sum((np.outer(alpha, alpha) * \\\n",
    "                   np.outer(y, y)) * K) - np.sum(alpha)\n",
    "        constraints = ({'type': 'eq', 'fun': lambda alpha: np.dot(alpha, y)})\n",
    "        bounds      = [(0, self.C) for _ in range(n_samples)]\n",
    "        initial_alpha = np.zeros(n_samples)\n",
    "        result = scp.minimize(objective, initial_alpha, method='SLSQP', \n",
    "                              bounds=bounds, constraints=constraints)\n",
    "        self.alpha = result.x\n",
    "        sv_indices = self.alpha > 1e-4\n",
    "        self.support_vectors = X[sv_indices]\n",
    "        self.support_vector_labels = y[sv_indices]\n",
    "        self.alpha = self.alpha[sv_indices]\n",
    "        self.compute_bias()\n",
    "    \n",
    "    def compute_bias(self):\n",
    "        free_sv_indices = (self.alpha < self.C - 1e-4)\n",
    "        if np.any(free_sv_indices):\n",
    "            b_values = []\n",
    "            free_alphas = self.alpha[free_sv_indices]\n",
    "            free_vecs   = self.support_vectors[free_sv_indices]\n",
    "            free_labels = self.support_vector_labels[free_sv_indices]\n",
    "            for i in range(len(free_alphas)):\n",
    "                sum_term = 0\n",
    "                for j in range(len(self.alpha)):\n",
    "                    sum_term += self.alpha[j] * self.support_vector_labels[j] * \\\n",
    "                                self.kernel(self.support_vectors[j], free_vecs[i])\n",
    "                b_values.append(free_labels[i] - sum_term)\n",
    "            self.b = np.mean(b_values)\n",
    "        else:\n",
    "            self.b = 0\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = []\n",
    "        for x in X:\n",
    "            prediction = 0\n",
    "            for i in range(len(self.alpha)):\n",
    "                prediction += self.alpha[i] * self.support_vector_labels[i] * \\\n",
    "                              self.kernel(self.support_vectors[i], x)\n",
    "            y_pred.append(prediction + self.b)\n",
    "        return np.sign(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation\n",
    "\n",
    "We load the dataset and select two features to allow for 2D visualization.\n",
    "\n",
    "**Selected Features:**\n",
    "1.  **Mean Radius** (Feature 0)\n",
    "2.  **Mean Texture** (Feature 1)\n",
    "\n",
    "These two features have significant overlap, making a linear boundary insufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Breast Cancer Data\n",
    "data = load_breast_cancer()\n",
    "X_full = data.data\n",
    "y_full = data.target\n",
    "\n",
    "# Select only the first two features: Mean Radius and Mean Texture\n",
    "X = X_full[:, :2]\n",
    "\n",
    "# Map Labels:\n",
    "# 0 (Malignant) -> -1\n",
    "# 1 (Benign) -> 1\n",
    "y = np.where(y_full == 0, -1, 1)\n",
    "\n",
    "# Subsample for speed (Optimization is O(N^2) or O(N^3))\n",
    "# We take 200 samples to keep computation under 30 seconds\n",
    "np.random.seed(42)\n",
    "indices = np.random.choice(len(y), 200, replace=False)\n",
    "X = X[indices]\n",
    "y = y[indices]\n",
    "\n",
    "print(f\"Data Shape: {X.shape}\")\n",
    "print(f\"Example Sample (Raw): {X[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Standardization (Normalization)\n",
    "\n",
    "Here we implement the crucial step of scaling our data. We transform the data so that each feature has a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "$$ x_{scaled} = \\frac{x - \\mu}{\\sigma} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Mean and Std Deviation manually\n",
    "mean = np.mean(X, axis=0)\n",
    "std = np.std(X, axis=0)\n",
    "\n",
    "# Apply Z-score normalization\n",
    "X_scaled = (X - mean) / std\n",
    "\n",
    "print(f\"Example Sample (Scaled): {X_scaled[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training the SVM\n",
    "\n",
    "We use the **Gaussian Kernel** with `sigma=0.7` and `C=2.0`.  \n",
    "\n",
    "Because the classes overlap significantly (cancer diagnosis is rarely clear-cut on just two features), we expect a large number of support vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper to inject sigma into the kernel function\n",
    "def rbf_wrapper(x1, x2):\n",
    "    return gaussian_kernel(x1, x2, sigma=0.7)\n",
    "\n",
    "# Train SVM\n",
    "# C=2.0 allows some flexibility for misclassified points (Soft Margin)\n",
    "svm = SVM(kernel=rbf_wrapper, C=1.0)\n",
    "svm.fit(X_scaled, y)\n",
    "\n",
    "print(f\"Training Complete. Found {len(svm.support_vectors)} support vectors out of {len(y)} samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualizing the Decision Boundary\n",
    "\n",
    "The plot below visualizes the diagnosis logic.\n",
    "* **Red Regions**: Predicted Malignant (-1)\n",
    "* **Blue Regions**: Predicted Benign (+1)\n",
    "* **Circled Points**: Support Vectors (the ambiguous cases that define the boundary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boundary(model, X, y):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Scatter Plot\n",
    "    # Red for Malignant (-1), Blue for Benign (1)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm_r', s=50, edgecolors='k')\n",
    "\n",
    "    # Highlight Support Vectors\n",
    "    plt.scatter(model.support_vectors[:, 0], model.support_vectors[:, 1],\n",
    "                s=150, facecolors='none', edgecolors='k', linewidth=1.5, label='Support Vectors')\n",
    "\n",
    "    # Create Grid\n",
    "    ax = plt.gca()\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "    xx = np.linspace(xlim[0], xlim[1], 40)\n",
    "    yy = np.linspace(ylim[0], ylim[1], 40)\n",
    "    YY, XX = np.meshgrid(yy, xx)\n",
    "    xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
    "\n",
    "    # Predict\n",
    "    Z = []\n",
    "    for x_sample in xy:\n",
    "        # Calculate prediction score manually for contour levels\n",
    "        prediction = 0\n",
    "        for i in range(len(model.alpha)):\n",
    "            prediction += model.alpha[i] * model.support_vector_labels[i] * \\\n",
    "                          model.kernel(model.support_vectors[i], x_sample)\n",
    "        Z.append(prediction + model.b)\n",
    "    Z = np.array(Z).reshape(XX.shape)\n",
    "\n",
    "    # Contour\n",
    "    ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5,\n",
    "               linestyles=['--', '-', '--'])\n",
    "\n",
    "    plt.title('Breast Cancer Diagnosis (SVM RBF Kernel)\\nMean Radius vs. Mean Texture')\n",
    "    plt.xlabel('Mean Radius (Standardized)')\n",
    "    plt.ylabel('Mean Texture (Standardized)')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n",
    "\n",
    "plot_boundary(svm, X_scaled, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "You can see that the SVM has learned a **curved decision boundary**. \n",
    "\n",
    "It has effectively identified a region (bottom-left) corresponding to smaller, smoother cells which are typically **Benign** (Blue), and separated them from the larger, more textured cells which are typically **Malignant** (Red). \n",
    "\n",
    "The **Standardization** step was critical here; without it, the \"Radius\" axis would have dominated the distance calculation, and the vertical nuance of \"Texture\" would have been lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
