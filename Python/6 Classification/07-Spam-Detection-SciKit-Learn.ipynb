{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(open(\"../style.css\", \"r\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext nb_mypy\n",
    "%nb_mypy On"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Detection Using Scikit-Learn\n",
    "\n",
    "In this notebook, we will build a spam detector using the **Naive Bayes** algorithm provided by the `scikit-learn` library. \n",
    "\n",
    "The process is streamlined into the following steps:\n",
    "\n",
    "  - **Data Loading**: Reading email text files from directories.\n",
    "  - **Feature Extraction**: Converting text data into numerical vectors (counts) using `CountVectorizer`.\n",
    "  - **Model Training**: Fitting a `MultinomialNB` classifier on the training data.\n",
    "  - **Evaluation**: Calculating the <em style='color:blue;'>precision</em> and <em style='color:blue;'>recall</em> using built-in metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Imports and Setup\n",
    "\n",
    "We need `os` for file handling and several modules from `sklearn` for the machine learning pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need several classes and functions from SciKit-Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The directory \n",
    "https://github.com/karlstroetmann/Artificial-Intelligence/tree/master/Python/6%20Classification/EmailData\n",
    "contains 960 emails that are divided into four subdirectories:\n",
    "\n",
    "  - `spam-train` contains 350 spam emails for training,\n",
    "  - `ham-train`  contains 350 non-spam emails for training,\n",
    "  - `spam-test`  contains 130 spam emails for testing,\n",
    "  - `ham-test`   contains 130 non-spam emails for testing.\n",
    "\n",
    "Originally, this data has been collected by **Ion Androutsopoulos**.  I have found this data on a now defunct \n",
    "*open classroom* page on https://online.stanford.edu/free-courses provided by Andrew Ng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_dir_train: str = 'EmailData/spam-train/'\n",
    "ham__dir_train: str = 'EmailData/ham-train/'\n",
    "spam_dir_test:  str = 'EmailData/spam-test/'\n",
    "ham__dir_test:  str = 'EmailData/ham-test/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Loading Data\n",
    "\n",
    "Unlike the manual implementation where we processed files one by one during prediction, `scikit-learn` works best when we load all data into memory first (lists of strings).\n",
    "\n",
    "We define a helper function `load_data` that reads all files from a spam directory and a ham directory, returning a list of email texts (`X`) and a list of labels (`y`).\n",
    "\n",
    "Convention:\n",
    "* **1**: Spam\n",
    "* **0**: Ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(spam_dir: str, ham_dir: str) -> tuple[list[str], list[int]]:\n",
    "    emails = []\n",
    "    labels = []\n",
    "    # Load Spam (Label = 1)\n",
    "    for filename in os.listdir(spam_dir):\n",
    "        path = os.path.join(spam_dir, filename)\n",
    "        with open(path, 'r', encoding='latin-1') as f:\n",
    "            emails.append(f.read())\n",
    "            labels.append(1)\n",
    "    # Load Ham (Label = 0)\n",
    "    for filename in os.listdir(ham_dir):\n",
    "        path = os.path.join(ham_dir, filename)\n",
    "        with open(path, 'r', encoding='latin-1') as f:\n",
    "            emails.append(f.read())\n",
    "            labels.append(0)\n",
    "    return emails, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load the training and testing sets into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_text, y_train = load_data(spam_dir_train, ham__dir_train)\n",
    "X_test_text, y_test   = load_data(spam_dir_test, ham__dir_test)\n",
    "\n",
    "print(f\"Training samples: {len(X_train_text)}\")\n",
    "print(f\"Testing samples:  {len(X_test_text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Vectorization (Feature Extraction)\n",
    "\n",
    "The naive Bayes algorithm requires numerical data. In the previous manual implementation, we created a dictionary of \"Common Words\" and counted them manually.\n",
    "\n",
    "`scikit-learn` provides `CountVectorizer` which automates this:\n",
    "1.  **Tokenization**: Splits text into words.\n",
    "2.  **Vocabulary Building**: Finds all unique words (features).\n",
    "3.  **Encoding**: Counts how often each word appears in each email.\n",
    "\n",
    "We fit the vectorizer *only* on the training data to avoid data leakage, then transform both sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize CountVectorizer\n",
    "# We can set max_features=2500 to match the original notebook's \"Common_Words\" logic if desired,\n",
    "# but Scikit-Learn can handle the full vocabulary efficiently. Let's stick to the default.\n",
    "vectorizer = CountVectorizer(max_features=2500)\n",
    "\n",
    "# Learn vocabulary from training text and vectorize it\n",
    "X_train = vectorizer.fit_transform(X_train_text)\n",
    "\n",
    "# Vectorize test text (using the vocabulary learned from training)\n",
    "X_test = vectorizer.transform(X_test_text)\n",
    "\n",
    "print(f\"Vocabulary size: {len(vectorizer.get_feature_names_out())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Training the Model\n",
    "\n",
    "We use `MultinomialNB`, which is the standard Naive Bayes variant for data with discrete counts (like word counts).\n",
    "\n",
    "This replaces the manual calculation of `Spam_Probability` and `Ham_Probability` dictionaries. The parameter `alpha=1.0` in `MultinomialNB` handles the *Laplace smoothing* automatically (just as we added +1 in the manual formula)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB(class_prior=[0.99, 0.01], fit_prior=False, alpha=1.0)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Evaluation\n",
    "\n",
    "We can now predict the labels for our test set and calculate **Precision** and **Recall**.\n",
    "\n",
    "Recall definitions:\n",
    "  - *Precision*: percentage of selected items that are relevant (True Positives / (True Positives + False Positives))\n",
    "  - *Recall*:    percentage of relevant items selected (True Positives / (True Positives + False Negatives))\n",
    "\n",
    "Note: In `scikit-learn`, we defined Spam as `1` (positive class). In the original notebook, the precision/recall was calculated specifically regarding *Ham* as the positive class (seeking to avoid filtering important emails).\n",
    "\n",
    "Below, we print a classification report which shows metrics for *both* classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"Classification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Ham', 'Spam']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
