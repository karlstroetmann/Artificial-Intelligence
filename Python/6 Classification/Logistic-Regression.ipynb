{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "with open (\"../style.css\", \"r\") as file:\n",
    "    css = file.read()\n",
    "HTML(css)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define the sigmoid function $S(t) := \\large \\frac{1}{1 + \\exp(-t)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    return 1.0 / (1.0 + np.exp(-t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are using NumPy to compute $\\exp(t)$, we can feed this function with a `numpy` array to compute the sigmoid function for every element of the array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid(np.array([-1.0, 0.0, 1.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check the limits.  In the lecture we have seen that\n",
    "$$ \\lim\\limits_{x \\rightarrow -\\infty} S(x) = 0 \\quad \\mbox{and} \\quad \n",
    "   \\lim\\limits_{x \\rightarrow +\\infty} S(x) = 1 \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid(-100), sigmoid(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the natural logarithm of the sigmoid function.  If we implement this as `log(sigmoid(t))` we will get overflow issues for negative values of $t$ such that $t < -1000$ as the expression `np.exp(-t)` will overflow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.exp(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-np.log(1 + np.exp(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not what we expected.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, for $t < -100$ we have that $1 + \\exp(-t) \\approx \\exp(-t)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "1 + np.exp(-(-100)) == np.exp(-(-100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, if $t < -100$ we have:\n",
    "$$ \n",
    "\\begin{array}{lcl}\n",
    "         \\ln\\left(\\large\\frac{1}{1+\\exp(-t)}\\right) \n",
    "  & = & -\\ln\\bigl(1+\\exp(-t)\\bigr) \\\\\n",
    "  & \\approx & -\\ln\\bigl(\\exp(-t)\\bigr)  \\\\\n",
    "  & = & t\n",
    "\\end{array}\n",
    "$$\n",
    "Hence $\\ln\\bigl(S(t)\\bigr) \\approx t$ for $t < -100$. The following implementation uses this approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logSigmoid(t):\n",
    "    if t > -100:\n",
    "        return -np.log(1.0 + np.exp(-t))\n",
    "    else:\n",
    "        return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logSigmoid(-1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a feature matrix `X` and a vector `y` of classification outputs, the *log-likelihood function* $\\ell\\ell(\\textbf{X}, \\textbf{y},\\textbf{w})$ is mathematically defined as follows:\n",
    "$$\\ell\\ell(\\mathbf{X},\\mathbf{y},\\mathbf{w}) = \n",
    " \\sum\\limits_{i=1}^N \\ln\\Bigl(S\\bigl(y_i \\cdot(\\mathbf{x}_i^\\top \\cdot \\mathbf{w})\\bigr)\\Bigr) =\n",
    " \\sum\\limits_{i=1}^N L\\bigl(y_i \\cdot(\\mathbf{x}_i^\\top \\cdot \\mathbf{w})\\bigr)\n",
    "$$\n",
    "The value of the *log-likelihood function* is interpreted as the logarithm of the probability that our model of the classifier predicts the observed values $y_i$ when the features are given by the vector $\\textbf{x}_i$ for all $i\\in\\{1,\\cdots,N\\}$.\n",
    "\n",
    "The arguments $\\textbf{X}$, $\\textbf{y}$, and $\\textbf{w}$ are interpreted as follows:\n",
    "* $\\textbf{X}$ is the feature matrix, $\\textbf{X}[i]$ is the $i$-th feature vector, i.e we have\n",
    "  $\\textbf{X}[i] = \\textbf{x}_i^\\top$.\n",
    "         \n",
    "  Furthermore, it is assumed that $\\textbf{X}[i][0]$ is 1.0 for all $i$.  \n",
    "  Hence we have a feature that is constant for all examples.\n",
    "* $\\textbf{y}$ is the output vector, $\\textbf{y}[i] \\in \\{-1,+1\\}$ for all $i$.\n",
    "* $\\textbf{w}$ is the weight vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ll(X, y, w):   \n",
    "    return np.sum([logSigmoid(y[i] * (X[i] @ w)) for i in range(len(X))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function $\\mathtt{gradLL}(\\mathbf{x}, \\mathbf{y}, \\mathbf{w})$ computes the gradient of\n",
    "the log-likelihood according to the formula\n",
    "$$ \\frac{\\partial\\quad}{\\partial\\, w_j}\\ell\\ell(\\mathbf{X},\\mathbf{y};\\mathbf{w}) =\n",
    "   \\sum\\limits_{i=1}^N y_i \\cdot x_{i,j} \\cdot  S(-y_i \\cdot \\mathbf{x}_i \\cdot \\mathbf{w}).\n",
    "$$\n",
    "The different components of this gradient are combined into a vector.\n",
    "The arguments are the same as the arguments to the function $\\ell\\ell$ that computes the log-likelihood, i.e.\n",
    "* $\\textbf{X}$ is the feature matrix, $\\textbf{X}[i]$ is the transpose of $i$-th feature vector.\n",
    "* $\\textbf{y}$ is the output vector, $\\textbf{y}[i] \\in \\{-1,+1\\}$ for all $i$.\n",
    "* $\\textbf{w}$ is the weight vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradLL(X, y, w):\n",
    "    Gradient = []\n",
    "    for j in range(len(X[0])):\n",
    "        L = [y[i] * X[i][j] * sigmoid(-y[i] * (X[i] @ w)) for i in range(len(X))]\n",
    "        Gradient.append(sum(L))\n",
    "    return np.array(Gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we want to investigate is stored in the file `'exam.csv'`.  The first column of this file is an integer from the set $\\{0,1\\}$.  The number is $0$ if the corresponding student has failed the exam and is $1$ otherwise.  The second column is a floating point number that lists the number of hours that the student has studied for the given exam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file `exam.csv` contains fictional data about an exam. The first column contains the number `0` if the student has failed the exam and `1` otherwise.  The second column contains the number of hours the student has studied for the given exam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat exam.csv || type exam.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('exam.csv') as file:\n",
    "    reader = csv.reader(file, delimiter=',')\n",
    "    count  = 0  # line count\n",
    "    Pass   = []\n",
    "    Hours  = []\n",
    "    for row in reader:\n",
    "        if count != 0:  # skip header\n",
    "            Pass .append(float(row[0]))\n",
    "            Hours.append(float(row[1]))\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To proceed, we will plot the data points.  To this end we transform the lists `Pass` and `Hours` into numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(Pass)\n",
    "x = np.array(Hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn           as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "sns.set(style='darkgrid')\n",
    "plt.title('Pass/Fail vs. Hours of Study')\n",
    "plt.axvline(x=0.0, c='k')\n",
    "plt.axhline(y=0.0, c='k')\n",
    "plt.xlabel('Hours of Study')\n",
    "plt.ylabel('Pass = 1, Fail = 0')\n",
    "plt.xticks(np.arange(0.0, 6.0, step=0.5))\n",
    "plt.yticks(np.arange(-0.0, 1.1, step=0.1))\n",
    "plt.scatter(x, y, color='b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of students is stored in the variable `n`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(y)\n",
    "n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to turn the vector `x` into the feature matrix `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.reshape(x, (n, 1))\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We append the number $1.0$ to every row of `X`. `axis=1` specifies that the ones are appended to each column.  If we had specified `axis=0` instead, the number of rows would have doubled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = np.append(X, np.ones((n, 1)), axis=1)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, the entries in the vector `y` are either $0$ or $1$.  These values need to be transformed to $-1$ and $+1$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 2 * y - 1\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have no real clue about the weights, we set them to $0$ initially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradient_ascent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start   = np.zeros((2,))\n",
    "eps     = 10 ** -8\n",
    "f       = lambda w: ll(X, y, w)\n",
    "gradF   = lambda w: gradLL(X, y, w)\n",
    "w, _, _ = gradient_ascent.findMaximum(f, gradF, start, eps, True)\n",
    "beta    = w[1]\n",
    "gamma   = w[0]\n",
    "print(f'model: P(pass|hours) = S({beta} + {gamma} * hours)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot this function together with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 9))\n",
    "sns.set_style('whitegrid')\n",
    "plt.title('Pass/Fail vs. Hours of Study')\n",
    "H = np.arange(0.0, 6.0, 0.05)\n",
    "P = sigmoid(beta + gamma * H)\n",
    "sns.lineplot(x=H, y=P, color='r')\n",
    "plt.axvline(x=0.0, c='k')\n",
    "plt.axhline(y=0.0, c='k')\n",
    "plt.xlabel('Hours of Study')\n",
    "plt.ylabel('Probability of Passing the Exam')\n",
    "plt.xticks(np.arange(0.0, 6.0, step=0.5))\n",
    "plt.yticks(np.arange(-0.0, 1.01, step=0.1))\n",
    "plt.scatter(x, (y + 1) / 2, color='b')\n",
    "plt.savefig('exam-probability.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
