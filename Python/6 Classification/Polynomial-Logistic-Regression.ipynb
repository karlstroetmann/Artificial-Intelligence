{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "with open (\"../style.css\", \"r\") as file:\n",
    "    css = file.read()\n",
    "HTML(css)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy  as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we want to investigate is stored in the file `'fake-data.csv'`.   It is data that I have found somewhere.  I am not sure whether this data is real or fake.  Therefore, I won't discuss the attributes of the data.  The point of the data is that it is a classification problem that can not be solved with \n",
    "ordinary logistic regression.  We will introduce <em style=\"color:blue;\">polynomial logistic regression</em> to solve this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DF = pd.read_csv('fake-data.csv')\n",
    "DF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract the features from the data frame and convert it into a `NumPy` <em style=\"color:blue;\">feature matrix</em>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(DF[['x','y']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract the target column and convert it into a `NumPy` array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.array(DF['class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to plot the instances according to their class we divide the feature matrix $X$ into two parts. $\\texttt{X_pass}$ contains those examples that have class $1$, while $\\texttt{X_fail}$ contains those examples that have class $0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pass = X[Y == 1.0]\n",
    "X_fail = X[Y == 0.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn           as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "sns.set(style='darkgrid')\n",
    "plt.title('A Classification Problem')\n",
    "plt.axvline(x=0.0, c='k')\n",
    "plt.axhline(y=0.0, c='k')\n",
    "plt.xlabel('x axis')\n",
    "plt.ylabel('y axis')\n",
    "plt.xticks(np.arange(-0.9, 1.1, step=0.1))\n",
    "plt.yticks(np.arange(-0.8, 1.2, step=0.1))\n",
    "plt.scatter(X_pass[:,0], X_pass[:,1], color='b') \n",
    "plt.scatter(X_fail[:,0], X_fail[:,1], color='r') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to split the data into a *training set* and a *test set*.\n",
    "The *training set* will be used to compute the parameters of our model, while the\n",
    "*testing set* is only used to check the *accuracy*.  SciKit-Learn has a predefined method\n",
    "`train_test_split` that can be used to randomly split data into a training set and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will split the data at a ratio of $4:1$, i.e. $80\\%$ of the data will be used for training, while the remaining $20\\%$ is used to test the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to build a <em style=\"color:blue;\">logistic regression</em> classifier, we import the module `linear_model` from SciKit-Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sklearn.linear_model as lm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function $\\texttt{logistic_regression}(\\texttt{X_train}, \\texttt{Y_train}, \\texttt{X_test}, \\texttt{Y_test})$ takes a feature matrix $\\texttt{X_train}$ and a corresponding vector $\\texttt{Y_train}$ and computes a logistic regression model $M$ that best fits these data.  Then, the accuracy of the model is computed using the test data $\\texttt{X_test}$ and $\\texttt{Y_test}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(X_train, Y_train, X_test, Y_test, reg=10000):\n",
    "    M = lm.LogisticRegression(C=reg, tol=1e-6)\n",
    "    M.fit(X_train, Y_train)\n",
    "    train_score = M.score(X_train, Y_train)\n",
    "    yPredict    = M.predict(X_test)\n",
    "    accuracy    = np.sum(yPredict == Y_test) / len(Y_test)\n",
    "    return M, train_score, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use this function to build a model for our data.  Initially, we will take all the available data to create the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M, score, accuracy = logistic_regression(X, Y, X, Y)\n",
    "score, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that there are only two classes, the accuracy of our first model is quite poor.  \n",
    "Let us extract the coefficients so we can plot the <em style=\"color:blue;\">decision boundary</em>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ϑ0     = M.intercept_[0]\n",
    "ϑ1, ϑ2 = M.coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "sns.set(style='darkgrid')\n",
    "plt.title('A Classification Problem')\n",
    "plt.axvline(x=0.0, c='k')\n",
    "plt.axhline(y=0.0, c='k')\n",
    "plt.xlabel('x axis')\n",
    "plt.ylabel('y axis')\n",
    "plt.xticks(np.arange(-0.9, 1.1, step=0.1))\n",
    "plt.yticks(np.arange(-0.8, 1.2, step=0.1))\n",
    "plt.scatter(X_pass[:,0], X_pass[:,1], color='b') \n",
    "plt.scatter(X_fail[:,0], X_fail[:,1], color='r') \n",
    "H = np.arange(-0.8, 1.0, 0.05)\n",
    "P = -(ϑ0 + ϑ1 * H)/ϑ2\n",
    "plt.plot(H, P, color='green')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, pure *logistic regression* is not working for this example.  The reason is, that a linear decision boundary is not able to separate the positive examples from the negative examples.  Let us add *polynomial features*.  This enables us to create more complex *decision boundaries*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function $\\texttt{extend}(X)$ takes a feature matrix $X$ that is supposed to contain two features $x$ and $y$.  It creates the new features $x^2$, $y^2$ and $x\\cdot y$ and returns a new feature matrix that also contains these additional features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend(X):\n",
    "    n  = len(X)\n",
    "    fx = np.reshape(X[:,0], (n, 1)) # extract first column\n",
    "    fy = np.reshape(X[:,1], (n, 1)) # extract second column\n",
    "    return np.hstack([fx, fy, fx*fx, fy*fy, fx*fy]) # stack everthing horizontally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_quadratic = extend(X_train)\n",
    "X_test_quadratic  = extend(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M, score, accuracy = logistic_regression(X_train_quadratic, Y_train, X_test_quadratic, Y_test)\n",
    "score, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to work better.  Let us compute the decision boundary and plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ϑ0                 = M.intercept_[0]\n",
    "ϑ1, ϑ2, ϑ3, ϑ4, ϑ5 = M.coef_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision boundary is now given by the following equation:\n",
    "$$ \\vartheta_0 + \\vartheta_1 \\cdot x + \\vartheta_2 \\cdot y + \\vartheta_3 \\cdot x^2 + \\vartheta_4 \\cdot y^2 + \\vartheta_5 \\cdot x \\cdot y = 0$$\n",
    "This is the equation of an ellipse.  Let us plot the *decision boundary* with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a    = np.arange(-1.0, 1.0, 0.005)\n",
    "b    = np.arange(-1.0, 1.0, 0.005)\n",
    "A, B = np.meshgrid(a,b)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = ϑ0 + ϑ1 * A + ϑ2 * B + ϑ3 * A * A + ϑ4 * B * B + ϑ5 * A * B \n",
    "Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "sns.set(style='darkgrid')\n",
    "plt.title('A Classification Problem')\n",
    "plt.axvline(x=0.0, c='k')\n",
    "plt.axhline(y=0.0, c='k')\n",
    "plt.xlabel('x axis')\n",
    "plt.ylabel('y axis')\n",
    "plt.xticks(np.arange(-0.9, 1.1, step=0.1))\n",
    "plt.yticks(np.arange(-0.8, 1.2, step=0.1))\n",
    "plt.scatter(X_pass[:,0], X_pass[:,1], color='b') \n",
    "plt.scatter(X_fail[:,0], X_fail[:,1], color='r') \n",
    "CS = plt.contour(A, B, Z, 0, colors='green')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try to add <em style=\"color:blue;\">quartic features</em> next.  These are features like $x^4$, $x^2\\cdot y^2$, etc.\n",
    "Luckily, SciKit-Learn has function that can automize this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quartic = PolynomialFeatures(4, include_bias=False)\n",
    "X_train_quartic = quartic.fit_transform(X_train)\n",
    "X_test_quartic  = quartic.fit_transform(X_test)\n",
    "print(quartic.get_feature_names(['x', 'y']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us fit the quartic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M, score, accuracy = logistic_regression(X_train_quartic, Y_train, X_test_quartic, Y_test)\n",
    "score, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy on the training set has increased, but we observe that the accuracy on the training set is actually not improving. Again, we proceed to plot the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ϑ0 = M.intercept_[0]\n",
    "ϑ1, ϑ2, ϑ3, ϑ4, ϑ5, ϑ6, ϑ7, ϑ8, ϑ9, ϑ10, ϑ11, ϑ12, ϑ13, ϑ14 = M.coef_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the decision boundary starts to get tedious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a    = np.arange(-1.0, 1.0, 0.005)\n",
    "b    = np.arange(-1.0, 1.0, 0.005)\n",
    "A, B = np.meshgrid(a,b)\n",
    "Z    = ϑ0 + ϑ1 * A + ϑ2 * B + \\\n",
    "       ϑ3 * A**2 + ϑ4 * A * B + ϑ5 * B**2 + \\\n",
    "       ϑ6 * A**3 + ϑ7 * A**2 * B + ϑ8 * A * B**2 + ϑ9 * B**3 + \\\n",
    "       ϑ10 * A**4 + ϑ11 * A**3 * B + ϑ12 * A**2 * B**2 + ϑ13 * A * B**3 + ϑ14 * B**4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "sns.set(style='darkgrid')\n",
    "plt.title('A Classification Problem')\n",
    "plt.axvline(x=0.0, c='k')\n",
    "plt.axhline(y=0.0, c='k')\n",
    "plt.xlabel('x axis')\n",
    "plt.ylabel('y axis')\n",
    "plt.xticks(np.arange(-0.9, 1.1, step=0.1))\n",
    "plt.yticks(np.arange(-0.8, 1.2, step=0.1))\n",
    "plt.scatter(X_pass[:,0], X_pass[:,1], color='b') \n",
    "plt.scatter(X_fail[:,0], X_fail[:,1], color='r') \n",
    "CS = plt.contour(A, B, Z, 0, colors='green')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision boundary looks strange.  Let's get bold and try to add features of a higher power.  \n",
    "However, in order to understand what is happening, we will only plot the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pass_train = X_train[Y_train == 1.0]\n",
    "X_fail_train = X_train[Y_train == 0.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to automatize the process, we define some auxiliary functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\texttt{polynomial}(n)$ creates a polynomial in the variables `A` and `B` that contains all terms of the form $\\Theta[k] \\cdot A^i \\cdot B^j$ where $i+j \\leq n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial(n):\n",
    "    sum = 'Θ[0]' \n",
    "    cnt = 0\n",
    "    for k in range(1, n+1):\n",
    "        for i in range(0, k+1):\n",
    "            cnt += 1\n",
    "            sum += f' + Θ[{cnt}] * A**{k-i} * B**{i}'\n",
    "    print('number of features:', cnt)\n",
    "    return sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check this out for $n=4$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function $\\texttt{polynomial_grid}(n, M)$ takes a number $n$ and a model $M$.  It returns a meshgrid that can be used to plot the decision boundary of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_grid(n, M):\n",
    "    Θ    = [M.intercept_[0]] + list(M.coef_[0])\n",
    "    a    = np.arange(-1.0, 1.0, 0.005)\n",
    "    b    = np.arange(-1.0, 1.0, 0.005)\n",
    "    A, B = np.meshgrid(a,b)\n",
    "    return eval(polynomial(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function $\\texttt{plot_nth_degree_boundary}(n)$ creates a polynomial logistic regression model of degree $n$.  It plots both the training data and the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_nth_degree_boundary(n, C=10000):\n",
    "    poly         = PolynomialFeatures(n, include_bias=False)\n",
    "    X_train_poly = poly.fit_transform(X_train)\n",
    "    X_test_poly  = poly.fit_transform(X_test)\n",
    "    M, score, accuracy = logistic_regression(X_train_poly, Y_train, X_test_poly, Y_test, C)\n",
    "    print('The accuracy on the training set is:', score)\n",
    "    print('The accuracy on the test     set is:', accuracy)\n",
    "    Z = polynomial_grid(n, M)\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    sns.set(style='darkgrid')\n",
    "    plt.title('A Classification Problem')\n",
    "    plt.axvline(x=0.0, c='k')\n",
    "    plt.axhline(y=0.0, c='k')\n",
    "    plt.xlabel('x axis')\n",
    "    plt.ylabel('y axis')\n",
    "    plt.xticks(np.arange(-0.9, 1.11, step=0.1))\n",
    "    plt.yticks(np.arange(-0.8, 1.21, step=0.1))\n",
    "    plt.scatter(X_pass_train[:,0], X_pass_train[:,1], color='b') \n",
    "    plt.scatter(X_fail_train[:,0], X_fail_train[:,1], color='r') \n",
    "    CS = plt.contour(A, B, Z, 0, colors='green')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us test this for the polynomial logistic regression model of degree $4$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_nth_degree_boundary(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to be the same shape that we have seen earlier.  It looks like the function $\\texttt{plot_nth_degree_boundary}(n)$ is working.  Let's try higher degree polynomials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_nth_degree_boundary(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score on the training set has improved.  What happens if we try still higher degrees?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_nth_degree_boundary(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We captured one more of the training examples.  Let's get bold, we want a $100\\%$ training accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_nth_degree_boundary(14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is getting more complicated, but it is not getting better, as the accuracy on the test set has not improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=2)\n",
    "X_pass_train = X_train[Y_train == 1.0]\n",
    "X_fail_train = X_train[Y_train == 0.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check whether regularization can help.  Below, the regularization parameter prevents the decision boundary from becoming to wiggly and thus the accuracy on the test set can increase.  The function below plots all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_nth_degree_boundary_all(n, C):\n",
    "    poly         = PolynomialFeatures(n, include_bias=False)\n",
    "    X_train_poly = poly.fit_transform(X_train)\n",
    "    X_test_poly  = poly.fit_transform(X_test)\n",
    "    M, score, accuracy = logistic_regression(X_train_poly, Y_train, X_test_poly, Y_test, C)\n",
    "    print('The accuracy on the training set is:', score)\n",
    "    print('The accuracy on the test     set is:', accuracy)\n",
    "    Z = polynomial_grid(n, M)\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    sns.set(style='darkgrid')\n",
    "    plt.title('A Classification Problem')\n",
    "    plt.axvline(x=0.0, c='k')\n",
    "    plt.axhline(y=0.0, c='k')\n",
    "    plt.xlabel('x axis')\n",
    "    plt.ylabel('y axis')\n",
    "    plt.xticks(np.arange(-0.9, 1.11, step=0.1))\n",
    "    plt.yticks(np.arange(-0.8, 1.21, step=0.1))\n",
    "    plt.scatter(X_pass[:,0], X_pass[:,1], color='b') \n",
    "    plt.scatter(X_fail[:,0], X_fail[:,1], color='r') \n",
    "    CS = plt.contour(A, B, Z, 0, colors='green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_nth_degree_boundary_all(14, 100.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_nth_degree_boundary_all(20, 100000.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
